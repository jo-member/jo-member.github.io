<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Image Classification - Jo Member</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Jo Member"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Jo Member"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="K Nearest Neighbors (k-NN) 기존의 data가 가지고있는 label을 활용해서 새로운 data의 label을 분류하는 문제가 된다. 이렇게 된다면 미리 유사도를 정의해야 한다. 그리고 system 복잡도가 너무 높다. 따라서 data를 NN의 parameter에 녹여넣는 것이다. Yann Lecun의 CNN 개발 : 우편번호인식에"><meta property="og:type" content="blog"><meta property="og:title" content="Image Classification"><meta property="og:url" content="https://jo-member.github.io/2021/03/08/2021-03-08-Boostcamp31.1/"><meta property="og:site_name" content="Jo Member"><meta property="og:description" content="K Nearest Neighbors (k-NN) 기존의 data가 가지고있는 label을 활용해서 새로운 data의 label을 분류하는 문제가 된다. 이렇게 된다면 미리 유사도를 정의해야 한다. 그리고 system 복잡도가 너무 높다. 따라서 data를 NN의 parameter에 녹여넣는 것이다. Yann Lecun의 CNN 개발 : 우편번호인식에"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://jo-member.github.io/images/image-20210308112418434.png"><meta property="og:image" content="https://jo-member.github.io/images/image-20210308113809064.png"><meta property="og:image" content="https://jo-member.github.io/images/image-20210309113004104.png"><meta property="og:image" content="https://jo-member.github.io/images/image-20210309113203089.png"><meta property="og:image" content="https://jo-member.github.io/images/image-20210309113447823.png"><meta property="og:image" content="https://jo-member.github.io/images/image-20210309140019684.png"><meta property="og:image" content="https://jo-member.github.io/images/image-20210311105154100.png"><meta property="og:image" content="https://jo-member.github.io/images/image-20210312111734568.png"><meta property="og:image" content="https://jo-member.github.io/images/image-20210312144707419.png"><meta property="article:published_time" content="2021-03-07T15:00:00.000Z"><meta property="article:modified_time" content="2021-07-12T04:45:29.581Z"><meta property="article:author" content="jo-member"><meta property="article:tag" content="CNN"><meta property="article:tag" content="Vision"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/images/image-20210308112418434.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://jo-member.github.io/2021/03/08/2021-03-08-Boostcamp31.1/"},"headline":"Image Classification","image":["https://jo-member.github.io/images/image-20210308112418434.png","https://jo-member.github.io/images/image-20210308113809064.png","https://jo-member.github.io/images/image-20210309113004104.png","https://jo-member.github.io/images/image-20210309113203089.png","https://jo-member.github.io/images/image-20210309113447823.png","https://jo-member.github.io/images/image-20210309140019684.png","https://jo-member.github.io/images/image-20210311105154100.png","https://jo-member.github.io/images/image-20210312111734568.png","https://jo-member.github.io/images/image-20210312144707419.png"],"datePublished":"2021-03-07T15:00:00.000Z","dateModified":"2021-07-12T04:45:29.581Z","author":{"@type":"Person","name":"jo-member"},"description":"K Nearest Neighbors (k-NN) 기존의 data가 가지고있는 label을 활용해서 새로운 data의 label을 분류하는 문제가 된다. 이렇게 된다면 미리 유사도를 정의해야 한다. 그리고 system 복잡도가 너무 높다. 따라서 data를 NN의 parameter에 녹여넣는 것이다. Yann Lecun의 CNN 개발 : 우편번호인식에"}</script><link rel="canonical" href="https://jo-member.github.io/2021/03/08/2021-03-08-Boostcamp31.1/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="Jo Member" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/jo-member"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-9-tablet is-9-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-03-07T15:00:00.000Z" title="2021. 3. 8. 오전 12:00:00">2021-03-08</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-07-12T04:45:29.581Z" title="2021. 7. 12. 오후 1:45:29">2021-07-12</time></span><span class="level-item"><a class="link-muted" href="/categories/Boostcamp/">Boostcamp</a></span></div></div><h1 class="title is-3 is-size-4-mobile">Image Classification</h1><div class="content"><br/>



<p>K Nearest Neighbors (k-NN)</p>
<p>기존의 data가 가지고있는 label을 활용해서 새로운 data의 label을 분류하는 문제가 된다. 이렇게 된다면 미리 유사도를 정의해야 한다. 그리고 system 복잡도가 너무 높다. 따라서 data를 NN의 parameter에 녹여넣는 것이다.</p>
<p>Yann Lecun의 CNN 개발 : 우편번호인식에 혁신을 이루어냄</p>
<p>Using better activation function   </p>
<p>annotation data의 효율적인 학습 기법 </p>
<p>data 부족문제의 완화 : 대표적인 방법들</p>
<ol>
<li>Data augmentation</li>
<li>Leveraging pre-trained information</li>
<li>Leveraging unlabeled dataset for training</li>
</ol>
<span id="more"></span>

<p>Data augmentation</p>
<p>Data를 통한 pattern의 분석</p>
<p>Dataset is almost biased != real data<br>결국 우리가 사용하는  data들은 사람이 bias해서 찍은 사진들이 대부분이기 때문에 우리가 얻어놓은 training data까지 모두 표현하지 못하는 data들이다.</p>
<p>ex) crop, rotate, Brightness, …</p>
<p>Affine transformation</p>
<p>변환전후에 선으로 유지가 되고, 길이의 비율과 평행관계가 유지가 되지만 각도가 달라지는. </p>
<p>기본적인 틀을 맞춘 attine transofrmation</p>
<p>mixing both images and labels</p>
<p>RandAugment</p>
<p>random하게 augmentation 방법을 수행후 잘나온것을 가져다 쓰자. 어떤걸 적용할까, 어떤 강도로 augmentation을 할까?</p>
<p>이걸 policy리고 한다. Random sampling시</p>
<p>dataset을 만들어야 하는데 이러한 data를 모을때 label이 필요하기 떄문에 이러한 data를  단기간에 수집하기가 쉽지가 않다.</p>
<p><strong>Transfer learning</strong></p>
<p>기존에 학습시킨 model에 조금 바꿔서 적용. 한데이터set에서 배운 지식을 다른 task에 적용</p>
<p>한 dataset에 적용된 경우에 다른곳에도 적용할 수 있지 않을까?<br>Freeze 기존의 CNN layer’s parameter<br>적은 data로 부터</p>
<p><img src="/images/image-20210308112418434.png" alt="image-20210308112418434"></p>
<p>Pseudo-labeling이 좀 신기하다.</p>
<p>Knowledge distillation</p>
<p><img src="/images/image-20210308113809064.png" alt="image-20210308113809064"></p>
<p>더 깊은 network -&gt; 더 높은 성능</p>
<p>깊게 쌓을수록 gradient explosion이나 vanishing gradient가 발생하였다, 계산복잡도가 올라가서 속도의 저하, overfitting문제가 아니라 degradation problem이라는게 밝혀졌다.</p>
<p>네트워크를 깊게 쌓기위한 network</p>
<ol>
<li>GoogLeNet</li>
</ol>
<p>하나의 layer에서 다양한 크기의 cnn filter를 사용하서 여러측면으로 image를 관찰하겠다. 한층에 이렇게 여러 filter를 사용하게 되면 계산복잡도가 올라가고, parameter숫자가 늘어나기 때문에, 1x1 filter를 추가해 주었다. 1x1 layer as bottle neck architecture</p>
<ul>
<li>공간크기는 변하지 않고, channel 수만 변화시켜준다.</li>
</ul>
<p>Overall architecture</p>
<ul>
<li>inception module을 깊게 쌓아서 전체 network 형성</li>
<li>Auxiliary classifiers : gradient vanising 문제를 해결하기 위해 추가해준 classifiers. 중간중간에 gradient를 꼽자주는 역할을 한다.</li>
<li>loss가 중간에서 부터 흘러들어가기 때문에 멀리있는 단까지 gradient 전달이 가능하다.</li>
</ul>
<p>Auxiliary classifier</p>
<p><img src="/images/image-20210309113004104.png" alt="image-20210309113004104"></p>
<h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>아직도 큰 영향력을 발휘하고 있는 network이다.</p>
<p>최초로 100개 이상의 layer를 쌓았다. 최초로 인간 level의 성능을 뛰어넘었다.</p>
<p>이러한 성과로 cvpr best paper를 받았다. 기존연구자들의 layer를 깊게 쌓는데 문제점</p>
<p><img src="/images/image-20210309113203089.png" alt="image-20210309113203089"></p>
<p>원래는 model parameter가 많으면 error가 줄어들 것이라고 생각했는데, 56 layer의 error가 더 크다는 결과가 나왔기 때문에, over fitting때문이 아니라는 결론이 나옴.</p>
<p>대신에 최적화 문제에 대해서 56 layer이 최적화 되지 않은 결과이다.</p>
<p>Í<img src="/images/image-20210309113447823.png" alt="image-20210309113447823"></p>
<p>이렇게 만들어 버리면 학습의 부담감이 덜어지고 분할정복이 가능한 문제가 되지 않았는가?</p>
<p>이를 해결해 주기 위해ㅐ</p>
<p>shortcut connection을 통해 back prop과정에서 길이 하나가 더생기는 것이다.gradient. vanishing 문제가 해결이 되었다. 왜성능이 잘나올까?</p>
<p>residual connection을 하나 추가할때마다 2배씩 path가 늘어난다. 다양한 경로를 통해서 굉장히 복잡한 mapping의 학습이 가능했다.</p>
<p>initialization으로 He initialization을 사용했다. Reason ? -&gt; initialize를 작게 해주어야 이후에 더해줄때 균형이 맞는다.</p>
<p>3x3 conv layer로 모두 이루어져 있다.</p>
<p>Only a single FC layer at final output</p>
<h2 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h2><p>channel 축으로 concatnate한다. 훨씬이전의 layer에 대한 정보들도 모두 이어준다. 상위 layer에서도 모든 하위 layer의 특징을 참조할 수 있도록 해주었다.</p>
<p>더하기 두 신호를 합쳐버린다</p>
<p>concatnate chanel은 늘어나지만 feature를 더욱 잘 보존</p>
<p>fix된 3x3 만큼의 weight paramter가 이미 존재를 하고 2d offset을 위한 branch가 따로 존재 한다. 각각의 weight들을 벌려준다?</p>
<h1 id="Semantic-segmentation"><a href="#Semantic-segmentation" class="headerlink" title="Semantic segmentation"></a>Semantic segmentation</h1><p>픽셀단위로 분류해보자</p>
<p>영상속의 mask를 생성하게 되는데 같은 class이지만 서로다른 물체를 구분하지는 않는다.</p>
<p>영상속에 자동차가 여러대 있어도다 같은 class (색) 으로 구분한다.</p>
<p>영상내의 장면 content를 이해하는데 사용하는 필수적인 기술이다. object들이 구분되는 특징을 이해를 하여 </p>
<h2 id="Fully-Convolutional-Networks"><a href="#Fully-Convolutional-Networks" class="headerlink" title="Fully Convolutional Networks"></a>Fully Convolutional Networks</h2><p>입력에서 부터 끝까지 NN으로 구성한다.<br>입력으로 임의의 해상도 출력도 입력에 맞춘 해상도, 중간의 layer들도 모두 미분가능한 layer들이다.</p>
<p>각위치다 channel축으로 flattening이후 각각의 vector를 쌓아서 각 위치마다 vector가 하나씩 나오게 된다. </p>
<p>Upsampling</p>
<p>receptive field가 작기 때문에 upsampling을 통해서 강제로 resolution을 맞추어준다.</p>
<p>일단은 작게 만들어서 receptive field를 최대한 키운다음에 upsampling한다.</p>
<ol>
<li>Transpose Convolution</li>
</ol>
<p>결과를 이렇게 그냥 더해도 되는건가?<br>cnn과 stride 사이즈를 조절해서 겹치는부분이 없게끔 조절해주어야 한다. (overlap problem)</p>
<ol start="2">
<li>Upsampling Convolution</li>
</ol>
<p>학습가능한 upsampling을 학습가능한 하나의 layer로 만들어주었다. </p>
<p>해상도가 낮아지지만 semantic하고 Holistic </p>
<p>중간층의 map을 upsampling한 이후에 </p>
<p>높은 layer에 있는 feature map을 upsampling을 통해 해상도를 올리고 이에 맞춰서 중간층의 map들또한 upsampling한다. 이들을 concatnate하여서 각픽셀마다 class의 score를 뱉어주게 된다. </p>
<p>최대한 많은 layer들을 합친것이 큰 도움이 된다.</p>
<p>FCN은 end to end로 손으로 만든게 아니라 모두 NN이라 병렬처리도 가능하고 성능도 좋으며, low high feature모두 잘 포함한다.</p>
<p>U-Net</p>
<p>built upon fully convolutional networks</p>
<p>with  <strong>skip connections</strong></p>
<p>channel size가 줄고 해상도가 느는 expanding path</p>
<p>fusion - concatnation을 사용한다.</p>
<h2 id="DeepLab"><a href="#DeepLab" class="headerlink" title="DeepLab"></a>DeepLab</h2><p>pixel과 pixel사이의 관계를 이어준후 pixel간의 거리를 모델링하였다.<br>확산의 반복으로 물체의 경계에 잘맞는 segmetation을</p>
<ul>
<li>Dilated convolution</li>
<li>parameter수는 늘어나지만 </li>
</ul>
<p>depthwise convolution</p>
<p>channel별로 conv연산을 해서 값을 각각 뽑은후, 각 channel별로 pointwise convolution을 통하여 하나로 합쳐준다. </p>
<p><img src="/images/image-20210309140019684.png" alt="image-20210309140019684"></p>
<p>Instance segmentation으로 빠르게 발전을 하고있다.</p>
<p>Instance segmantation : 같은 사람이여도 같은색이 아닌 따로따로 segmentation이 가능한 기능</p>
<p>panoptic segmentation</p>
<p>Instance segmentation을 포함하는 기술</p>
<p>객체들을 구분하는 기술 : object detection</p>
<p>scene understanding을 위한 기술</p>
<p>bounding ob와 classification을 동시에 추정하는 기술이다.</p>
<p>해당하는 box의 물체의 category까지 추정한다.<br>2개의 좌표로 bounging box를 결정한다. 나머지는 class에대한 probability를 결정해 준다. Bounding box localization</p>
<p><strong>selective search</strong></p>
<p>oversegmentation 이후 비슷한 색깔끼리 합쳐준다.</p>
<h1 id="Two-stage-detector"><a href="#Two-stage-detector" class="headerlink" title="Two-stage detector"></a>Two-stage detector</h1><ol>
<li>R-CNN</li>
</ol>
<p>기존의 image classification을 활용</p>
<p>selective serch 로 region proposal을 구하고<br>적절한 크기로 warping을 해서 CNN (pretrained)에 넣어준후 category를 구해준다.<br>마지막 classifier은 SVM을 썼다.<br>단점 : model 하나하나마다 모두 cnn을 돌려야하고 selective search를 사용해서 학습을 통한 성능향상에 제한이있다.</p>
<ol start="2">
<li>Fast R-CNN</li>
</ol>
<p>recycle a pre-computed feature for multiple object detection</p>
<p>영상전체에 대한 feature을 추출후 이를 재활용</p>
<ul>
<li>CNN에서 Convolutional feature map을 뽑아주고(warping x)</li>
<li>ROI pooling layer로 feature map으로 부터 ROI feature를 뽑아낸다</li>
<li>feature pooling이후 class와 bbox regression을 사용한다. </li>
</ul>
<p><img src="/images/image-20210311105154100.png" alt="image-20210311105154100"></p>
<p>여전히 roi를 찾기 위해 selective search를 쓰고있다</p>
<ol start="3">
<li>Faster R-CNN</li>
</ol>
<p>최초의 endtoend object detection</p>
<p>IoU = Area overlap/Area of Union, 높을수록 두영역이 많이 겹친다</p>
<ul>
<li>Anchor boxes- 9개의 actor box를 사용하였다. 미리 정해놓은 bbox의 크기</li>
</ul>
<p>Selective search를 대체하는 RPN을 제안하였다.</p>
<p>그럴듯한 bbox만 남기기 위해 non maximum suppression을 사용하였다.</p>
<h1 id="Single-stage-detector"><a href="#Single-stage-detector" class="headerlink" title="Single stage detector"></a>Single stage detector</h1><p>정확도 보다 속도를 선택한 것이다</p>
<p>image를 gird로 나누어서 4개의 좌표와 confidence score를 예측한다.</p>
<p>각각의 task보다 Instance segmentation과 Panoptic segmentation</p>
<h1 id="Instance-Segmentation"><a href="#Instance-Segmentation" class="headerlink" title="Instance Segmentation"></a>Instance Segmentation</h1><p>Instance segmentation = Sementic segmentation + distuguishing instances</p>
<h2 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h2><ol>
<li><p>RPN</p>
<p>기존의 ROI 풀링은 정수좌표만 지원을 했었는데, interpolation을 위해서 소수점 pixel level을 지원하였다. </p>
</li>
</ol>
<h1 id="Panoptic-Segmentation"><a href="#Panoptic-Segmentation" class="headerlink" title="Panoptic Segmentation"></a>Panoptic Segmentation</h1><h2 id="UPSNet"><a href="#UPSNet" class="headerlink" title="UPSNet"></a>UPSNet</h2><p>FPN구조로 고해상도 feature를 뽑은 이후 Semantic Head 와 Instance Head로 나누어 predict를 하게 된다.</p>
<h1 id="Landmark-localization"><a href="#Landmark-localization" class="headerlink" title="Landmark localization"></a>Landmark localization</h1><p>Facial landmark localizaiton</p>
<p>Human pose estimation</p>
<p>다양한 data를 사용한 학습</p>
<h1 id="Multi-model-learning"><a href="#Multi-model-learning" class="headerlink" title="Multi-model learning"></a>Multi-model learning</h1><p>Challenges</p>
<ol>
<li>각각의 감각의 데이터가 모두 다른 representation을 띈다</li>
<li>Feature space에 대하여 balance가 맞지 않는다.</li>
<li>여러 modelity를 사용할 경우 특정 model에 bias될수 있다.</li>
</ol>
<p>대표적인 구조</p>
<ol>
<li>Matching</li>
<li>Translating</li>
<li>Referencing</li>
</ol>
<h2 id="Visual-data-amp-Text"><a href="#Visual-data-amp-Text" class="headerlink" title="Visual data &amp; Text"></a>Visual data &amp; Text</h2><p>Joint embedding</p>
<ol>
<li>Image tagging</li>
</ol>
<p>태그 -&gt; 이미지, 이미지 -&gt; 태그</p>
<p>각 feature들은 차원을 맞춰주고 이둘의 Joint embedding을 만들어준다.</p>
<p>같은 space에 이미지와 text를 embedding해주고 matching되는 image와 text 끼리 거리가 가까워 지게끔 학습을 진행한다.</p>
<p>Metric Learning</p>
<p><img src="/images/image-20210312111734568.png" alt="image-20210312111734568"></p>
<p><img src="/images/image-20210312144707419.png" alt="image-20210312144707419"></p>
<p>창 - 프로세스 (현재 진행중)<br>탭 - 쓰레드 (그냥 띄워진 창) </p>
<ol>
<li>쓰레드마다 갖는 메모리 공간 / 프로세스가 공유하는 메모리 공간이 있다.</li>
<li>프로세스가 늘어나면 쓰레드 공유 공간이 늘어나게 된다.</li>
</ol>
<p>process: 코어수에 따라 병렬처리 가능<br>thread: 프로세스 위에 올라가있는 task<br><del>보통 1개의 process로 concurrent로 처리하는 것 보다, max core의 50</del>70%정도로 process를 나눠서 처리해주는 게 훨씬 좋은 성능을 낸다.</p>
<p>—- ps. 파이썬은 멀티프로세싱 &gt;&gt; 멀티쓰레딩<br>search keyword: multi threading/processing, python global interpreter lock(GIL)</p>
<p>mini task) [멀티 프로세싱/쓰레딩] 으로 10만까지의 소수 찾고 성능 비교 후 github에 올리기 (~3.15 월)</p>
<p>—- <em>point</em> —-<br> 피어세션 뿐만 아니라, 앞으로 공부방향에 있어 수업 내용 외적으로 전체적인 그림을 그리며 공부를 이어나갈 것! (cs, ml pipeline 등…)</p>
<p> Q) ml 엔지니어라면, 검색을 했을 때 가장 좋은 결과를 내기 위해서는 어떻게 해야 할까요?<br> A) 어떤 것이랑 어떤 것을 연결시킬 건지…등등 잘 생각해보자! ^_^</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Image Classification</p><p><a href="https://jo-member.github.io/2021/03/08/2021-03-08-Boostcamp31.1/">https://jo-member.github.io/2021/03/08/2021-03-08-Boostcamp31.1/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>jo-member</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2021-03-08</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2021-07-12</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/CNN/">CNN</a><a class="link-muted mr-2" rel="tag" href="/tags/Vision/">Vision</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2021/04/22/Pstage2-KLUE/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Pstage2_KLUE</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2021/02/23/2021-02-23-Boostcamp22/"><span class="level-item">Graph2</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://jo-member.github.io/2021/03/08/2021-03-08-Boostcamp31.1/';
            this.page.identifier = '2021/03/08/2021-03-08-Boostcamp31.1/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'jo-member' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-3-tablet is-3-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><ul class="menu-list"><li><a class="level is-mobile" href="#ResNet"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">ResNet</span></span></a></li><li><a class="level is-mobile" href="#DenseNet"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">DenseNet</span></span></a></li></ul><li><a class="level is-mobile" href="#Semantic-segmentation"><span class="level-left"><span class="level-item">2</span><span class="level-item">Semantic segmentation</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Fully-Convolutional-Networks"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">Fully Convolutional Networks</span></span></a></li><li><a class="level is-mobile" href="#DeepLab"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">DeepLab</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Two-stage-detector"><span class="level-left"><span class="level-item">3</span><span class="level-item">Two-stage detector</span></span></a></li><li><a class="level is-mobile" href="#Single-stage-detector"><span class="level-left"><span class="level-item">4</span><span class="level-item">Single stage detector</span></span></a></li><li><a class="level is-mobile" href="#Instance-Segmentation"><span class="level-left"><span class="level-item">5</span><span class="level-item">Instance Segmentation</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Mask-R-CNN"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">Mask R-CNN</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Panoptic-Segmentation"><span class="level-left"><span class="level-item">6</span><span class="level-item">Panoptic Segmentation</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#UPSNet"><span class="level-left"><span class="level-item">6.1</span><span class="level-item">UPSNet</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Landmark-localization"><span class="level-left"><span class="level-item">7</span><span class="level-item">Landmark localization</span></span></a></li><li><a class="level is-mobile" href="#Multi-model-learning"><span class="level-left"><span class="level-item">8</span><span class="level-item">Multi-model learning</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Visual-data-amp-Text"><span class="level-left"><span class="level-item">8.1</span><span class="level-item">Visual data &amp; Text</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Boostcamp/"><span class="level-start"><span class="level-item">Boostcamp</span></span><span class="level-end"><span class="level-item tag">26</span></span></a></li><li><a class="level is-mobile" href="/categories/Competition/"><span class="level-start"><span class="level-item">Competition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Mathmatics-for-ML/"><span class="level-start"><span class="level-item">Mathmatics_for_ML</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/PaperReview/"><span class="level-start"><span class="level-item">PaperReview</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/blog/"><span class="level-start"><span class="level-item">blog</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-07-12T03:09:10.000Z">2021-07-12</time></p><p class="title"><a href="/2021/07/12/VQA/">VQA (Visual Question Answering)</a></p><p class="categories"><a href="/categories/Competition/">Competition</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-25T11:52:06.000Z">2021-04-25</time></p><p class="title"><a href="/2021/04/25/Pstage3-Image-Segmentation-Detection/">Pstage3_Image_Segmentation_Detection</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-22T06:29:38.000Z">2021-04-22</time></p><p class="title"><a href="/2021/04/22/Pstage2-KLUE/">Pstage2_KLUE</a></p><p class="categories"><a href="/categories/Boostcamp/">Boostcamp</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-03-07T15:00:00.000Z">2021-03-08</time></p><p class="title"><a href="/2021/03/08/2021-03-08-Boostcamp31.1/">Image Classification</a></p><p class="categories"><a href="/categories/Boostcamp/">Boostcamp</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-02-22T15:00:00.000Z">2021-02-23</time></p><p class="title"><a href="/2021/02/23/2021-02-23-Boostcamp22/">Graph2</a></p><p class="categories"><a href="/categories/Boostcamp/">Boostcamp</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="Jo Member" height="28"></a><p class="is-size-7"><span>&copy; 2021 jo-member</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>