<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Paper Review) Attention Is All You Need - Jo Member</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Jo Member"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Jo Member"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="AbstractSeuence transduction model들은 현재 복잡한 recurrent한 구조 (RNN) 이나 encoder decoder를 포함한 CNN이 주를 이룬다. 가장 좋은성능을 내는 model또한 attention mechanism을 이용하여 encoder와 decoder를 연결하는 형태이다. 이 논문에서는 새로운 방법인 Transf"><meta property="og:type" content="blog"><meta property="og:title" content="Paper Review) Attention Is All You Need"><meta property="og:url" content="https://jo-member.github.io/2021/02/05/2021-02-05-Attention/"><meta property="og:site_name" content="Jo Member"><meta property="og:description" content="AbstractSeuence transduction model들은 현재 복잡한 recurrent한 구조 (RNN) 이나 encoder decoder를 포함한 CNN이 주를 이룬다. 가장 좋은성능을 내는 model또한 attention mechanism을 이용하여 encoder와 decoder를 연결하는 형태이다. 이 논문에서는 새로운 방법인 Transf"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://jo-member.github.io/images/image-20210204114520892-1612513436394.png"><meta property="og:image" content="https://jo-member.github.io/images/image-20210205125403138.png"><meta property="og:image" content="https://jo-member.github.io/images/image-20210205131926013.png"><meta property="og:image" content="https://jo-member.github.io/images/image-20210205132622631.png"><meta property="og:image" content="https://jo-member.github.io/images/image-20210205134008271.png"><meta property="og:image" content="https://jo-member.github.io/images/image-20210205134243929.png"><meta property="og:image" content="https://jo-member.github.io/images/image-20210205132622631-1612515516800.png"><meta property="og:image" content="https://jo-member.github.io/images/image-20210205155145255.png"><meta property="og:image" content="https://jo-member.github.io/images/image-20210205094724396-1612516229839.png"><meta property="og:image" content="https://jo-member.github.io/images/image-20210205181144831.png"><meta property="og:image" content="https://jo-member.github.io/images/image-20210205181818372.png"><meta property="og:image" content="https://jo-member.github.io/images/image-20210205182950061.png"><meta property="og:image" content="https://jo-member.github.io/images/image-20210205184119740.png"><meta property="og:image" content="https://jo-member.github.io/images/image-20210205185431936.png"><meta property="article:published_time" content="2021-02-04T15:00:00.000Z"><meta property="article:modified_time" content="2021-04-22T04:25:19.368Z"><meta property="article:author" content="jo-member"><meta property="article:tag" content="AI, Deep_learning, python, nlp, cv"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/images/image-20210204114520892-1612513436394.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://jo-member.github.io/2021/02/05/2021-02-05-Attention/"},"headline":"Paper Review) Attention Is All You Need","image":["https://jo-member.github.io/images/image-20210204114520892-1612513436394.png","https://jo-member.github.io/images/image-20210205125403138.png","https://jo-member.github.io/images/image-20210205131926013.png","https://jo-member.github.io/images/image-20210205132622631.png","https://jo-member.github.io/images/image-20210205134008271.png","https://jo-member.github.io/images/image-20210205134243929.png","https://jo-member.github.io/images/image-20210205132622631-1612515516800.png","https://jo-member.github.io/images/image-20210205155145255.png","https://jo-member.github.io/images/image-20210205094724396-1612516229839.png","https://jo-member.github.io/images/image-20210205181144831.png","https://jo-member.github.io/images/image-20210205181818372.png","https://jo-member.github.io/images/image-20210205182950061.png","https://jo-member.github.io/images/image-20210205184119740.png","https://jo-member.github.io/images/image-20210205185431936.png"],"datePublished":"2021-02-04T15:00:00.000Z","dateModified":"2021-04-22T04:25:19.368Z","author":{"@type":"Person","name":"jo-member"},"description":"AbstractSeuence transduction model들은 현재 복잡한 recurrent한 구조 (RNN) 이나 encoder decoder를 포함한 CNN이 주를 이룬다. 가장 좋은성능을 내는 model또한 attention mechanism을 이용하여 encoder와 decoder를 연결하는 형태이다. 이 논문에서는 새로운 방법인 Transf"}</script><link rel="canonical" href="https://jo-member.github.io/2021/02/05/2021-02-05-Attention/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="Jo Member" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/jo-member"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-9-tablet is-9-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-02-04T15:00:00.000Z" title="2021. 2. 5. 오전 12:00:00">2021-02-05</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-04-22T04:25:19.368Z" title="2021. 4. 22. 오후 1:25:19">2021-04-22</time></span><span class="level-item"><a class="link-muted" href="/categories/PaperReview/">PaperReview</a></span></div></div><h1 class="title is-3 is-size-4-mobile">Paper Review) Attention Is All You Need</h1><div class="content"><br/>

<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Seuence transduction model들은 현재 복잡한 recurrent한 구조 (RNN) 이나 encoder decoder를 포함한 CNN이 주를 이룬다. 가장 좋은성능을 내는 model또한 attention mechanism을 이용하여 encoder와 decoder를 연결하는 형태이다.</p>
<p>이 논문에서는 새로운 방법인 Transformer를 제안</p>
<p>이는 오로지 attention mechanism만을 사용!</p>
<p>이는 RNN이나 CNN보다 더 <strong>병렬화가 가능하고 train하는데 적은 시간이 걸린다!</strong></p>
<p>WMT 2014 English to-German data를 사용하여 BLEU라는 score에서 28.4점을 얻었다.(여러 논문을 읽다보면 자주 등장하는 이 BLUE score은 정리해 놓은게 있는데 추후에 posting )</p>
<p>이는 앙상블을 포함한 이전의 가장 좋은 성능보다 2BLUE가 높다.</p>
<span id="more"></span>

<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><br/>

<p>RNN모델 (LSTM이나 GRU)는 machine translation과 같은 sequence modeling의 State of the art한(최신의 가장 좋은 성능의) 접근방식으로 알려져있다. RNN은 input과 output의 위치를 계산한 결과를 담고있다. 계산하는 시간이나 순서에 의해 정렬된 위치들은 이전의 hidden state h<del>t-1</del>로 표현된 연속적인 hidden state h<del>t</del>를 생성한다. 이것은 본질적으로 training examples의 병렬화를 배제하며, 이로인해 memory의 한계로 인한 batch size의 한계 때문에 긴 sequence length에 굉장히 critical한 요소로 작용한다. 최근의 연구들은 factorization과 conditional computation(이것에 대한 논문: Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks, Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer )을 이용한 계산과정의 효율화로 큰 발전을 이루어냈다. 하지만 순차적인 계산에 의한 제약은 아직 남아있다</p>
<p>Attention mechanism은 input과 out사이의 길이에 상관없이 dependencies를  modeling할수있다는 부분에서 sequence modeling과 transduction modeling의 필수적인 부분이 되었다. 하지만 몇몇 경우에서는 아직 attention mechanism과 RNN을 합쳐서 사용하고 있다.</p>
<p>이 연구에서는 Transformer라는 attention mechanism에만 의존하여 input과 output의 dependency를 이끌어내는 architecture을 제안한다. Transformer은 병렬화를 가능하게 하고, 성능을 더욱 향상시킬수 있다</p>
<br/>

<p>요약 : 우리의 transformer가 training example의 병렬화로 인한 속도 향상과 좋은 점수를 낸다.</p>
<br/>

<h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>encoder , decoder에대한 background</p>
<p>중간의 latent space는 input이나 output보다 훨씬 최소화된 vector이다. Sequential한 계산을 줄이는 목표는 기본적인 building block에서 CNN을 사용하여 병렬적으로 input과 output의 hidden representation을 계산하는 ByteNet이나 ConvS2S의 기반을 이루고있다. 위와 같은 model에서는 2개의 input이나 output의 길이가 증가할수록 계산량이 늘어난다.(ByteNet은 log적으로, ConvS2S는 linear하게). 이러한 결과는 거리에 따른 dependencies들을 학습하기에 더욱 어렵게 만든다. </p>
<br/>

<p>※ (input과 output사이의 길이가 길어지면 계산량이 증가해 서로의 연관관계를 학습하기가 어렵다는 뜻</p>
<p>cnn은 한번에 kernel size를 진짜 커봤자 최대 7x7을 쓰기 때문에 만약 input이 엄청 길다면 CNN연산시 계산량이 증가하게 되고 위치에 대한 정보의 일부만이 담기게됨. 예를 들어 간단하게 she is pretty and good at playing piano with her own piano와 같은 문장에서 뒤에 her과 처음 she는 긴 거리를 가지게 되어서 이 정보를 담기에 CNN은 부적절?).</p>
<br/>



<p>Transformer에서는 linear나 logmatric하게 계산량이 증가하지 않고 constant한 number로 증가한다.Attention-weigheted position의 <strong>평균</strong>을 사용하여  Effective한 **해상도?**가 감소함에도Multi-Head Attention과 상호작용 함으로서 계산량을 줄였다.</p>
<p>Self-attention은 서로 다른 position에 있는 sequence를 표현하기 위해 서로를 relating한다. </p>
<p>Self-attention은 reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representation과 같은 분야에서 성공적으로 사용되어져 왔다.</p>
<p>End-to-end memory network은 순서에 따라 정렬된 recurrence가 아닌 recurrent attention mechanism을 기반으로 하고있고, 좋은 성능을 보여주고 있다.</p>
<p>Transformer은 처음으로 RNN을 사용하지 않고 오로지 self-attention만이 쓰인 첫번째 변역 model이다.</p>
<br/>

<br/>

<p>요약 :  Sequence한 문제에서의 모델</p>
<p>​    RNN의 단점 : 병렬화의 어려움으로 인한 계산의 복잡도 증가, train 시간의 증가,</p>
<p>​    (그리고 강의에서 만약     sequential한 데이터중 중간에 어느 하나가빠진다면 해결하기가 어렵다고 했다)</p>
<p>​    CNN의 단점 : 병렬적인 계산은 이루어 지지만, input이나 output의 길이가 증가할수록 계산도 많고 단어간의 관    계파악이 빡셈</p>
<p>​    따라서 Attention만 쓴 Transformer 짱</p>
<p><br/><br/></p>
<h2 id="3-Model-Atchitecture"><a href="#3-Model-Atchitecture" class="headerlink" title="3. Model Atchitecture"></a>3. Model Atchitecture</h2><p>가장 경쟁력이 좋은 neural sequence transduction model은 encoder-decoder 구조를 가지고 있다. Encoder은 입력 sequence를 x = (x<del>1</del>,….x<del>n</del>)으로 표현하였고 이를 z = (z<del>1</del>,….z<del>n</del>)으로 map한다. 주어진 z로 decoder가 output sequence인 (y<del>1</del>,…,y<del>m</del>)을 생성해 낸다.(보통 중간의 latent 층은 input과 output에 비해 작은 dimension을 가진다고 조교님께서 설명) 각 step마다 model은 <strong>auto-regressive</strong>하며, 문장을 생성할때 이전에 생성된 symbol을 additional input으로 가정한다.</p>
<p><br/><br/></p>
<br/>

<p>※ Auto regressive 복습</p>
<p>(고정된 길이인 $\tau$만큼의 시퀀스만 활용하는 경우 Autoregressive Model(자기회귀모델)이라고 부른다</p>
<p>직전과거의 정보랑 직전정보가 아닌 정보들을 H<del>t</del>로 묶어서 활용)</p>
<p><img src="/images/image-20210204114520892-1612513436394.png" alt="image-20210204114520892"></p>
<p><br/><br/></p>
<p><br/><br/></p>
<p>Transformer은 stacked된 self-attention을 사용하고 있고, encoder와 decoder부분에 모두 fully connected layer를 삽입하였다.</p>
<p><img src="/images/image-20210205125403138.png" alt="image-20210205125403138"></p>
<br/>

<h3 id="3-1-Encoder-and-Decoder-Stacks"><a href="#3-1-Encoder-and-Decoder-Stacks" class="headerlink" title="3.1 Encoder and Decoder Stacks"></a>3.1 Encoder and Decoder Stacks</h3><p><strong>Encoder</strong> : encoder은 N=6 (6개)인 각각의 identical한 layer들이 층층이 쌓여있다. 각각의 layer들은 2개의 sub-layer로 구성되어있다. 첫번째는 Multi-Head Attention이고, 두번째는 간단한 fully-connected된 feed-forward network이다.</p>
<p>우리는 각각의 sublayer에 <strong>residual connection</strong>을 들어 주었다.</p>
<br/>

<p>※여기서 과연 residual connection을 넣은 이유가 뭘까? overfitting 방지 like ResNet??</p>
<br/>

<p>각각의 sub-layer의 output은 <em>LayerNorm(x + Sublayer(x))</em>. 모든 sublayer model과 embedding layer의 output의 차원은 d<del>model</del> = 512 이다.</p>
<br/>

<p><strong>Decoder</strong> : Decoder또한 N=6인 각각의 identical한 layer들이 층층이 쌓여있다. Encoder의 2개의 각 sub-layer에 Decoder은 <strong>encoder stack의 output에 대한 multi-Head attention</strong>을 수행하는 층이 추가가 되었다.</p>
<p>Encoder와 같이 sublayer에 residual connection을 만들어 주었다. 하위 position이 attend하는것을 방지하기 위해 self-attention-layer을 약간 수정하였다**(이게 masking). 이 masking은 i위치의 예측이 i보다 과거의 것으로만 구해지게 하기 위함이다.**</p>
<br/>

<p>※ Masking은 NLP 문제에서 굉장히 많이 쓰인다고 한다. 알아두자</p>
<br/>



<br/>

<h3 id="3-2-Attention"><a href="#3-2-Attention" class="headerlink" title="3.2 Attention"></a>3.2 Attention</h3><p>Attention function은 query와 set of key-value pair들을 output에 mapping하는 함수이다. (query,key,value,output은 모두 vector). Output은 value들의 weighted sum으로 계산하며, 이 weight는 query와 다른모든 key값들의 <strong>compatibility function</strong>으로 정해진다.</p>
<p>※여기서 compatibility function이란?</p>
<p>뒤에서 sum의 형태와 dot product로 나누어 진다. 이들의 차이점은 뒤에 기술</p>
<p><img src="/images/image-20210205131926013.png" alt="image-20210205131926013"></p>
<br/>

<h4 id="3-2-1-Scaled-Dot-Product-Attention"><a href="#3-2-1-Scaled-Dot-Product-Attention" class="headerlink" title="3.2.1 Scaled Dot-Product Attention"></a>3.2.1 Scaled Dot-Product Attention</h4><p>우리는 이러한 attention을 “Scaled Dot-Product Attention”이라고 부른다. Input은 d<del>k</del>의 차원을 가지는 keys와 queries(둘은 연산(내적)을 위해 같은 차원을 가진다)와  d<del>v</del>의 차원을 가지는 values로 이루어져 있다. 우리는 하나의 query 를 다은 모든 keys들과 내적하고(이 결과 값이 바로 강의에서 score) sqrt(d<del>k</del>)로 나누어 준다. 이후 softmax function을 적용하여 value의 weight를 얻어낸다.</p>
<p>※ i번째 단어에 대한 score vector 계산시 i의 쿼리 vector와 다른모든 key vectors 사이의 내적 (Matmul)</p>
<p>위의 과정들을 queries들을 Q matrix, keys and values를 각각 K and V라고 한다면 아래의 식으로 표현가능</p>
<p><img src="/images/image-20210205132622631.png" alt="image-20210205132622631"></p>
<p>가장 많이쓰는 attention function의 함수는 additive attention과 위와 같은 dot-product attention이다. 우리는 dot-product attention을 썼다. </p>
<p>Additive attention은 하나의 hidden layer와 feed forward network를 사용하여compatibility function을 계산한다. 이두가지는 복잡도 측면에서 비슷하지만, dot-product attention이 더빠르고 공간 절약적이다.(이유는 행렬의 계산으로 표현가능)</p>
<p>작은 값을 가지는 d<del>k</del>에서의  두 mechanism은 유사하겠지만, 큰 d<del>k</del>로 나누어 주지 않으면 additive attention이 dot-product의 성능을 넘는다. 큰  d<del>k</del>는  dot -product는 큰값을 가지게 되고, 이는 softmax function이 매우 작은 gradient를 가지게 한다. 이러한 영향을 줄이기 위해 sqrt(d<del>k</del>)로 나누어 주었다. (scale)</p>
<br/>

<h4 id="3-2-2-Multi-Head-Attention"><a href="#3-2-2-Multi-Head-Attention" class="headerlink" title="3.2.2 Multi-Head Attention"></a>3.2.2 Multi-Head Attention</h4><p>d<del>model</del>(max sequence)의 차원을 가지는 keys,values,queries으로 이루어진 single attention을 수행하는것이 아니라, 우리는 queries, keys, values들에 각각 h번 d<del>k</del>,d<del>k</del>,d<del>v</del>를 곱하여 project한값이 더욱 좋은것을 알아내었다.</p>
<p>이러한 <strong>projection</strong>을 거치면 attention function을 병렬적으로 수행 할 수있으며, d<del>v</del>의 차원을 가지는 output을 얻어낼 수 있다. 이 output은 다시 하나로 concatnated되어 projected된다.</p>
<p><img src="/images/image-20210205134008271.png" alt="image-20210205134008271"></p>
<p>Multi-Head Attention은 서로다른 위치에서의 서로다른 subspace의 표현을 jointly attend 하게 한다.</p>
<p><img src="/images/image-20210205134243929.png" alt="image-20210205134243929"></p>
<p>우리는 h = 8개의 parallel attention layer을 사용하였고,d<del>k</del>,d<del>v</del>,d<del>model</del>/h = 64</p>
<p><strong>각각의 head에서 차원을 줄임으로서 전체적인 계싼비용이 single head attention과 비슷하게 만들었다???????</strong></p>
<p>※ 한마디로 이제 d<del>model</del>의 차원을 h만큼 parrel layer에 나누어서 넣었으니 결국 single head attention과 비슷하다는 이야기인가??</p>
<br/>

<h4 id="3-2-3-Applications-of-Attention-in-our-Model"><a href="#3-2-3-Applications-of-Attention-in-our-Model" class="headerlink" title="3.2.3 Applications of Attention in our Model"></a>3.2.3 Applications of Attention in our Model</h4><p>Transformer은 multi-head attention을 3가지 다른 방식으로 사용하고 있다</p>
<ol>
<li><p>“Encoder - Decoder attention” layer에서 queries는 이전의 decoder layer에서 오고, encoder의 output에서 오는 key와 value들을 저장한다. 이것은 input sequence의 모든 position들을 모든 position의 decoder가 attend 하게 해준다.</p>
<p>한마디로 decoder을 쿼리만 들고있어도 된다.</p>
<br/></li>
<li><p>Self attention layer를 포함하는 encoder. Self attention layer에서는 이전 encoder의 layer의 결과에서부터 나온 위치와 같은 위치에서  모든 key, values, and queries가 나온다. encoder속의 각각의 위치들은  이전 encoder의 이전 layer의 모든 위치에 집중한다. (모든 현재 layer의 위치가 이전 layer의 모든 position 정보들을 가진다? 이런느낌?)</p>
<br/></li>
<li><p>비슷하게 decoder의 self attention layer또한 모든  decoder안의 모든(자기자신까지) position에 집중한다.<strong>Auto regressive 특성을 보존시키기 위해 왼쪽의 정보들이 decoder로 flow in 하는걸 막아주어야 한다. 우리는 이러한 걸 scaled dot product attention안의 softmax의 output 값에masking out함으로서 해결한다</strong></p>
<p>이걸 다시한번 생각해 보아야 겠다</p>
<p>Auto regressive한 특성이란 이전의 정보들 만으로 현재값을 도출해내는 특성</p>
<p>그니까 결국은 위에(1)식에서 softmax의 결과값에 미래의 정보들은 모두 masking 해준다는것이다.</p>
<p>그니까 결국엔 <img src="/images/image-20210205132622631-1612515516800.png" alt="image-20210205132622631"> </p>
<p>이식에서 미래의 정보들까지 K와 Q의 내적결과가 다담고 있으니까 미래의 정보는 마스킹 해준다.</p>
<br/></li>
</ol>
<h3 id="3-3-Position-wise-Feed-Forward-Networks"><a href="#3-3-Position-wise-Feed-Forward-Networks" class="headerlink" title="3.3 Position-wise Feed Forward Networks"></a>3.3 Position-wise Feed Forward Networks</h3><p>encoder와 decoder안의 각 layer에는 fully connected feed forward network를 가져야 한다.  이 <strong>fully connected feed forward network는 각각의 위치에 독립적으로 따로 적용된다</strong></p>
<p><img src="/images/image-20210205155145255.png" alt="image-20210205155145255"></p>
<p>위식을 보면 2개의 linear transformation과 ReLU activation을 그사이에 사용하였다.</p>
<p><strong>Linear transformation을 각각의 position에 같은 걸 적용해야 한다. 그리고 layer과 layer사이에는 다른 parameter를 적용해야 한다. 또다른 방법은 1의 kernel size를 가지는 2개의 convoltion을 사용하는 것이다.</strong></p>
<p>※ 그니까 하나의 layer에는 같은 weight를 적용하고 다른 layer사이에는 다른 weight를 적용한다는 뜻???</p>
<br/>

<p>d<del>model</del> = 512</p>
<p>inner-layer’s dimension d<del>ff</del> = 2048</p>
<br/>

<h3 id="3-4-Embedding-and-Softmax"><a href="#3-4-Embedding-and-Softmax" class="headerlink" title="3.4 Embedding and Softmax"></a>3.4 Embedding and Softmax</h3><p>여타 다른 번역 모델과 같이 여기서도 학습된 embadding을 사용했다.</p>
<p>Decode되어 나온결과도 학습된 linear transformation과 softmax함수를 사용하여 다음 token의 확률을 계산하였다.</p>
<p>embedding layer들에는 같은 weight와 presoftmax linear transformation을 사용, weight의 결과에 sqrt(d<del>model</del>)을 사용했다.</p>
<br/>

<h3 id="3-5-Positional-Encoding"><a href="#3-5-Positional-Encoding" class="headerlink" title="3.5 Positional Encoding"></a>3.5 Positional Encoding</h3><p>우리의 model은 recurrence도 없고 convolution도 없어서 각각의 model이 sequence의 순서를 사용하게 하여면 position 정보를 삽입해 주어야 한다.</p>
<p>따라서 </p>
<p><img src="/images/image-20210205094724396-1612516229839.png" alt="image-20210205094724396"></p>
<p>이와 같이 각각 embedding된 vector에 positional encoding된 vector를 더해준다</p>
<p>이 연구에서는 cos과 sin 함수를 사용했다</p>
<p><img src="/images/image-20210205181144831.png" alt="image-20210205181144831"></p>
<p>pos 는 position i는 차원</p>
<p>각각의 위치가 sinusodial하게 encoding 되도혹 하였다. 주기가 조올라 길어서 다른위친데 주기성 때문에 같은 값을 가지는 경우는 드물다</p>
<br/>

<h2 id="4-Why-Self-Attention"><a href="#4-Why-Self-Attention" class="headerlink" title="4. Why Self-Attention"></a>4. Why Self-Attention</h2><p>이 부분에서는 self attention layer를 RNN과 CNN에 더욱 자세히 비교한다.</p>
<p>앞에서 설명한거에 대한 보충설명</p>
<ol>
<li><p>한 layer에서 계산 복잡도에서의 이득</p>
</li>
<li><p>병렬화 될 수 있는계산의 총량</p>
</li>
<li><p>길이가 기이이일어졌을때 얼마나 network에 영향을 미치는지</p>
<p>길이가 졸라리 길어졌을때 dependencies는 매우 중요하다. 이에 가장 중요하게 미치는 영향중 하나가 signal하나가 network를 순회하는 길이이다?? 이게 짧아질수록 긴 길이에 대한 상호적인 관계가 더 잘 학습된다. 따라서 각 model마다 model에서 input과 output 사이의 거리? 뭐하이튼 그 얼마나 model이 compact한가</p>
</li>
</ol>
<p><img src="/images/image-20210205181818372.png" alt="image-20210205181818372"></p>
<p>위 table을 보면 computational한 성능을 높이기 위해 self attention 모델은 해당하는 output 위치의 오직 size r 만큼의 input sequence 주위를 고려하도록 제한되어있다.</p>
<p>이거에 대한 연구는 추후에 발표하겠다고 적혀있다. 그럼 이미 나와있겠지?</p>
<p>하이튼 위에꺼 비교해보면 모든 측면에서 self attention이 와따</p>
<br/>

<h2 id="5-Training"><a href="#5-Training" class="headerlink" title="5. Training"></a>5. Training</h2><h3 id="1-Training-data-and-batching"><a href="#1-Training-data-and-batching" class="headerlink" title="1. Training data and batching"></a>1. Training data and batching</h3><p>WMT 2014를 사용하여 train함</p>
<p>그리고 문장은 target vocab와 37000개를 공유하는 byte-pair encoding이라는 방식을 사용했음</p>
<p>각 traning batch는 25000개의 source token과 25000개의 target token을 포함하는 문장의 set으로 정해주었다.</p>
<h3 id="2-Optimizer"><a href="#2-Optimizer" class="headerlink" title="2. Optimizer"></a>2. Optimizer</h3><p>Adam을 썼고, lr을 단계적으로 변화시켰다.아래와 같은 수식으로</p>
<p><img src="/images/image-20210205182950061.png" alt="image-20210205182950061"></p>
<br/>

<h3 id="3-Regularization"><a href="#3-Regularization" class="headerlink" title="3. Regularization"></a>3. Regularization</h3><ol>
<li><p>Residual Dropout</p>
<p>더해지고 normalized 되기전에 각각의 sublayer의 output에 dropout을 적용하였다</p>
<p>그리고 또 embedding된 vector와 position의 합이후에도 적용하였다</p>
<p>P<del>drop</del>은 0.1 </p>
</li>
<li><p>Label Smoothing???</p>
<p>이런걸 적용했다고 하느네 이게 약간 예측불가능한걸 더해줘서 model이 더 새로운것을 배우게끔하는 거라하는데 걍 간단하게 나와있다</p>
<br/></li>
</ol>
<h2 id="6-Results"><a href="#6-Results" class="headerlink" title="6. Results"></a>6. Results</h2><p>BLEU score 잘나왔다 어쩌구 저쩌구 하다가</p>
<p>base model에서는 5개의 checkpoint를 만들어서 그것의 평균을 낸 하나의 model을 썼고, 각 check point는 10분마다 한번씩 interval을 주었다.</p>
<p>이게 내가 조교님한테 질문했던 부분과 좀 연관성이 있다. 이렇게 중간중간에 model을 기록하고 평균을 내는 방식도 있구나</p>
<p>그리고 이 beam search를 사용하였다고 한다</p>
<p>이건 이전의 논문들을 읽을때도 자주 사용했던 기법이다</p>
<p>간단하게 설명하면 가장 확률이 높은 K개을 선택하며 진행하는 것이다</p>
<p>greedy방법보다 효율적이고 score가 잘나온다고 들었다</p>
<h3 id="6-2-Model-Variations"><a href="#6-2-Model-Variations" class="headerlink" title="6.2 Model Variations"></a>6.2 Model Variations</h3><p>아래 table의 (A)를 보면 attention의 head의 개수와 key,value의 dimension을 변화시켜주었다. 너무 많은 head를 사용해도 안되고 하나만 사용해도 안됨</p>
<p>이건 너무 당연한거다 뭐든지 적당한게 좋다</p>
<p><img src="/images/image-20210205184119740.png" alt="image-20210205184119740"></p>
<h3 id="6-3-English-Constituency-Parsing"><a href="#6-3-English-Constituency-Parsing" class="headerlink" title="6.3 English Constituency Parsing"></a>6.3 English Constituency Parsing</h3><p>이 Transformer를 활용한 model은 통역에서 나아가서 영어 구문을 분석해주는 방법으로 발전시켜나가야 한다. </p>
<p>이건 별로 중요하지 않은것 같다</p>
<p>해보니까 RNN보다 좋은 성능을 나타내었다 끝</p>
<h2 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7. Conclusion"></a>7. Conclusion</h2><p>결론 </p>
<p>기존과 다르게 attention에만 기반을 둔 multi-headed self attention을 사용한 이 transformer은 다른 RNN이나 CNN보다 성능이 빠르며 이 Transfomer를 더욱큰 input과 output을 가지는 image나 비디오 오디오 등에적용시키는 것을 기대하고 있다.</p>
<p>Transformer 짱짱맨</p>
<p>그리고 조교님께서 관심 있으신 Neuroscience와 Attention사이의 관련</p>
<p><img src="/images/image-20210205185431936.png" alt="image-20210205185431936"></p>
<p>보면 우리 인간의 황반에서도 이 attention의 개념을 적용해서 사물을 인지하고 있으니, 잘되는게 어찌보면 당연하다</p>
<p>출처 : <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a></p>
<p>그리고 naver boostcamp</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Paper Review) Attention Is All You Need</p><p><a href="https://jo-member.github.io/2021/02/05/2021-02-05-Attention/">https://jo-member.github.io/2021/02/05/2021-02-05-Attention/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>jo-member</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2021-02-05</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2021-04-22</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2021/02/05/2021-02-05-Boostcamp15.1/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Day15) Generative Models</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2021/02/04/2021-02-04-Boostcamp14.1/"><span class="level-item">Day14) RNN1</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://jo-member.github.io/2021/02/05/2021-02-05-Attention/';
            this.page.identifier = '2021/02/05/2021-02-05-Attention/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'jo-member' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-3-tablet is-3-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Abstract"><span class="level-left"><span class="level-item">1</span><span class="level-item">Abstract</span></span></a></li><li><a class="level is-mobile" href="#1-Introduction"><span class="level-left"><span class="level-item">2</span><span class="level-item">1. Introduction</span></span></a></li><li><a class="level is-mobile" href="#2-Background"><span class="level-left"><span class="level-item">3</span><span class="level-item">2. Background</span></span></a></li><li><a class="level is-mobile" href="#3-Model-Atchitecture"><span class="level-left"><span class="level-item">4</span><span class="level-item">3. Model Atchitecture</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#3-1-Encoder-and-Decoder-Stacks"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">3.1 Encoder and Decoder Stacks</span></span></a></li><li><a class="level is-mobile" href="#3-2-Attention"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">3.2 Attention</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#3-2-1-Scaled-Dot-Product-Attention"><span class="level-left"><span class="level-item">4.2.1</span><span class="level-item">3.2.1 Scaled Dot-Product Attention</span></span></a></li><li><a class="level is-mobile" href="#3-2-2-Multi-Head-Attention"><span class="level-left"><span class="level-item">4.2.2</span><span class="level-item">3.2.2 Multi-Head Attention</span></span></a></li><li><a class="level is-mobile" href="#3-2-3-Applications-of-Attention-in-our-Model"><span class="level-left"><span class="level-item">4.2.3</span><span class="level-item">3.2.3 Applications of Attention in our Model</span></span></a></li></ul></li><li><a class="level is-mobile" href="#3-3-Position-wise-Feed-Forward-Networks"><span class="level-left"><span class="level-item">4.3</span><span class="level-item">3.3 Position-wise Feed Forward Networks</span></span></a></li><li><a class="level is-mobile" href="#3-4-Embedding-and-Softmax"><span class="level-left"><span class="level-item">4.4</span><span class="level-item">3.4 Embedding and Softmax</span></span></a></li><li><a class="level is-mobile" href="#3-5-Positional-Encoding"><span class="level-left"><span class="level-item">4.5</span><span class="level-item">3.5 Positional Encoding</span></span></a></li></ul></li><li><a class="level is-mobile" href="#4-Why-Self-Attention"><span class="level-left"><span class="level-item">5</span><span class="level-item">4. Why Self-Attention</span></span></a></li><li><a class="level is-mobile" href="#5-Training"><span class="level-left"><span class="level-item">6</span><span class="level-item">5. Training</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-Training-data-and-batching"><span class="level-left"><span class="level-item">6.1</span><span class="level-item">1. Training data and batching</span></span></a></li><li><a class="level is-mobile" href="#2-Optimizer"><span class="level-left"><span class="level-item">6.2</span><span class="level-item">2. Optimizer</span></span></a></li><li><a class="level is-mobile" href="#3-Regularization"><span class="level-left"><span class="level-item">6.3</span><span class="level-item">3. Regularization</span></span></a></li></ul></li><li><a class="level is-mobile" href="#6-Results"><span class="level-left"><span class="level-item">7</span><span class="level-item">6. Results</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#6-2-Model-Variations"><span class="level-left"><span class="level-item">7.1</span><span class="level-item">6.2 Model Variations</span></span></a></li><li><a class="level is-mobile" href="#6-3-English-Constituency-Parsing"><span class="level-left"><span class="level-item">7.2</span><span class="level-item">6.3 English Constituency Parsing</span></span></a></li></ul></li><li><a class="level is-mobile" href="#7-Conclusion"><span class="level-left"><span class="level-item">8</span><span class="level-item">7. Conclusion</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/Boostcamp/"><span class="level-start"><span class="level-item">Boostcamp</span></span><span class="level-end"><span class="level-item tag">25</span></span></a></li><li><a class="level is-mobile" href="/categories/Further-Q/"><span class="level-start"><span class="level-item">Further_Q</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Mathmatics-for-ML/"><span class="level-start"><span class="level-item">Mathmatics_for_ML</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/PaperReview/"><span class="level-start"><span class="level-item">PaperReview</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/blog/"><span class="level-start"><span class="level-item">blog</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-04-22T06:29:38.000Z">2021-04-22</time></p><p class="title"><a href="/2021/04/22/Pstage2-KLUE/">Pstage2_KLUE</a></p><p class="categories"><a href="/categories/Boostcamp/">Boostcamp</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-03-07T15:00:00.000Z">2021-03-08</time></p><p class="title"><a href="/2021/03/08/2021-03-08-Boostcamp31.1/">Day31) Image Classification</a></p><p class="categories"><a href="/categories/Boostcamp/">Boostcamp</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-02-22T15:00:00.000Z">2021-02-23</time></p><p class="title"><a href="/2021/02/23/2021-02-23-Boostcamp22/">Day22) Graph2</a></p><p class="categories"><a href="/categories/Boostcamp/">Boostcamp</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-02-21T15:00:00.000Z">2021-02-22</time></p><p class="title"><a href="/2021/02/22/2021-02-22-Boostcamp21.1/">Day21) Graph</a></p><p class="categories"><a href="/categories/Boostcamp/">Boostcamp</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-02-17T15:00:00.000Z">2021-02-18</time></p><p class="title"><a href="/2021/02/18/2021-02-18-Boostcamp19.1/">Day16) Transformer</a></p><p class="categories"><a href="/categories/Boostcamp/">Boostcamp</a></p></div></article></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="Jo Member" height="28"></a><p class="is-size-7"><span>&copy; 2021 jo-member</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>