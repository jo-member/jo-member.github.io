<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Jo Member</title>
    <link>https://jo-member.github.io/</link>
    
    <atom:link href="https://jo-member.github.io/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>끄적끄적</description>
    <pubDate>Mon, 12 Jul 2021 10:38:23 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>pstage4</title>
      <link>https://jo-member.github.io/2021/07/12/pstage4/</link>
      <guid>https://jo-member.github.io/2021/07/12/pstage4/</guid>
      <pubDate>Mon, 12 Jul 2021 10:30:08 GMT</pubDate>
      
      <description>&lt;h1 id=&quot;Pstage4-수식인식&quot;&gt;&lt;a href=&quot;#Pstage4-수식인식&quot; class=&quot;headerlink&quot; title=&quot;Pstage4_수식인식&quot;&gt;&lt;/a&gt;Pstage4_수식인식&lt;/h1&gt;&lt;h3 id=&quot;Github-Repository&quot;&gt;&lt;a href=&quot;#Github-Repository&quot; class=&quot;headerlink&quot; title=&quot;Github Repository&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/bcaitech1/p4-ocr-hansarang&quot;&gt;Github Repository&lt;/a&gt;&lt;/h3&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="Pstage4-수식인식"><a href="#Pstage4-수식인식" class="headerlink" title="Pstage4_수식인식"></a>Pstage4_수식인식</h1><h3 id="Github-Repository"><a href="#Github-Repository" class="headerlink" title="Github Repository"></a><a href="https://github.com/bcaitech1/p4-ocr-hansarang">Github Repository</a></h3><span id="more"></span><p><strong>팀 소개</strong></p><h2 id="🎸-조원"><a href="#🎸-조원" class="headerlink" title="🎸 조원                                                                                                           "></a>🎸 조원                                                                                                           <a href="https://github.com/jo-member"><git></a></h2><ul><li><p>SATRN (Locality Feedforward, Shallow CNN) 구현</p></li><li><p>Augmentation (Resize &amp; Noramlization&amp; pixel 평균에 따른 이진화…) 제안 &amp; 실험</p></li><li><p>Ensemble (soft voting) 구현 &amp; 실험</p></li><li><p>teacherforcing ratio scheduling 실험</p></li><li><p>Beam search 구현</p></li><li><p>부족한 token에 대한추가 Data 생성</p></li><li><p><a href="http://boostcamp.stages.ai/competitions/43/discussion/post/363">BERT를 사용한 misspelled된 수식 잡아내기 제안</a></p></li><li><p>CSTR 논문 reading &amp; 공유</p></li></ul><p><strong>협업</strong></p><ul><li><em><strong>Git의 Discussion, Pull &amp; Request, Wiki를 활용하여 토론, 자료, 결과공유</strong></em></li><li><em><strong>Wandb를 사용하여 실험공유</strong></em></li></ul><p><strong>수식인식 Competition Overall</strong></p><ul><li><p><em><strong>상세개요</strong></em></p><p>수식 이미지를 latex 포맷의 텍스트로 변환하는 문제입니다. 수식은 여러 자연과학 분야에서 어려운 개념들은 간단하고 간결하게 표현하는 방법으로서 널리 사용되어 왔습니다. Latex 또한 여러 과학 분야에서 사용되는 논문 및 기술 문서 작성 포맷으로서 현재까지도 널리 사용되고 있습니다.</p><p><img src="/images/Untitled%206.png"></p><p>일반적인 OCR과 달리 분수, 시그마, 극한과 같은 표현을 인식하기 위해 multi line recognition을 특징으로 가집니다.</p></li><li><p><em><strong>Dataset</strong></em></p><p>Scale : 각각의 image scale은 제각각</p><p>Label : Latex 형식의 수식</p><ul><li><p>Train Data</p><ul><li>Hand Written Data : 50000</li><li>Printed Data            : 50000</li></ul></li><li><p>Evaluation Data</p><ul><li>Public : 6000</li><li>Private : 6000</li></ul></li><li><p>Prediction</p><ul><li>241개의 token class중 하나를 생성한 뒤 이들의 sequence</li></ul></li></ul></li><li><p><em><strong>평가 Metric</strong></em></p><h3 id="0-9-x-“Sentence-Accuracy”-0-1-x-1-“WER”"><a href="#0-9-x-“Sentence-Accuracy”-0-1-x-1-“WER”" class="headerlink" title="0.9 x “Sentence Accuracy” + 0.1 x (1-“WER”)"></a><strong>0.9 x “Sentence Accuracy” + 0.1 x (1-“WER”)</strong></h3><ul><li><p>Sentence Accuracy</p><p>전체 추론 결과(수식) 중에서 몇 개의 수식이 정답(ground truth)과 정확히 일치하는 지</p><p><img src="/images/Untitled%207.png"></p></li><li><p>WER (Word Error Rate)</p><p>단어 단위로 삽입(insertion), 삭제(deletion), 대체(substitution)된 글자 개수를 계산</p><p><img src="/images/Untitled%208.png"></p></li></ul></li></ul><p><strong>Problem &amp; Solving</strong></p><h2 id="EDA"><a href="#EDA" class="headerlink" title="EDA"></a>EDA</h2><ul><li><p><em><strong>부족한 token data</strong></em></p><p><img src="/images/Untitled%209.png"></p><p>Train data에 등장하는 token들중 빈도수가 가장적은 50개를 뽑아보았다.</p><p>10회 미만으로 등장한 token들에 대한 추가적인 data가 필요하다고 판단되었다. </p><p><a href="http://www.hostmath.com/">사이트</a>를 활용하여 부족한 token들을 포함하는 수식을 train파일에 50개 정도의 data를 추가해주었다.</p><p>train_100003.jpg    \ominus \vdots \nexists \rightleftarrows A \supsetneq B</p><p><img src="/images/Untitled%2010.png"></p></li><li><p><em><strong>세로형태의 수식 이미지</strong></em></p><p><img src="/images/Untitled%2011.png"></p></li><li><p><em><strong>다양한 Aspect Ratio</strong></em></p><p><img src="/images/Untitled%2012.png"></p></li><li><p><em><strong>잘못된 labeling</strong></em></p><p>P = 1이 아닌</p><p>P = 으로 labeling 되어있음</p><p><img src="/images/Untitled%2013.png"></p></li><li><p><em><strong>선이 그어져 있거나 형광팬이 칠해져 있는 case</strong></em></p><p><img src="/images/Untitled%2014.png"></p></li></ul><h2 id="Augmentation"><a href="#Augmentation" class="headerlink" title="Augmentation"></a>Augmentation</h2><ul><li><p><em><strong>Resize</strong></em></p></li><li><p><em><strong>Rotate</strong></em></p><p>OCR task에서 세로로 되어진 image들은 모두 noise로 작용</p><p>EDA에서 적당한 Aspect ratio의 threshold를 찾은 뒤, threshold 미만인 data들에 한하여 Rotate</p></li><li><p><em><strong>Normalization</strong></em></p><p>이미지의 각 pixel  값들을 0-1의 값으로 normalize 시켜준다</p></li><li><p><em><strong>이진화 &amp; 가로선 제거</strong></em></p></li></ul><h2 id="Model-SATRN"><a href="#Model-SATRN" class="headerlink" title="Model (SATRN)"></a>Model (SATRN)</h2><ul><li><p><em><strong>Locality-aware feedforward layer</strong></em></p><p><img src="/images/Untitled%2015.png"></p><p>Base-line model에 구현된 Fully-connected feed forward에서 논문에서 제시하고 있는 Convolutuon feed forward로 교체</p></li><li><p><em><strong>Adaptive 2D Positional Encoding</strong></em></p><p><img src="/images/Untitled%2016.png"></p><p>Baseline에서 구현된 일반적인 2D positional encoding에서 논문에서 제시한 학습가능한 adaptive 2D positional encoding으로 변경</p></li><li><p><em><strong>Backbone</strong></em></p><ol><li><a href="https://arxiv.org/pdf/1905.11946.pdf">EfficientNet</a></li><li><a href="https://arxiv.org/pdf/1608.06993.pdf">DenseNet</a></li><li>ShallowCNN</li></ol></li><li><p><em><strong>Mini SATRN for fast experiment</strong></em></p><p>다양한 실험을 빠르게 진행하기 위해 SATRN의 size를 줄여서 Mini SATRN으로 다양한 실험을 진행</p></li><li><p><em><strong>SATRN의 layer parameter 수정</strong></em></p><ol><li><p>Increase number of Decoder Layer</p><p><img src="/images/2021-06-16__10.07.43.png"></p><p>mini model 기준, Decoder layer를 추가할수록 성능이 향상</p></li><li><p>Change Activation Function</p><p><img src="/images/2021-06-16__10.14.13.png"></p><p>ShallowCNN의 activation function을 ReLU대신 mish를 사용</p></li></ol></li></ul><h2 id="Post-Processing"><a href="#Post-Processing" class="headerlink" title="Post - Processing"></a>Post - Processing</h2><ul><li><p><em><strong>Beam search</strong></em></p><p>Token을 뽑을 때 argmax를 사용하여 하나의 token만을 뽑는게 아닌 각 step마다 beam size k 만큼의 token을 뽑아 최대한 적합한 sequence를 선택하려 시도함</p></li><li><p><em><strong>Ensemble</strong></em></p><p>다양한 augementation을 거친 model들을 한번에 불러와서 soft voting을 사용하여 ensemble</p><p>Baseline에는 빠져있는 torch.nograd로 memory 절약</p><p><img src="/images/Untitled%2017.png"></p></li><li><p><em><strong>Use Language Model (x)</strong></em></p><p><a href="http://boostcamp.stages.ai/competitions/43/discussion/post/363">관련 토론 link</a></p></li></ul><h2 id="OCR-Demo"><a href="#OCR-Demo" class="headerlink" title="OCR Demo"></a>OCR Demo</h2><p>데모페이지 주소 : <a href="http://35.74.99.158:8501/">http://35.74.99.158:8501/</a></p><p><img src="/images/2021-06-16__10.31.11.png"></p><p><strong>Paper Review</strong></p><p>프로젝트 진행을 위해 읽은 논문의 목록은 다음과 같음</p><p><a href="/images/CSTR.pdf">CSTR.pdf</a></p><p><a href="/images/Misspelling_Correction_with_Pre-trained_Contextual_Language_Model.pdf">Misspelling Correction with Pre-trained Contextual Language Model.pdf</a></p><p><a href="/images/CRNN__.pdf">CRNN.pdf</a></p><p><a href="/images/An_Attentional_Scene_Text_Recognizer_with_Flexible_Rectification.pdf">An_Attentional_Scene_Text_Recognizer_with_Flexible_Rectification.pdf</a></p><p><a href="/images/CBAM.pdf">CBAM.pdf</a></p><p><a href="/images/Towards_End-to-end_Text_Spotting_with_Convolutional_Recurrent_Neural_Networks.pdf">Towards End-to-end Text Spotting with Convolutional Recurrent Neural Networks.pdf</a></p><p><a href="/images/TextBoxes_A_Fast_Text_Detector_with_a_Single_Deep_Neural_Network.pdf">TextBoxes_A Fast Text Detector with a Single Deep Neural Network.pdf</a></p>]]></content:encoded>
      
      
      <category domain="https://jo-member.github.io/categories/Boostcamp/">Boostcamp</category>
      
      
      <category domain="https://jo-member.github.io/tags/Vision/">Vision</category>
      
      <category domain="https://jo-member.github.io/tags/NLP/">NLP</category>
      
      <category domain="https://jo-member.github.io/tags/OCR/">OCR</category>
      
      
      <comments>https://jo-member.github.io/2021/07/12/pstage4/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>swin_transformer</title>
      <link>https://jo-member.github.io/2021/07/12/swin-transformer/</link>
      <guid>https://jo-member.github.io/2021/07/12/swin-transformer/</guid>
      <pubDate>Mon, 12 Jul 2021 08:50:01 GMT</pubDate>
      
      
      
      
      
      
      <comments>https://jo-member.github.io/2021/07/12/swin-transformer/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>fpn</title>
      <link>https://jo-member.github.io/2021/07/12/fpn/</link>
      <guid>https://jo-member.github.io/2021/07/12/fpn/</guid>
      <pubDate>Mon, 12 Jul 2021 08:49:42 GMT</pubDate>
      
      
      
      
      
      
      <comments>https://jo-member.github.io/2021/07/12/fpn/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>cstr</title>
      <link>https://jo-member.github.io/2021/07/12/cstr/</link>
      <guid>https://jo-member.github.io/2021/07/12/cstr/</guid>
      <pubDate>Mon, 12 Jul 2021 08:49:30 GMT</pubDate>
      
      
      
      
      
      
      <comments>https://jo-member.github.io/2021/07/12/cstr/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>satrn</title>
      <link>https://jo-member.github.io/2021/07/12/satrn/</link>
      <guid>https://jo-member.github.io/2021/07/12/satrn/</guid>
      <pubDate>Mon, 12 Jul 2021 08:49:13 GMT</pubDate>
      
      
      
      
      
      
      <comments>https://jo-member.github.io/2021/07/12/satrn/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>VQA: Visual Question Answering vs Competition Baseline</title>
      <link>https://jo-member.github.io/2021/07/12/vqa_paper1/</link>
      <guid>https://jo-member.github.io/2021/07/12/vqa_paper1/</guid>
      <pubDate>Mon, 12 Jul 2021 05:42:40 GMT</pubDate>
      
      <description>&lt;p&gt;VQA task의 시초격인 논문이다.&lt;/p&gt;
&lt;p&gt;VQA challenge의 전반적인 개요와 dataset, Base model등을 다루고 있다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>VQA task의 시초격인 논문이다.</p><p>VQA challenge의 전반적인 개요와 dataset, Base model등을 다루고 있다.</p><span id="more"></span><p><img src="/images/image-20210712145742160.png" alt="image-20210712145742160"></p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><ul><li>VQA란 Vision, NLP, knowledge representation을 모두 접목시킨 multi-discipline task이다</li><li><img src="/images/image-20210712150251708.png" alt="image-20210712150251708" style="zoom:50%;" /></li><li>위와 같이 어떠한 image가 주어지고, image가 없으면 맞추기 힘든 질문들로 구성되어있다. </li><li>Answer의 유형에 따라 open-ended questions 과 multiple-choice task로 나누어 진다.</li><li>Open-ended question은 answer가 다양해 질 수 있지만, multiple-choice task의 경우에는 미리 정해진 answer list중 하나가 답안이여야 한다.</li><li>이 논문에서는 두가지의 answer유형  전부 다루고 있다. (우리의 VQA task는 multiple-choice task 만을 다루고 있기 때문에 open-ended 는생략)</li></ul><h2 id="2-Dataset"><a href="#2-Dataset" class="headerlink" title="2. Dataset"></a>2. Dataset</h2><h3 id="Image-data"><a href="#Image-data" class="headerlink" title="Image data"></a>Image data</h3><ol><li><p>MS COCO dataset</p><ul><li>Object Detection과 image captioning에서 사용되는 dataset</li><li>containing multiple objects and rich contextual information</li><li>123,287 training and validation images and 81,434 test images</li></ul></li><li><p>Abstract Scene dataset</p><ul><li>The dataset contains 20 “paperdoll” human models [2] spanning genders, races, and ages with 8 different expressions.</li><li>Paperdoll로 real한 상황을 표현함</li><li>we create a new abstract scenes dataset containing 50K scenes</li></ul></li></ol><h3 id="Question-data"><a href="#Question-data" class="headerlink" title="Question data"></a>Question data</h3><p>간단한 question이 아닌 복잡하고 어려운 question을 만들어내기 위해 </p><blockquote><p>“We have built a smart robot. It understands a lot about images. It can recognize and name all the objects, it knows where the objects are, it can recognize the scene (e.g., kitchen, beach), people’s expressions and poses, and properties of objects (e.g., color of objects, their texture). Your task is to stump this smart robot! Ask a question about this scene that this smart robot probably can not answer, but any human can easily answer while looking at the scene in the image.”</p></blockquote><p>이러한 요청을 주어 question을 만들어 내도록 하였다.</p><h3 id="Answer-data"><a href="#Answer-data" class="headerlink" title="Answer data"></a>Answer data</h3><p>18개의 선택지를 구성</p><ul><li><strong>Correct</strong> : The most common (out of ten) correct answer</li><li><strong>Plausible</strong> : To generate incorrect, but still plausible answers we ask three subjects to answer the questions without seeing the image</li><li><strong>Popular</strong> : These are the 10 most popular answers. For instance, these are “yes”, “no”, “2”, “1”, “white”, “3”, “red”, “blue”, “4”, “green” for real images</li><li><strong>Random</strong> : Correct answers from random questions in the dataset</li></ul><h2 id="3-Model"><a href="#3-Model" class="headerlink" title="3. Model"></a>3. Model</h2><p><img src="/images/image-20210712153623216.png" alt="image-20210712153623216"></p><p><img src="/images/image-20210712160208777.png" alt="image-20210712160208777"></p><p>VQA를 위한 Base model과 Competition에서 제공해준 Baseline의 구조도이다.</p><ol><li><p>Image channel</p><ul><li>Image로 부터 embedding vector를 뽑아내는 역할을 한다.<br>Pretrained된 VGGNet을 사용하였으면 기존 VGGNet의 마지막 Fully-Connected MLP의 layer에서 4096 dim으로 뽑아내었다.</li></ul><ul><li>인공지능 경진대회측에서 제공한 baseline에서는 pretrained된 Resnet-50을 사용하여 마지막 Fully-Connected MLP의 layer에서 768 dim으로 뽑아내고 있다. 이후 논문과 다르게 별도의 Fully-Connected layer를 통과시켜주지 않는다. 애초에 question channel과 dimension을 맞추어주었다. 마지막 target dimension은 83으로 train data의 label 개수이다.</li></ul></li><li><p>Question channel</p><ul><li>Question으로 부터 embedding vector를 뽑아내는 역할을 한다.<br>Each question word is encoded with 300-dim embedding by a fully-connected layer + tanh non-linearity which is then fed to the LSTM. Cell state, hidden state dim = 512<br>Concate last cell state and last hidden state representations 을 하여 1024의 차원을 만들어주었다.</li><li>인공지능 경진대회측에서 제공한 baseline에서는 huggingface를 사용하여 pretrained된 RoBERTa-base model을 통해 embedding vector를 뽑아 내었다.  뽑아낸 embedding vector의 dim이 768이라 Image channel의 last dimension또한 768로 맞춰준것이다. </li></ul></li><li><p>Multi-Layer Perceptron</p><ul><li><p>이부분은 둘의 구조가 동일하다. 각 channel에서 나온 embedding vector들을 element wise multiplication해준다.</p></li><li><p>이후 layer을 추가해주어 차원을 늘려준뒤, 마지막 layer에서 target label의 개수만큼의 dimension을 뽑아낸다.</p></li><li><p>이부분에서 궁금했던점은 각 channel의 embedding vector를 합치는 방법에 따른 성능의 차이였다.</p><ol><li>Concat</li><li>Element-wise Multiplication</li><li>Element-wise Add</li></ol><p>이와 관련된 논문을 찾아보았다.</p><p>Component Analysis for Visual Question Answering Architectures 이라는 논문에서 각 fusion 방식에 따른 성능을 실험해보았다.</p><img src="/images/image-20210712161607350.png" alt="image-20210712161607350" style="zoom:50%;" /><p>위 논문의 결과에 따라 3가지 fusion 방식중 Multiplication 방식을 계속해서 고수했다.</p><p>또한 위논문에서는 BERT를 사용하여 question문장을 embedding만 하고 GRU를 사용하여 최종 vector를 뽑아내고 있다. 이러한 방식도 시도해 볼만 할것 같다.</p></li></ul></li></ol><h2 id="4-Result"><a href="#4-Result" class="headerlink" title="4. Result"></a>4. Result</h2><img src="/images/image-20210712163712232.png" alt="image-20210712163712232" style="zoom:50%;" />]]></content:encoded>
      
      
      <category domain="https://jo-member.github.io/categories/PaperReview/">PaperReview</category>
      
      
      <category domain="https://jo-member.github.io/tags/Vision/">Vision</category>
      
      <category domain="https://jo-member.github.io/tags/NLP/">NLP</category>
      
      <category domain="https://jo-member.github.io/tags/Multimodal/">Multimodal</category>
      
      <category domain="https://jo-member.github.io/tags/VQA/">VQA</category>
      
      
      <comments>https://jo-member.github.io/2021/07/12/vqa_paper1/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>VQA (Visual Question Answering)</title>
      <link>https://jo-member.github.io/2021/07/12/VQA/</link>
      <guid>https://jo-member.github.io/2021/07/12/VQA/</guid>
      <pubDate>Mon, 12 Jul 2021 03:09:10 GMT</pubDate>
      
      <description>&lt;p&gt;Boostcamp에서 만난 동료들과 함께 &lt;a href=&quot;https://www.aiconnect.kr/main/competition/list&quot;&gt;2021 인공지능 온라인 경진대회&lt;/a&gt;에 참여했습니다.&lt;br&gt;총 10개의 과제중 시각장애인 시스템 개발을 위한 VQA 모델이라는 Competition에 참여하였습니다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>Boostcamp에서 만난 동료들과 함께 <a href="https://www.aiconnect.kr/main/competition/list">2021 인공지능 온라인 경진대회</a>에 참여했습니다.<br>총 10개의 과제중 시각장애인 시스템 개발을 위한 VQA 모델이라는 Competition에 참여하였습니다.</p><span id="more"></span><h2 id="개요"><a href="#개요" class="headerlink" title="개요"></a>개요</h2><p><strong>이미지를 보고 주어진 질문에 답변하는 Visual Question Answering 모델 개발</strong><br>VQA란 시각정보를 기반으로 질문에 답변하는 시스템입니다.<br>실내 및 실외 생활 거주 환경에서 촬영된 이미지와 그에 관련된 질문, 대답이 세트로 이루어져 있습니다.<br>총 224,464개의 이미지 파일과 702,135건의 질문-답변 쌍이 train data로 주어졌습니다.</p><h2 id="관련-논문-review"><a href="#관련-논문-review" class="headerlink" title="관련 논문 review"></a>관련 논문 review</h2><ol><li><a href="">VQA: Visual Question Answering</a></li><li>Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</li><li>Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge</li><li>Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</li><li>UNITER: UNiversal Image-TExt Representation Learning</li></ol>]]></content:encoded>
      
      
      <category domain="https://jo-member.github.io/categories/Competition/">Competition</category>
      
      
      <category domain="https://jo-member.github.io/tags/Vision/">Vision</category>
      
      <category domain="https://jo-member.github.io/tags/NLP/">NLP</category>
      
      <category domain="https://jo-member.github.io/tags/Multimodal/">Multimodal</category>
      
      <category domain="https://jo-member.github.io/tags/Competition/">Competition</category>
      
      
      <comments>https://jo-member.github.io/2021/07/12/VQA/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Pstage3_Image_Segmentation_Detection</title>
      <link>https://jo-member.github.io/2021/04/25/Pstage3-Image-Segmentation-Detection/</link>
      <guid>https://jo-member.github.io/2021/04/25/Pstage3-Image-Segmentation-Detection/</guid>
      <pubDate>Sun, 25 Apr 2021 11:52:06 GMT</pubDate>
      
      <description>&lt;h1 id=&quot;Image-Segmentation&quot;&gt;&lt;a href=&quot;#Image-Segmentation&quot; class=&quot;headerlink&quot; title=&quot;Image Segmentation&quot;&gt;&lt;/a&gt;Image Segmentation&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/bcaitech1/p3-ims-obd-hansarang&quot;&gt;https://github.com/bcaitech1/p3-ims-obd-hansarang&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;문제정의 : 쓰레기가 찍힌 사진에서 쓰레기를 Segmentation &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="Image-Segmentation"><a href="#Image-Segmentation" class="headerlink" title="Image Segmentation"></a>Image Segmentation</h1><p><a href="https://github.com/bcaitech1/p3-ims-obd-hansarang">https://github.com/bcaitech1/p3-ims-obd-hansarang</a></p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><ul><li><p>문제정의 : 쓰레기가 찍힌 사진에서 쓰레기를 Segmentation </p></li><li><span id="more"></span></li><li><p><strong>Input data</strong> :  4109장의 쓰레기 사진중, 3287장 (80%)는 train data, 나머지 812장(20%)는 private test data  (512,512)의 이미지</p><ul><li><p><strong>Annotation</strong> </p><ol><li>train_all.json: train에 쓰일 수 있는 모든 image, annotation 정보 (image: 3272, annotation: 26400)</li><li>train.json: train_all.json 중 4/5에 해당하는 정보 (image: 2617, annotation: 21116)</li><li>val.json: train_all.json 중 1/5에 해당하는 정보 (image: 655, annotation: 5284)</li><li>test.json: 예측해야할 이미지들의 정보 (image: 837)</li></ol><ul><li>id: 파일 안에 annotation 고유 id, 이건 한 image 안에 여러가지의 객체가 있기 떄문에 image별로 각각의 객체의 annotation들이 있다.</li><li>segmentation: masking 되어 있는 고유의 좌표</li><li>bbox: 객체가 존재하는 박스의 좌표 (x_min, y_min, w, h)</li><li>area: 객체가 존재하는 영역의 크기</li><li>category_id: 객체가 해당하는 class의 id</li><li>image_id: annotation이 표시된 이미지 고유 id</li></ul></li><li><p>images</p><ul><li>id: 파일 안에서 image 고유 id, ex) 1</li><li>height: 512</li><li>width: 512</li><li>file_name: ex) batch_01_vt/002.jpg</li></ul></li></ul></li><li><p><strong>Output data</strong> : 11 class = {UNKNOWN, General trash, Paper, Paper pack, Metal, Glass, Plastic, Styrofoam, Plastic bag, Battery, Clothing}</p></li><li><p><strong>평가 Metric</strong></p><img src="/images/image-20210426120429513.png" alt="image-20210426120429513" style="zoom:50%;" /><img src="/images/image-20210426120549788.png" alt="image-20210426120549788" style="zoom:50%;" /></li></ul><p>벌써 2주전의 기억이라 가물가물하지만 일렬의 과정들을 하나하나 되짚어 보며 이어나가도록 하겠습니다.</p><p>deeplab V3+, efficientnet-b5</p><ol><li>EDA</li></ol><p>먼저 EDA 부터 진행하였습니다. 수업에서 제공해주신 eda들로 우리 data의 label분포를 확인할 수 있었고, 따로 이미지 data를 시각화를 해보며 이전의 stage와 마찬가지로 상당히 imbalance하다는 사실을 알게되었습니다. </p><p>이에 stage 2때도 적용하였던 focal loss와 다양한 augmentation을 사용하여 해결해야겠다는 생각이 들었습니다.</p><ol start="2"><li>Model</li></ol><p>위에서 언급한것 처럼 처음부터 sota에 가까운 deeplab V3+를 선정하였고, 효율적이고 다양한 실험과정을 위해 backbone은 efficientnet b1으로진행하였습니다. 이에 작은 model에서의 parameter가 과연 큰 model에서도 똑같이 적용될까라는 의문이 들었지만, 이는 lr이나 scheduler에 해당한다고 생각이 되어, augmentation실험에서만 model의 size를 낮추었습니다.</p><p>가장처음 한 실험은 제기억에는 동일조건에서의  backbone에 따른 성능이였습니다.</p><p>다양한 크기의 Resnext와 efficientNet으로 실험을 진행하였고, 결론적으로 efficientNet-b5를 backbone으로 쓴 model이 가장 성능이 좋았습니다.<br>Resnext는 efficientNet에 비해 수렴속도도 빠르고 epoch당 시간도 적게 걸려, 이후의 앙상블을 위해 best model을 저장해 두었습니다. </p><ol start="3"><li>Augmentation</li></ol><p>이번 task에서 쓰게된 augmentation은 아래와 같습니다</p><ol><li>HorizontalFlip(p=0.5)</li><li>Rotate(p=0.5, limit=45)</li><li>Cutout(num_holes=4, max_h_size=20, max_w_size=20),            </li><li>CLAHE(),            </li><li>RandomBrightnessContrast(p=0.5),            </li><li>Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0)</li></ol><p>이렇게 조합해서 썼을때 가장 성능이 좋았다는 결론을 얻었었습니다.<br>다양한 조합과 확률 값들을 적용하여 비교하여 진행하는 일련의 과정들은 매우 고되고 많은 시간을 필요로 하였습니다.<br>여기서 지금와서 생각해보면 Auto Augmentation을 적용해 보았으면 좋았을듯 싶습니다…</p><p>또한 추가적으로 horizontalflip을 이용한 tta를 적용시켜 보았지만, 성능의 하락을 야기했습니다.</p><ol start="4"><li>Loss &amp; Optimizer &amp; Scheduler</li></ol><p>Loss는 Focal loss와 soft-crossentropy-loss를 각각 0.3,0.7의 가중치를 두어 학습하였습니다. 원래는 Focal만을 사용하였지만, stage1에서 multi loss에서 재미를 많이 봤었기 때문에, 마스터님의 의견을 듣고 scl을 추가해 주었습니다.</p><p>왜인지는 모르겠지만  soft-crossentropy-loss만을 사용하였을때 가장 성능이 좋아, 앙상블때의 다양성을 위해 multiloss로도 학습을 해두었습니다.</p><p>Optimizer또한 Adam계열의 Adamp를 사용하였고, Adam계열과 잘어울리는 Customized된 CosineAnnealingWarmRestarts의 scheduler를 사용하였습니다. 확실이 중간중간에 lr을 높혀주는게 local minimum을 잘빠져나오는 모습을 확인 할 수있었습니다. 내부에 내장된 Cosin scheduler은 gamma가 없기때문에 customized된 scheduler를 불러다 사용하였습니다. Adam에 잘맞는 cosine 계열의 scheduler를 사용한 결과, steplr을 사용한 타 팀원의 model 대비 제 model의 성능이 잘나왔음을 확인하였습니다.</p><p>wandb의 그래프를 보시면 보통 18 epoch쯤에서 최고점을 찍고 수렴하는 모습을 관찰하였습니다.</p><ol start="5"><li>K-fold &amp; Pseudo Labeled data</li></ol><p>single model의 성능의 한계에 부딛혀 0.63대를 헤어나오지 못하고있었던 2주차…<br>기존의 최고성능 parameter를 고정하고 Train+all과 pseudo labeled된 data를 합쳐서 2배의 data로 학습을 진행하였고 결과는 매우 성공적이였습니다. K-fold로 진행하고 싶었지만, GPU자원의 부족으로 인한 시간의 한계때문에 Train-all로 진행하여 제가 경험적으로 체득한 18 epoch에서 끊는 방식을 체택하였습니다. 결과는 매우성공적으로 single model 기준 0.6842라는 큰 성능향상을 얻어내었습니다.</p><p>다른 팀원분들도 pseudo label을 적용하여 앙상블을 하였다면 더 좋은 결과를 얻어낼 수 있었을텐데 매우 아쉽습니다.</p><ol start="6"><li>앙상블</li></ol><p>최종적으로 저의 다양한 backbone과 loss를 가지는 model들을 조합하여 soft voting을 하였습니다. 가중치는 LB상으로 가장높은 model에 0.4를 주었고 나머지에 0.2씩을 주어 총 4개의 singel   model을 앙상블 하여 제출을 해봤는데, 0.6961이라는 아주 높은 점수가 나왔습니다. 이 model에 다른 팀원분들의 model hard voting 해보았지만 성능이 계속 하락하여 결국에는 저의 model만을 사용한 점수가 최종점수가 되는 아쉬운 상황이 연출되었습니다…</p><p>어느정도 팀원들간의 평균적인 점수대가 비슷해야 앙상블 했을때 좋은 점수를 낼수있었지만, psudo label을 저만 돌렸었기 때문에…<br>시간이 2일정도 더있었다면 다른 팀원 분들도 수도라벨로 성능을 어느정도 향상시켜 비슷한 점수대로 맞춰줄수 있었을 텐데 하는 아쉬움이 남았습니다…</p><h1 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h1><ul><li><strong>평가 Metric</strong></li></ul><p><img src="/images/image-20210522134724067.png" alt="image-20210522134724067"></p><p><img src="/images/image-20210522140707268.png" alt="image-20210522140707268"></p><p>위와 같은 PB curve를 그린다. 이 때 recall과 precision은 confidence score별로 점들이 생성됩니다.<br>이후 (A+B)에 해당하는 영역의 넓이가 AP가 되고, 각각의 class의 AP의 평균이 저희가 구하려는 mAP입니다.</p><p>이번 object task에서는 mmdetection이라는 강력한 tool을 기반으로 실험을 진행하였습니다.<br>mmdetection에서는 저희가 config파일만을 수정하여 준다면 손쉽게 다양한 방법으로 다양한 model들을 실험해 볼 수있었습니다.</p><ol><li>Model</li></ol><p>이번 task에서또한 sota model로 알려진 swin transformer를 backbone으로 쓰고 detector 부분은 cascade mask rcnn과 htc를 사용하였습니다.<br>다행이도 Swin transformer의 config파일이 git에 전부 올라와있었고, 저희의 실험환경에 맞게 조금 변경해 주면 되었습니다. 하지만 이 mmdetection이라는 툴자체에 적응을 하는데 시간이 좀 소요가 되었고, 한 3-4일 정도가 지나서야 어느정도 가닥이 잡히면서 어떻게 써야할지 감이 잡혔던것 같습니다.</p><p>backbone을 고정한 후 neck을 변경시켜서 다양한 실험을 해보았습니다.</p><ol><li>FPN</li><li>PAFPN </li><li>NAS-FPN</li><li>BiFPN</li></ol><p>이렇게 4가지의 선택지가 있었는데 이중 가장 오래된 FPN을 선택한 이유는 한가지 입니다. 왜냐하면 pretrained된 pth파일을 github에서 제공해주고 있는데, 이 model에서 FPN을 쓰고있기 때문입니다.</p><p>처음에는 backbone만을 pretrained된걸 가져와서 쓰다가, 전체가 trained된 model이 있는것을 발견하고 실험해보았는데 전체가 pretrained된걸 가져와서 저희 task에 fine tuning? transfer learning하는 방식이 더욱 성능이 좋았습니다. </p><p>FPN으로 trained된 전체 model을 가져다가 NAS-FPN으로 바꾸어준 model에 적용을 시켜줄시 neck쪽의 weight들에는 값이 들어가지 않게됩니다. 이것이 성능이 더 좋을 수도 있기때문에 이러한 방법도 시도해 보았지만, 바꾸지 않고 그대로 FPN을 사용하는것이 더 좋았습니다.</p><p>한가지 아쉬웠던 것은 pretrained된 Swin transformer를 backbone으로 쓰면서 swin에 대한 어느정도 전반적인 이해만을 가지고 있었을뿐, 세세한 model의 구조는 알지 못한채 작성되어진 config 파일만을 가지고 실험에만 집중할 수 밖에 없었던 상황이였습니다.</p><p>약간씩 parameter들을 수정해 주면서, 근본적인 이해없이 직관에 의해 실험을 반복하고 있는 제 자신을 발견한 후 competition에 대한 약간의 회의감이 들었습니다. 하지만 멘토님께서 library를 잘다루는 것도 하나의 능력이라고 말씀해주셔서 다시 한번 생각해 보았던것 같습니다.</p><ol start="2"><li>Augmentation</li></ol><p>Augmentation에는 Flip과 Autoaugmentation, Normalize등을 적용해보았습니다.<br>가장 critical하게 작용했던 augementation이 바로 autoaug로, 여기서 resize와 crop size를 어떻게 주느냐에 따라 성능차이가 조금씩 발생했습니다. 이전 stage들에서의 경험으로, image task에서 high scale의 image training은 오랜 시간을 요구하지만 그만큼 성능이 잘나왔습니다. 따라서 resize의 list에는 upscale된 정사각형과 가로가 긴 직사각형, 세로가 긴 직사각형등을 고루 섞어 autoaug안에 인자로 넣어주었습니다.</p><p>이러한 변경점은 정사각형만을 넣어줬을때, low scaleing 해주었을때에 비해서 점수의 큰 향상을 야기했습니다.</p><ol start="3"><li>Loss Optimizer Scheduler</li></ol><p><strong>Loss</strong></p><p>저희가 바꾸어 줄 수있었던 loss는 bbox loss로 3가지 정도의 선택지가 있었습니다. 그중 DIoU Loss를 채택하였을때 성능이 약간 상승했고, classification loss쪽의 cross entropy loss는 건드리지 않았습니다.  </p><p><strong>Optimizer</strong> </p><p>Optimizer은 AdamW를 사용하였고, 초기 lr값은 1e-4으로 고정시켜 주었습니다.</p><p><strong>Scheduler</strong></p><p>Scheduler은 model에 따라 다르게 적용시켜 주었습니다.</p><p>HTC를 이용한 model에는 cosineannealing을 cascade mask rcnn을 적용한 model에는 steplr을 사용하였습니다.</p><p>StepLR을 사용시 어떠한 epoch에서 lr값을 감소시켜줄지를 정할 수 있었는데, 평균적으로 8,11 epoch에서 map50값이 수렴하기 시작하는것을 확인하고 이쯤에서 gamma=0.1의 factor로 lr값을 감소시켜주었습니다.</p><ol start="4"><li>Pseudo-labeling</li></ol><p>이전 segementation에서 pseudo labeling으로 큰 재미를 보았었기 때문에 이번 task에서는 좀 일찍 최고성능의 model로 pseudo data를 만들어 빠르게 실험해 보았습니다. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./train_all.json&#x27;</span>) <span class="keyword">as</span> json_file1:</span><br><span class="line">    train_data = json.load(json_file1)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./test.json&#x27;</span>) <span class="keyword">as</span> json_file2:</span><br><span class="line">    test_data = json.load(json_file2)</span><br><span class="line"></span><br><span class="line">start = <span class="built_in">len</span>(train_data[<span class="string">&#x27;images&#x27;</span>])</span><br><span class="line">train_id = train_data[<span class="string">&#x27;images&#x27;</span>]</span><br><span class="line">test_id = test_data[<span class="string">&#x27;images&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> test_id:</span><br><span class="line">    idx[<span class="string">&#x27;id&#x27;</span>] = start</span><br><span class="line">    start+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">PredictionString = pd.read_csv(<span class="string">&#x27;./output1.csv&#x27;</span>)[<span class="string">&#x27;PredictionString&#x27;</span>]</span><br><span class="line">annotation = []</span><br><span class="line">ids = <span class="number">26402</span></span><br><span class="line">image_id = start</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> bboxs <span class="keyword">in</span> PredictionString:</span><br><span class="line">    bboxs = bboxs.strip().split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">    bboxs = [<span class="built_in">float</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> bboxs]</span><br><span class="line">    bboxs = [bboxs[i:i + <span class="number">6</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(bboxs), <span class="number">6</span>)]</span><br><span class="line">    <span class="keyword">for</span> bbox <span class="keyword">in</span> bboxs:</span><br><span class="line">        prob = bbox[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> prob&lt;<span class="number">0.8</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        temp = <span class="built_in">dict</span>()</span><br><span class="line">        temp[<span class="string">&#x27;id&#x27;</span>] = ids</span><br><span class="line">        temp[<span class="string">&#x27;image_id&#x27;</span>] = image_id</span><br><span class="line">        temp[<span class="string">&#x27;category_id&#x27;</span>] = <span class="built_in">int</span>(bbox[<span class="number">0</span>])</span><br><span class="line">        temp[<span class="string">&#x27;segmentation&#x27;</span>] = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]]</span><br><span class="line">        temp[<span class="string">&#x27;area&#x27;</span>] = <span class="number">1</span></span><br><span class="line">        bbox = bbox[<span class="number">2</span>:]</span><br><span class="line">        new = [bbox[<span class="number">0</span>],bbox[<span class="number">3</span>],bbox[<span class="number">2</span>]-bbox[<span class="number">0</span>],bbox[<span class="number">3</span>]-bbox[<span class="number">1</span>]]</span><br><span class="line">        new = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">list</span>(np.<span class="built_in">round</span>(new, <span class="number">1</span>))]</span><br><span class="line">        temp[<span class="string">&#x27;bbox&#x27;</span>] = new</span><br><span class="line">        temp[<span class="string">&#x27;iscrowd&#x27;</span>] = <span class="number">0</span></span><br><span class="line">        annotation.append(temp)</span><br><span class="line">        ids+=<span class="number">1</span></span><br><span class="line">    image_id+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">train_data[<span class="string">&#x27;images&#x27;</span>] += test_id</span><br><span class="line">train_data[<span class="string">&#x27;annotations&#x27;</span>] += annotation</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./train+pseudo.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> outfile:</span><br><span class="line">    json.dump(json_data1, outfile,indent=<span class="number">4</span>)</span><br><span class="line">    </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>위의 코드로 pseudo data를 coco형태로 바꾸어준뒤 통합된 json 파일로 train을 진행하였습니다. 그러나 threshold값을 설정해주는게 매우 애매했다.<br>이에 따라 성능이 너무 하락하는 현상이 발생하였고 결국 pseudo label된 data는 쓰지 못하였습니다…</p><p>segementation에서 잘먹히던 pseudo label이 detection에서는 부정확한 label값들로 학습이 어려운가 봅니다.</p><ol start="5"><li>WBF</li></ol><p>마지막으로 최고 single model 기준 htc와 cascade 모두 0.5572의 점수를 얻어냈고 총 4개의 model을 앙상블한 결과 0.5824의 결과를 얻어냈습니다.<br>이후 모든 팀원들의 csv 파일을 WBF하여 최종적인 score 0.5884를 얻어내었습니다.</p><p>Conclusion</p><p>한가지 가장 중요하게 느낀점은 이렇게 competition을 마친이후에 관련 논문들과 kaggle notebook들을 자세히 정독하며 쓰였던 방법론들과 model들을 상세하게 공부해야 겠다는 필요성입니다.</p><p>competition 진행중에 개선해야 할 사항은 중간중간 팀원들간의 평균적인 점수대를 맞추어 놓아야 최종 앙상블 과정에서 큰 성능 향상을 이룰수 있다는 점입니다. </p><p>굉장히 열정적인 4주를 보냈습니다… 아쉬움도 남고 후련하기도 합니다…<br>진행했던 많은 실험 내용들을 모두 랩업레포트에 담지 못했다…. 추후에 체계적으로 정리해서 git에 올려둬야겠습니다.</p>]]></content:encoded>
      
      
      <category domain="https://jo-member.github.io/categories/Boostcamp/">Boostcamp</category>
      
      
      <category domain="https://jo-member.github.io/tags/Summary/">Summary</category>
      
      <category domain="https://jo-member.github.io/tags/CV/">CV</category>
      
      
      <comments>https://jo-member.github.io/2021/04/25/Pstage3-Image-Segmentation-Detection/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Pstage2_KLUE</title>
      <link>https://jo-member.github.io/2021/04/22/Pstage2-KLUE/</link>
      <guid>https://jo-member.github.io/2021/04/22/Pstage2-KLUE/</guid>
      <pubDate>Thu, 22 Apr 2021 06:29:38 GMT</pubDate>
      
      <description>&lt;h1 id=&quot;문장내-개체관-관계-추출&quot;&gt;&lt;a href=&quot;#문장내-개체관-관계-추출&quot; class=&quot;headerlink&quot; title=&quot;문장내 개체관 관계 추출&quot;&gt;&lt;/a&gt;문장내 개체관 관계 추출&lt;/h1&gt;&lt;p&gt;뭔가 아쉬웠던 P-stage 2 KLUE가 끝이 났다.&lt;/p&gt;
&lt;p&gt;이번 stage에서는 리더보드 순위를 올리는데에만 집중하기 보다는 다양한 task를 써보고 원리를 이해하고 결과를 토론계시판에 꼭 조금이라도 공유하는 방식으로 하기로 마음먹었었다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="문장내-개체관-관계-추출"><a href="#문장내-개체관-관계-추출" class="headerlink" title="문장내 개체관 관계 추출"></a>문장내 개체관 관계 추출</h1><p>뭔가 아쉬웠던 P-stage 2 KLUE가 끝이 났다.</p><p>이번 stage에서는 리더보드 순위를 올리는데에만 집중하기 보다는 다양한 task를 써보고 원리를 이해하고 결과를 토론계시판에 꼭 조금이라도 공유하는 방식으로 하기로 마음먹었었다.</p><span id="more"></span><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><ul><li><p><strong>문제정의</strong> : 문장의 단어(Entity)에 대한 속성과 관계를 예측하라. </p></li><li><p>Input data : 9000개의 train data, 1000개의 test data</p><p><img src="/images/image-20210423130448198.png"></p><p>우리가 빼내야 할 column : sentence, entities, place of entities</p></li><li><p><strong>Output</strong></p><p>총 42개의 class를 예측 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;관계_없음&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;인물:배우자&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;인물:직업/직함&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;단체:모회사&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;인물:소속단체&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;인물:동료&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;단체:별칭&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;인물:출신성분/국적&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;인물:부모님&#x27;</span>: <span class="number">8</span>, <span class="string">&#x27;단체:본사_국가&#x27;</span>: <span class="number">9</span>, <span class="string">&#x27;단체:구성원&#x27;</span>: <span class="number">10</span>, <span class="string">&#x27;인물:기타_친족&#x27;</span>: <span class="number">11</span>, <span class="string">&#x27;단체:창립자&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;단체:주주&#x27;</span>: <span class="number">13</span>, <span class="string">&#x27;인물:사망_일시&#x27;</span>: <span class="number">14</span>, <span class="string">&#x27;단체:상위_단체&#x27;</span>: <span class="number">15</span>, <span class="string">&#x27;단체:본사_주(도)&#x27;</span>: <span class="number">16</span>, <span class="string">&#x27;단체:제작&#x27;</span>: <span class="number">17</span>, <span class="string">&#x27;인물:사망_원인&#x27;</span>: <span class="number">18</span>, <span class="string">&#x27;인물:출생_도시&#x27;</span>: <span class="number">19</span>, <span class="string">&#x27;단체:본사_도시&#x27;</span>: <span class="number">20</span>, <span class="string">&#x27;인물:자녀&#x27;</span>: <span class="number">21</span>, <span class="string">&#x27;인물:제작&#x27;</span>: <span class="number">22</span>, <span class="string">&#x27;단체:하위_단체&#x27;</span>: <span class="number">23</span>, <span class="string">&#x27;인물:별칭&#x27;</span>: <span class="number">24</span>, <span class="string">&#x27;인물:형제/자매/남매&#x27;</span>: <span class="number">25</span>, <span class="string">&#x27;인물:출생_국가&#x27;</span>: <span class="number">26</span>, <span class="string">&#x27;인물:출생_일시&#x27;</span>: <span class="number">27</span>, <span class="string">&#x27;단체:구성원_수&#x27;</span>: <span class="number">28</span>, <span class="string">&#x27;단체:자회사&#x27;</span>: <span class="number">29</span>, <span class="string">&#x27;인물:거주_주(도)&#x27;</span>: <span class="number">30</span>, <span class="string">&#x27;단체:해산일&#x27;</span>: <span class="number">31</span>, <span class="string">&#x27;인물:거주_도시&#x27;</span>: <span class="number">32</span>, <span class="string">&#x27;단체:창립일&#x27;</span>: <span class="number">33</span>, <span class="string">&#x27;인물:종교&#x27;</span>: <span class="number">34</span>, <span class="string">&#x27;인물:거주_국가&#x27;</span>: <span class="number">35</span>, <span class="string">&#x27;인물:용의자&#x27;</span>: <span class="number">36</span>, <span class="string">&#x27;인물:사망_도시&#x27;</span>: <span class="number">37</span>, <span class="string">&#x27;단체:정치/종교성향&#x27;</span>: <span class="number">38</span>, <span class="string">&#x27;인물:학교&#x27;</span>: <span class="number">39</span>, <span class="string">&#x27;인물:사망_국가&#x27;</span>: <span class="number">40</span>, <span class="string">&#x27;인물:나이&#x27;</span>: <span class="number">41</span>&#125; </span><br></pre></td></tr></table></figure></li></ul><br/><h2 id="EDA"><a href="#EDA" class="headerlink" title="EDA"></a>EDA</h2><p>이번 자연어 task의 경우에는 데이터도 적고 image task에 비해 복잡한 eda가 필요한것 같지는 않았다.</p><p>따라서 가장 중요한 label들의 갯수만을 확인하고 빠르게 다음 단계로 넘어갔다. (토론글에도 하나 작성하긴 했지만, 거기서 작성했던 label들간의 유사성으로 class imbalnace를 해결하기 보다는 focal loss를 사용하는게 직관적으로 더 쉬워 보여서 focal loss를 사용하기로 하였다.</p><ol><li>label들의 분포</li></ol><p>주피터 노트북을 통해 빠르게 data를 불러와서 label들의 분포를 확인하여 보니. label들간의 불균형이 매우 매우 심했다. 심지어 인물 : 사망국가의 label을 가지는 data는 1개 뿐 이였다. 이 1개로 과연 test data의 해당 label을 잘 맞출수 있을까? 아닐것 같다.</p><p>이러한 imbalnace문제에 효과적으로 대처하는 방법은 이전 p-stage에서도 배웠었다.  바아아아로 focal loss</p><p>focal loss에 대해서 간략하게 알아보자</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FocalLoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, weight=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 gamma=<span class="number">2.</span>, reduction=<span class="string">&#x27;mean&#x27;</span></span>):</span></span><br><span class="line">        nn.Module.__init__(self)</span><br><span class="line">        self.weight = weight</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.reduction = reduction</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input_tensor, target_tensor</span>):</span></span><br><span class="line">        log_prob = F.log_softmax(input_tensor, dim=-<span class="number">1</span>)</span><br><span class="line">        prob = torch.exp(log_prob)</span><br><span class="line">        <span class="keyword">return</span> F.nll_loss(</span><br><span class="line">            ((<span class="number">1</span> - prob) ** self.gamma) * log_prob,</span><br><span class="line">            target_tensor,</span><br><span class="line">            weight=self.weight,</span><br><span class="line">            reduction=self.reduction</span><br><span class="line">        )</span><br></pre></td></tr></table></figure><p>Focal loss는 페이스북의 Lin et al이 제안한 loss function이다</p><p>간단하게 말하면 분류 에러에 근거하여 맞춘 확률이 높은 Class는 조금의 loss를, 맞춘 확률이 낮은 Class는 Loss를 훨씬 높게 부여해주는 가중치를 주어서 class imbalance에 더욱 효율적으로 대처하는 loss funtion이라고 생각하면 된다.</p><p>loss 함수를 호출하게 되면 input tensor들에 대한 확률로 표현된 tensor를 얻은뒤 F.nll_loss를 호출한다.</p><p><code>F.nll_loss(((1-prob) ** self.gamma) * log_prob,target_tensor,weight=self.weight,reduction=self.reduction)</code></p><p>Weight 값을 우리가 가지고 있는 label 분포에 대한 1-d tensor로 넣어준다.</p><p>Ex) 우리가 가지고 있는 label들은 42개니까 42 length를 가지는 1-d tensor 값은 label의 분포에 맞게</p><p>이렇게 loss 함수를 불러다가 짜주면 끄읏</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>모델을 굉장히 여러개 불러다가처음엔 돌려보았다. 사용해본 model 목록</p><ol><li>bert-base-multilingual-cased</li><li>xlm-roberta-large</li><li>koelectra-base-v3-discriminator</li><li>kobert</li></ol><p>이렇게 4개정도 실험해 보았던것 같았다. 결국 결론만 말해보자면 RoBERTa-large를 사용했다.<br>hevitz 님의 토론계시판 글을 보니, bert도 large를 사용하면 좋겠지만?????? 아직 Huggingface에  model이 공개된것 같지 않다.</p><p>그리고 </p><blockquote><p>This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks.</p></blockquote><p>라는 RoBERTa의 논문을 보면 이 significant performance gains 라는 문구는 이게 맞다라는 확신을 가져다 주었다. (물론 논문에서는 다들 자기가 짱이라고하긴하지)</p><p>또한 4개를 각각 적당한 hyperparameter로 돌려본 결과 평균적인 정확도가 roberta-large를 사용하면 대폭 증가함을 확인하였다.<br>RoBERTa는 bert와 유사하지만 BERT에 다양한 방법을 적용시켜 성능을 향상한 model이다. Model의 구조는 bert와 흡사하니 생략하겠다.</p><p>단지 hyperparameter를 최적화하고 NSP를 없애고 최대한 max_length에 맞춰서 문장을 넣어주고, masking을 더 다양한 방법으로 해주었다고 한다.</p><h2 id="train-방법"><a href="#train-방법" class="headerlink" title="train 방법?"></a>train 방법?</h2><ol><li>loss</li><li>optimizer</li><li>Train-set, validation-set 나누기</li></ol><p>이번 KLUE에서 Huggingface의 라이브러리는 질리도록 다룬것 같다. 물론 아직 모자르지만 ㅎㅎ<br>이전 P-stage에서는 training 과정을 우리가 다 pytorch 라이브러리를 사용하여 train 함수를 만들고 쏼라쏼라 해서 구현했었는데!!!!</p><p>이런 편리한 trainer라는게 있는 hugging face 아주 칭찬해 ^^</p><p>하지만 단순히 trainer를 사용하는것은 실력 증진에 별로 의미가 없다고 생각했다. (물론 마스터님 말씀처럼 Hugging face만 잘 사용하더라도 그만큼 장점이 있다고 한다!!!)</p><p>그래서 huggingface 홈페이지에 들어가서 trainer를 자세히 살펴보았다.</p><p>처음에 정했던 focal loss를 사용하기 위해서는 trainer class를 상속받아서 나만의 trainer class를 구현해주어야 했다. 홈페이지를 보면 관련 예제가 있어 쉽게 바꿔줄수 있었다. (현규님의 도움과 함께라면 ㅎㅎ)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FocalLossTrainer</span>(<span class="params">Trainer</span>) :</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span>(<span class="params">self, model, inputs, return_outputs=<span class="literal">False</span></span>) :</span></span><br><span class="line">        labels = inputs.pop(<span class="string">&#x27;labels&#x27;</span>)</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">        logits = outputs.logits</span><br><span class="line">        loss_fn = FocalLoss(weight=weight)</span><br><span class="line">        loss = loss_fn(logits, labels)</span><br><span class="line">        <span class="keyword">return</span> (loss, outputs) <span class="keyword">if</span> return_outputs <span class="keyword">else</span> los</span><br></pre></td></tr></table></figure><p>여기서 <code>loss_fn</code>만 위에서 정의한 FocalLoss()를 불러다가 사용하면 끄읏! easy</p><p>Optimizer관련해서는 trainer 안에서 사용하는 AdamW를 그대로 사용하면 될거 같았고 수정해주어야 할 것은</p><ul><li>lr_scheduler</li><li>lr</li></ul><p>이정도? 인것 같다. default 로 설정된 lr_scheduler은 step에 따라 linear 하게 lr 이 감소하는 scheduler를 쓴거 같은데<br>저번에 사용했던 <strong>CosineAnnealingWarmRestarts</strong> 으로 바꾸어서 사용해보면 어떨까? 생각을 해보았다.</p><p>저번 stage에서 AdamP와 CosineAnnealingWarmRestarts의 조합으로 꽤나 쏠쏠한 재미를 보았기 때문에 ㅎㅎ</p><p>세세한 parameter은 seed를 고정시킨 이후 validation score를 기준으로 설정해주면 될듯 싶다.<br>validation 과 train은 2:8 로 나누었고 제출전에는 data모두를 사용해서 train 한 model로 inference 하였다.</p><p><strong>자 이제 가장크게 고민을 해준 부분이다</strong></p><h2 id="Input-형식에-따른-성능"><a href="#Input-형식에-따른-성능" class="headerlink" title="Input 형식에 따른 성능"></a>Input 형식에 따른 성능</h2><p>이번 stage에서 제공된 baseline code는 꽤나 simple하고 간결하지만 있을거는 다있다.</p><p>가장 의문점이 들었던 것은 tokenizer에 넣어주는 data 의 형식이였다.</p><p>ent01 : 이순신</p><p>ent02 : 무신</p><p>sentence : 이순신은 조선중기의 무신이다.</p><p># RoBERTa tokenizer 기준 special token</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#39;bos_token&#39;: &#39;&lt;s&gt;&#39;,</span><br><span class="line"> &#39;eos_token&#39;: &#39;&lt;&#x2F;s&gt;&#39;,</span><br><span class="line"> &#39;unk_token&#39;: &#39;&lt;unk&gt;&#39;,</span><br><span class="line"> &#39;sep_token&#39;: &#39;&lt;&#x2F;s&gt;&#39;,</span><br><span class="line"> &#39;pad_token&#39;: &#39;&lt;pad&gt;&#39;,</span><br><span class="line"> &#39;cls_token&#39;: &#39;&lt;s&gt;&#39;,</span><br><span class="line"> &#39;mask_token&#39;: &#39;&lt;mask&gt;&#39;&#125;</span><br><span class="line"> </span><br><span class="line"> &lt;s&gt; 이순신 &lt;&#x2F;s&gt; 무신 &lt;&#x2F;s&gt;&lt;s&gt; 문장 &lt;&#x2F;s&gt;</span><br></pre></td></tr></table></figure><p> 이러한 형식으로 들어갔다.<br>오피스아워에서 여쭈어 보니까, 별다른 이유 없이 간단하게 정해준 형식이라고 했다.</p><p>따라서 초코송이님께서 올려주신 ner을 entitiy사이에 넣어주는 논문글을 읽고 pororo library를 이용하여 ner을 얻어낸뒤 이를 entity양옆에 삽입하여 주었다. </p><p><img src="../images/image-20210429181250424.png" alt="image-20210429181250424"></p><p>이와같은 형식이다.</p><p>처음에는 저 기호들을 special token에 추가해준뒤, model에 넣어주었지만!!</p><p>혜린님의 질문과 피드백으로 논문에서는 special 토큰으로 지정하지 않고, 원래 vocab에 있는 기호들을 사용하였다고 한다……<br>이미 토론글에 올렸는데……… 올리기 잘했다는 생각이 들었다. 올리지 않았다면 이 문제를 평생 모르고 잘못된 지식을 가진채로 실험하였을것이다.</p><p>마스터이나 조교님들 말씀대로 일단 나대는게 좋은것 같다. 남들에게 배울점도 많고, 나대면서 스스로 좀더 찾아보고 학습하게 되는것 같다.</p><p>이러한 input 형식은 동일 seed model hyperparamter으로 약 1.5퍼 정도 leaderboard acc의 상승을 이끌어냈다.</p><p>autotokenizer로 불러온 tokenizer은 대부분의 vocab들을 포함하고 있어, unknown으로 나오는 token들이 적은것을 확인하였다.<br>특히 entity가 unk로 나오게되면 큰 문제임으로 이를 체크하였는데, 모두 잘 tokenize된것을 확인하였다.</p><p>optimzer은 trainer에서 사용했던걸로 동일하게 사용하였고<br>lr과 scheduler를 바꾸어 가며 RoBERTa에 맞는 hyperparameter를 찾기위해 노렸했다.;)</p><p>이번 stage에는 따로 하고있는 ROS 실습과 겹쳐 하고싶은게 3가지 있었는데 못해봤다ㅠㅠ</p><ol><li><p>wandb로 실험 관리하기, sweep 사용해서 automl까지 해보기</p></li><li><p>input에 무작위로 masking 적용해보기</p></li><li><p>augmentation으로 data 증강해보기</p></li></ol><p>하지만 이번 stage로 관심이 없었던 NLP에 대해 다시한번 생각해보게 되었다. 생각보다 재미있는 아이디어들이 많았고, 특히 오피스아워나 마스터 클래스에서 멘토님과 마스터님의 열정을 보고 굉장히 큰 영감과 자극을 받았다. 아직 한국어 관련 NLP task들이 많이 부족하다는 걸 듣고 확실히 고려해보게 되었다. Hugging face 를 많이 다루어 본점도 매우 득이되었던 stage였다.</p>]]></content:encoded>
      
      
      <category domain="https://jo-member.github.io/categories/Boostcamp/">Boostcamp</category>
      
      
      <category domain="https://jo-member.github.io/tags/nlp/">nlp</category>
      
      <category domain="https://jo-member.github.io/tags/hugging-face/">hugging_face</category>
      
      <category domain="https://jo-member.github.io/tags/ner/">ner</category>
      
      
      <comments>https://jo-member.github.io/2021/04/22/Pstage2-KLUE/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Image Classification</title>
      <link>https://jo-member.github.io/2021/03/08/2021-03-08-Boostcamp31.1/</link>
      <guid>https://jo-member.github.io/2021/03/08/2021-03-08-Boostcamp31.1/</guid>
      <pubDate>Sun, 07 Mar 2021 15:00:00 GMT</pubDate>
      
      <description>&lt;br/&gt;



&lt;p&gt;K Nearest Neighbors (k-NN)&lt;/p&gt;
&lt;p&gt;기존의 data가 가지고있는 label을 활용해서 새로운 data의 label을 분류하는 문제가 된다. 이렇게 된다면 미리 유사도를 정의해야 한다. 그리고 system 복잡도가 너무 높다. 따라서 data를 NN의 parameter에 녹여넣는 것이다.&lt;/p&gt;
&lt;p&gt;Yann Lecun의 CNN 개발 : 우편번호인식에 혁신을 이루어냄&lt;/p&gt;
&lt;p&gt;Using better activation function   &lt;/p&gt;
&lt;p&gt;annotation data의 효율적인 학습 기법 &lt;/p&gt;
&lt;p&gt;data 부족문제의 완화 : 대표적인 방법들&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Data augmentation&lt;/li&gt;
&lt;li&gt;Leveraging pre-trained information&lt;/li&gt;
&lt;li&gt;Leveraging unlabeled dataset for training&lt;/li&gt;
&lt;/ol&gt;</description>
      
      
      
      <content:encoded><![CDATA[<br/><p>K Nearest Neighbors (k-NN)</p><p>기존의 data가 가지고있는 label을 활용해서 새로운 data의 label을 분류하는 문제가 된다. 이렇게 된다면 미리 유사도를 정의해야 한다. 그리고 system 복잡도가 너무 높다. 따라서 data를 NN의 parameter에 녹여넣는 것이다.</p><p>Yann Lecun의 CNN 개발 : 우편번호인식에 혁신을 이루어냄</p><p>Using better activation function   </p><p>annotation data의 효율적인 학습 기법 </p><p>data 부족문제의 완화 : 대표적인 방법들</p><ol><li>Data augmentation</li><li>Leveraging pre-trained information</li><li>Leveraging unlabeled dataset for training</li></ol><span id="more"></span><p>Data augmentation</p><p>Data를 통한 pattern의 분석</p><p>Dataset is almost biased != real data<br>결국 우리가 사용하는  data들은 사람이 bias해서 찍은 사진들이 대부분이기 때문에 우리가 얻어놓은 training data까지 모두 표현하지 못하는 data들이다.</p><p>ex) crop, rotate, Brightness, …</p><p>Affine transformation</p><p>변환전후에 선으로 유지가 되고, 길이의 비율과 평행관계가 유지가 되지만 각도가 달라지는. </p><p>기본적인 틀을 맞춘 attine transofrmation</p><p>mixing both images and labels</p><p>RandAugment</p><p>random하게 augmentation 방법을 수행후 잘나온것을 가져다 쓰자. 어떤걸 적용할까, 어떤 강도로 augmentation을 할까?</p><p>이걸 policy리고 한다. Random sampling시</p><p>dataset을 만들어야 하는데 이러한 data를 모을때 label이 필요하기 떄문에 이러한 data를  단기간에 수집하기가 쉽지가 않다.</p><p><strong>Transfer learning</strong></p><p>기존에 학습시킨 model에 조금 바꿔서 적용. 한데이터set에서 배운 지식을 다른 task에 적용</p><p>한 dataset에 적용된 경우에 다른곳에도 적용할 수 있지 않을까?<br>Freeze 기존의 CNN layer’s parameter<br>적은 data로 부터</p><p><img src="/images/image-20210308112418434.png" alt="image-20210308112418434"></p><p>Pseudo-labeling이 좀 신기하다.</p><p>Knowledge distillation</p><p><img src="/images/image-20210308113809064.png" alt="image-20210308113809064"></p><p>더 깊은 network -&gt; 더 높은 성능</p><p>깊게 쌓을수록 gradient explosion이나 vanishing gradient가 발생하였다, 계산복잡도가 올라가서 속도의 저하, overfitting문제가 아니라 degradation problem이라는게 밝혀졌다.</p><p>네트워크를 깊게 쌓기위한 network</p><ol><li>GoogLeNet</li></ol><p>하나의 layer에서 다양한 크기의 cnn filter를 사용하서 여러측면으로 image를 관찰하겠다. 한층에 이렇게 여러 filter를 사용하게 되면 계산복잡도가 올라가고, parameter숫자가 늘어나기 때문에, 1x1 filter를 추가해 주었다. 1x1 layer as bottle neck architecture</p><ul><li>공간크기는 변하지 않고, channel 수만 변화시켜준다.</li></ul><p>Overall architecture</p><ul><li>inception module을 깊게 쌓아서 전체 network 형성</li><li>Auxiliary classifiers : gradient vanising 문제를 해결하기 위해 추가해준 classifiers. 중간중간에 gradient를 꼽자주는 역할을 한다.</li><li>loss가 중간에서 부터 흘러들어가기 때문에 멀리있는 단까지 gradient 전달이 가능하다.</li></ul><p>Auxiliary classifier</p><p><img src="/images/image-20210309113004104.png" alt="image-20210309113004104"></p><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>아직도 큰 영향력을 발휘하고 있는 network이다.</p><p>최초로 100개 이상의 layer를 쌓았다. 최초로 인간 level의 성능을 뛰어넘었다.</p><p>이러한 성과로 cvpr best paper를 받았다. 기존연구자들의 layer를 깊게 쌓는데 문제점</p><p><img src="/images/image-20210309113203089.png" alt="image-20210309113203089"></p><p>원래는 model parameter가 많으면 error가 줄어들 것이라고 생각했는데, 56 layer의 error가 더 크다는 결과가 나왔기 때문에, over fitting때문이 아니라는 결론이 나옴.</p><p>대신에 최적화 문제에 대해서 56 layer이 최적화 되지 않은 결과이다.</p><p>Í<img src="/images/image-20210309113447823.png" alt="image-20210309113447823"></p><p>이렇게 만들어 버리면 학습의 부담감이 덜어지고 분할정복이 가능한 문제가 되지 않았는가?</p><p>이를 해결해 주기 위해ㅐ</p><p>shortcut connection을 통해 back prop과정에서 길이 하나가 더생기는 것이다.gradient. vanishing 문제가 해결이 되었다. 왜성능이 잘나올까?</p><p>residual connection을 하나 추가할때마다 2배씩 path가 늘어난다. 다양한 경로를 통해서 굉장히 복잡한 mapping의 학습이 가능했다.</p><p>initialization으로 He initialization을 사용했다. Reason ? -&gt; initialize를 작게 해주어야 이후에 더해줄때 균형이 맞는다.</p><p>3x3 conv layer로 모두 이루어져 있다.</p><p>Only a single FC layer at final output</p><h2 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h2><p>channel 축으로 concatnate한다. 훨씬이전의 layer에 대한 정보들도 모두 이어준다. 상위 layer에서도 모든 하위 layer의 특징을 참조할 수 있도록 해주었다.</p><p>더하기 두 신호를 합쳐버린다</p><p>concatnate chanel은 늘어나지만 feature를 더욱 잘 보존</p><p>fix된 3x3 만큼의 weight paramter가 이미 존재를 하고 2d offset을 위한 branch가 따로 존재 한다. 각각의 weight들을 벌려준다?</p><h1 id="Semantic-segmentation"><a href="#Semantic-segmentation" class="headerlink" title="Semantic segmentation"></a>Semantic segmentation</h1><p>픽셀단위로 분류해보자</p><p>영상속의 mask를 생성하게 되는데 같은 class이지만 서로다른 물체를 구분하지는 않는다.</p><p>영상속에 자동차가 여러대 있어도다 같은 class (색) 으로 구분한다.</p><p>영상내의 장면 content를 이해하는데 사용하는 필수적인 기술이다. object들이 구분되는 특징을 이해를 하여 </p><h2 id="Fully-Convolutional-Networks"><a href="#Fully-Convolutional-Networks" class="headerlink" title="Fully Convolutional Networks"></a>Fully Convolutional Networks</h2><p>입력에서 부터 끝까지 NN으로 구성한다.<br>입력으로 임의의 해상도 출력도 입력에 맞춘 해상도, 중간의 layer들도 모두 미분가능한 layer들이다.</p><p>각위치다 channel축으로 flattening이후 각각의 vector를 쌓아서 각 위치마다 vector가 하나씩 나오게 된다. </p><p>Upsampling</p><p>receptive field가 작기 때문에 upsampling을 통해서 강제로 resolution을 맞추어준다.</p><p>일단은 작게 만들어서 receptive field를 최대한 키운다음에 upsampling한다.</p><ol><li>Transpose Convolution</li></ol><p>결과를 이렇게 그냥 더해도 되는건가?<br>cnn과 stride 사이즈를 조절해서 겹치는부분이 없게끔 조절해주어야 한다. (overlap problem)</p><ol start="2"><li>Upsampling Convolution</li></ol><p>학습가능한 upsampling을 학습가능한 하나의 layer로 만들어주었다. </p><p>해상도가 낮아지지만 semantic하고 Holistic </p><p>중간층의 map을 upsampling한 이후에 </p><p>높은 layer에 있는 feature map을 upsampling을 통해 해상도를 올리고 이에 맞춰서 중간층의 map들또한 upsampling한다. 이들을 concatnate하여서 각픽셀마다 class의 score를 뱉어주게 된다. </p><p>최대한 많은 layer들을 합친것이 큰 도움이 된다.</p><p>FCN은 end to end로 손으로 만든게 아니라 모두 NN이라 병렬처리도 가능하고 성능도 좋으며, low high feature모두 잘 포함한다.</p><p>U-Net</p><p>built upon fully convolutional networks</p><p>with  <strong>skip connections</strong></p><p>channel size가 줄고 해상도가 느는 expanding path</p><p>fusion - concatnation을 사용한다.</p><h2 id="DeepLab"><a href="#DeepLab" class="headerlink" title="DeepLab"></a>DeepLab</h2><p>pixel과 pixel사이의 관계를 이어준후 pixel간의 거리를 모델링하였다.<br>확산의 반복으로 물체의 경계에 잘맞는 segmetation을</p><ul><li>Dilated convolution</li><li>parameter수는 늘어나지만 </li></ul><p>depthwise convolution</p><p>channel별로 conv연산을 해서 값을 각각 뽑은후, 각 channel별로 pointwise convolution을 통하여 하나로 합쳐준다. </p><p><img src="/images/image-20210309140019684.png" alt="image-20210309140019684"></p><p>Instance segmentation으로 빠르게 발전을 하고있다.</p><p>Instance segmantation : 같은 사람이여도 같은색이 아닌 따로따로 segmentation이 가능한 기능</p><p>panoptic segmentation</p><p>Instance segmentation을 포함하는 기술</p><p>객체들을 구분하는 기술 : object detection</p><p>scene understanding을 위한 기술</p><p>bounding ob와 classification을 동시에 추정하는 기술이다.</p><p>해당하는 box의 물체의 category까지 추정한다.<br>2개의 좌표로 bounging box를 결정한다. 나머지는 class에대한 probability를 결정해 준다. Bounding box localization</p><p><strong>selective search</strong></p><p>oversegmentation 이후 비슷한 색깔끼리 합쳐준다.</p><h1 id="Two-stage-detector"><a href="#Two-stage-detector" class="headerlink" title="Two-stage detector"></a>Two-stage detector</h1><ol><li>R-CNN</li></ol><p>기존의 image classification을 활용</p><p>selective serch 로 region proposal을 구하고<br>적절한 크기로 warping을 해서 CNN (pretrained)에 넣어준후 category를 구해준다.<br>마지막 classifier은 SVM을 썼다.<br>단점 : model 하나하나마다 모두 cnn을 돌려야하고 selective search를 사용해서 학습을 통한 성능향상에 제한이있다.</p><ol start="2"><li>Fast R-CNN</li></ol><p>recycle a pre-computed feature for multiple object detection</p><p>영상전체에 대한 feature을 추출후 이를 재활용</p><ul><li>CNN에서 Convolutional feature map을 뽑아주고(warping x)</li><li>ROI pooling layer로 feature map으로 부터 ROI feature를 뽑아낸다</li><li>feature pooling이후 class와 bbox regression을 사용한다. </li></ul><p><img src="/images/image-20210311105154100.png" alt="image-20210311105154100"></p><p>여전히 roi를 찾기 위해 selective search를 쓰고있다</p><ol start="3"><li>Faster R-CNN</li></ol><p>최초의 endtoend object detection</p><p>IoU = Area overlap/Area of Union, 높을수록 두영역이 많이 겹친다</p><ul><li>Anchor boxes- 9개의 actor box를 사용하였다. 미리 정해놓은 bbox의 크기</li></ul><p>Selective search를 대체하는 RPN을 제안하였다.</p><p>그럴듯한 bbox만 남기기 위해 non maximum suppression을 사용하였다.</p><h1 id="Single-stage-detector"><a href="#Single-stage-detector" class="headerlink" title="Single stage detector"></a>Single stage detector</h1><p>정확도 보다 속도를 선택한 것이다</p><p>image를 gird로 나누어서 4개의 좌표와 confidence score를 예측한다.</p><p>각각의 task보다 Instance segmentation과 Panoptic segmentation</p><h1 id="Instance-Segmentation"><a href="#Instance-Segmentation" class="headerlink" title="Instance Segmentation"></a>Instance Segmentation</h1><p>Instance segmentation = Sementic segmentation + distuguishing instances</p><h2 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h2><ol><li><p>RPN</p><p>기존의 ROI 풀링은 정수좌표만 지원을 했었는데, interpolation을 위해서 소수점 pixel level을 지원하였다. </p></li></ol><h1 id="Panoptic-Segmentation"><a href="#Panoptic-Segmentation" class="headerlink" title="Panoptic Segmentation"></a>Panoptic Segmentation</h1><h2 id="UPSNet"><a href="#UPSNet" class="headerlink" title="UPSNet"></a>UPSNet</h2><p>FPN구조로 고해상도 feature를 뽑은 이후 Semantic Head 와 Instance Head로 나누어 predict를 하게 된다.</p><h1 id="Landmark-localization"><a href="#Landmark-localization" class="headerlink" title="Landmark localization"></a>Landmark localization</h1><p>Facial landmark localizaiton</p><p>Human pose estimation</p><p>다양한 data를 사용한 학습</p><h1 id="Multi-model-learning"><a href="#Multi-model-learning" class="headerlink" title="Multi-model learning"></a>Multi-model learning</h1><p>Challenges</p><ol><li>각각의 감각의 데이터가 모두 다른 representation을 띈다</li><li>Feature space에 대하여 balance가 맞지 않는다.</li><li>여러 modelity를 사용할 경우 특정 model에 bias될수 있다.</li></ol><p>대표적인 구조</p><ol><li>Matching</li><li>Translating</li><li>Referencing</li></ol><h2 id="Visual-data-amp-Text"><a href="#Visual-data-amp-Text" class="headerlink" title="Visual data &amp; Text"></a>Visual data &amp; Text</h2><p>Joint embedding</p><ol><li>Image tagging</li></ol><p>태그 -&gt; 이미지, 이미지 -&gt; 태그</p><p>각 feature들은 차원을 맞춰주고 이둘의 Joint embedding을 만들어준다.</p><p>같은 space에 이미지와 text를 embedding해주고 matching되는 image와 text 끼리 거리가 가까워 지게끔 학습을 진행한다.</p><p>Metric Learning</p><p><img src="/images/image-20210312111734568.png" alt="image-20210312111734568"></p><p><img src="/images/image-20210312144707419.png" alt="image-20210312144707419"></p><p>창 - 프로세스 (현재 진행중)<br>탭 - 쓰레드 (그냥 띄워진 창) </p><ol><li>쓰레드마다 갖는 메모리 공간 / 프로세스가 공유하는 메모리 공간이 있다.</li><li>프로세스가 늘어나면 쓰레드 공유 공간이 늘어나게 된다.</li></ol><p>process: 코어수에 따라 병렬처리 가능<br>thread: 프로세스 위에 올라가있는 task<br><del>보통 1개의 process로 concurrent로 처리하는 것 보다, max core의 50</del>70%정도로 process를 나눠서 처리해주는 게 훨씬 좋은 성능을 낸다.</p><p>—- ps. 파이썬은 멀티프로세싱 &gt;&gt; 멀티쓰레딩<br>search keyword: multi threading/processing, python global interpreter lock(GIL)</p><p>mini task) [멀티 프로세싱/쓰레딩] 으로 10만까지의 소수 찾고 성능 비교 후 github에 올리기 (~3.15 월)</p><p>—- <em>point</em> —-<br> 피어세션 뿐만 아니라, 앞으로 공부방향에 있어 수업 내용 외적으로 전체적인 그림을 그리며 공부를 이어나갈 것! (cs, ml pipeline 등…)</p><p> Q) ml 엔지니어라면, 검색을 했을 때 가장 좋은 결과를 내기 위해서는 어떻게 해야 할까요?<br> A) 어떤 것이랑 어떤 것을 연결시킬 건지…등등 잘 생각해보자! ^_^</p>]]></content:encoded>
      
      
      <category domain="https://jo-member.github.io/categories/Boostcamp/">Boostcamp</category>
      
      
      <category domain="https://jo-member.github.io/tags/CNN/">CNN</category>
      
      <category domain="https://jo-member.github.io/tags/Vision/">Vision</category>
      
      
      <comments>https://jo-member.github.io/2021/03/08/2021-03-08-Boostcamp31.1/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Graph2</title>
      <link>https://jo-member.github.io/2021/02/23/2021-02-23-Boostcamp22/</link>
      <guid>https://jo-member.github.io/2021/02/23/2021-02-23-Boostcamp22/</guid>
      <pubDate>Mon, 22 Feb 2021 15:00:00 GMT</pubDate>
      
      <description>&lt;h1 id=&quot;검색엔진에서의-그래프&quot;&gt;&lt;a href=&quot;#검색엔진에서의-그래프&quot; class=&quot;headerlink&quot; title=&quot;검색엔진에서의 그래프&quot;&gt;&lt;/a&gt;검색엔진에서의 그래프&lt;/h1&gt;&lt;br/&gt;

&lt;h1 id=&quot;페이지랭크의-배경&quot;&gt;&lt;a href=&quot;#페이지랭크의-배경&quot; class=&quot;headerlink&quot; title=&quot;페이지랭크의 배경&quot;&gt;&lt;/a&gt;페이지랭크의 배경&lt;/h1&gt;&lt;h3 id=&quot;1-1-웹과-그래프&quot;&gt;&lt;a href=&quot;#1-1-웹과-그래프&quot; class=&quot;headerlink&quot; title=&quot;1.1 웹과 그래프&quot;&gt;&lt;/a&gt;1.1 웹과 그래프&lt;/h3&gt;&lt;p&gt;웹(방향성이 있는 그래프) = 웹페이지(node) + 하이퍼링크(edge)&lt;/p&gt;
&lt;p&gt;웹페이지는 추가적으로 키워드 정보를 포함하고있다.&lt;/p&gt;
&lt;h3 id=&quot;2-2-구글이전의-검색엔진&quot;&gt;&lt;a href=&quot;#2-2-구글이전의-검색엔진&quot; class=&quot;headerlink&quot; title=&quot;2.2 구글이전의 검색엔진&quot;&gt;&lt;/a&gt;2.2 구글이전의 검색엔진&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;웹을 거대한 디렉토리로 정리&lt;/p&gt;
&lt;p&gt;웹페이지의 수가 증가함에 따라 카테고리 수도 무한정 커지는 문제가 있다&lt;/p&gt;
&lt;p&gt;카테고리 분류가 모호할수가 있다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;키워드에 의존한 검색엔진&lt;/p&gt;
&lt;p&gt;악의적인 웹피이지에 취약하다&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="검색엔진에서의-그래프"><a href="#검색엔진에서의-그래프" class="headerlink" title="검색엔진에서의 그래프"></a>검색엔진에서의 그래프</h1><br/><h1 id="페이지랭크의-배경"><a href="#페이지랭크의-배경" class="headerlink" title="페이지랭크의 배경"></a>페이지랭크의 배경</h1><h3 id="1-1-웹과-그래프"><a href="#1-1-웹과-그래프" class="headerlink" title="1.1 웹과 그래프"></a>1.1 웹과 그래프</h3><p>웹(방향성이 있는 그래프) = 웹페이지(node) + 하이퍼링크(edge)</p><p>웹페이지는 추가적으로 키워드 정보를 포함하고있다.</p><h3 id="2-2-구글이전의-검색엔진"><a href="#2-2-구글이전의-검색엔진" class="headerlink" title="2.2 구글이전의 검색엔진"></a>2.2 구글이전의 검색엔진</h3><ol><li><p>웹을 거대한 디렉토리로 정리</p><p>웹페이지의 수가 증가함에 따라 카테고리 수도 무한정 커지는 문제가 있다</p><p>카테고리 분류가 모호할수가 있다.</p></li><li><p>키워드에 의존한 검색엔진</p><p>악의적인 웹피이지에 취약하다</p></li></ol><span id="more"></span><p>따라서 page rank라는 하나의 알고리즘을 구글이 만들었다</p><p>페이지랭크의 핵심은 투표이다.</p><p>웹페이지는 하이퍼 링크를 통해 투표를하게 된다.</p><p>사용자가 입력한 키워드를 포함한 웹페이지에서 u가 v에 연결되어있다면 v는 신뢰가능</p><p>많이 인용된 논문을 신뢰하는것과 비슷한 알고리즘</p><p>웹페이지를 여러개 만들어서 간서의 수를 부풀릴수 있다.</p><p>이런식의 악용은 온라인 sns에서도 흔히 발견이 된다.</p><p>이러한 악용을 막기위해 가중투표를 사용한다.</p><p>측정하려는 웹페이지의 관련성과 신뢰도 </p><p>자시의 점수 / 나가는 이웃의 수 </p><p><img src="/Users/jowon/workspace/jo-member.github.io/_posts/images/image-20210223103102754.png" alt="image-20210223103102754"></p><p>또한 페이지 랭크는 임의보행의 관점에서도 정의 할 수있다.</p><p>웹서퍼는 현재 하이퍼링크중 하나를 균일한 확률로 클릭하는 방식으로 웹을 서핑한다.</p><p><img src="/images/image-20210223103757983.png" alt="image-20210223103757983"></p><p>이 과정을 무한히 수행하면 p(t) = p(t+1)이된다. 수렴한 p는 정상분포라고 부른다. 결국 이를 정리해보면 투표관점에서의 pagerank정의 수식과 비슷함을 확인할 수 있었더.</p><h2 id="페이지랭크의-계산"><a href="#페이지랭크의-계산" class="headerlink" title="페이지랭크의 계산"></a>페이지랭크의 계산</h2><ol><li><p>반복곱</p><p>Power iteration은 각웹페이지에 페이지 랭크를 구할때 사용된다</p><p>각웹페이지 i의 페이지 렝크 점수를 동일하게 (1/웹페이지수) 로 초기화 한다</p><p>아래의 식을 사용하여 웹페이지의 점수를 갱신한다</p><p><img src="/images/image-20210223105936645.png" alt="image-20210223105936645"></p><p>페이지 랭크 점수가 수렴할때까지 계산을 반복한다.</p><p><img src="/images/image-20210223110133426.png" alt="image-20210223110133426"></p></li><li><p>과연 반복곱이 할상 수렴을 할까요?</p><p>수렴하지 않을수가 있다. -&gt; 들어오는 정점은 있지만 나가는 간선이 없는 spider trap에 의해 일어남</p></li><li><p>과연 수렴을 한다고 해도 이 결과가 합리적인 값일까요?</p><p><img src="/images/image-20210223110453360.png" alt="image-20210223110453360"></p></li></ol><p>이 문제점들에 대한 해결책 : 순간이동</p><p>임의 보행관점에서</p><p>(1) 현재 웹페이지에 하이퍼링크가 없다면 임의의 웹페이지로 순간이동을 한다</p><p>(2) 현재 웹페이지에 하이퍼링크가 있다면</p><p>$\alpha$의 확률로 파이퍼 링크중 하나를 균일한 확률로 선택하고 클릭한다</p><p>(1-$\alpha$)의 확률로 임의의 웹페이지로 순간이동 한다</p><p>이로 인해 spider trap이나 dead end에 갇히는 일이 사라졌다. </p><p>순간이동의 도입은 수식이 바뀐다</p><ol><li><p>각 막다른 정점에서 다른모든 정점으로 가는 간선을 추가한 후</p><p><img src="/images/image-20210223110837800.png" alt="image-20210223110837800"></p><p>이제는 이 수식을 사용한다.</p></li></ol><h1 id="그래프를-이용한-바이럴-마케팅"><a href="#그래프를-이용한-바이럴-마케팅" class="headerlink" title="그래프를 이용한 바이럴 마케팅"></a>그래프를 이용한 바이럴 마케팅</h1><h2 id="의사결정-기반의-전파"><a href="#의사결정-기반의-전파" class="headerlink" title="의사결정 기반의 전파"></a>의사결정 기반의 전파</h2><p>주변의 의사결정을 고려하여 의사결정을 할때 의사결정 기반의 전파모형을 사용한다</p><p>—&gt; 선형임계치모형 (linear threshold model)</p><p><img src="/images/image-20210223120102155.png" alt="image-20210223120102155"></p><h2 id="확률적-전파"><a href="#확률적-전파" class="headerlink" title="확률적 전파"></a>확률적 전파</h2><p>코로나의 전파를 수학적으로 나타낼때는 확률적 모형을 사용해야 한다.</p><p>Independent Cascade model</p><p>바리럴 마케팅이란?</p><p>소비자로 하여금 상품에대한 긍정적인 입소문을 내게 하는 기법</p><p>바이럴 마케팅을 위해서는 소문의 시작점이 중요하다.</p><p>시드 집합이 전파에 많은 영향을 미친다</p><p>그래프. 전파모형, 시드집합의 크기가 주어졋을때 전파의 최대화를 위한 시드집합은 전파최대화 문제이다.</p><p>어려운 문제이다. 그래프에 V개의 정점이 있는경우 시드집합이 k개일때 경우의 수는 vCk 이다</p><p>이론적으로 전파최대화 문제는 풀기가 힘든 문제임이 증명이되어있다.</p><ol><li><p>대표적 휴리스틱으로 정점의 중심성 을 사용합니다.</p><p>즉시드 크기가 k개로 고정이되어있을때 정점의 중심성이 높은수으로 k개 정점을 선택하는 방법이다.</p><p>정점의 중심성으로는 페이지 랭크 점수, 연결 중심성, 근접 중심성, 매개 중심성등이있다. </p></li><li><p>탐욕 알고리즘</p><p>시드 집합의 원소를 한번에 한명씩 선택을 한다.</p><p>정점의 집합 : {1,2,….,V}</p><p>각 집합에 대해 시뮬레이션을 반복하여 평균값을 사용한다. x라는정점이 최초의전파자로 선정이 되어있다. 이런 비교를 통해 뽑힌 집합은 x라고 하자. 이제 x를 포함한 크기가 2이 시드 집합을 찾는다.이를 목표의 크기까지 반복한다.</p><p>최초전파자간의 조합을 고려하지 않는다.   </p><p>탐욕 알고리즘은 항상 최고의 시드 집합을 찾는다는 보장이 없는 근사의 알고리즘이다</p><p>항상 최적의 값이 아니라는 말이다.</p><p>하지만 적어도 어느정도의 시드집합은 찾을 수 있다.</p></li></ol>]]></content:encoded>
      
      
      <category domain="https://jo-member.github.io/categories/Boostcamp/">Boostcamp</category>
      
      
      <category domain="https://jo-member.github.io/tags/Graph/">Graph</category>
      
      
      <comments>https://jo-member.github.io/2021/02/23/2021-02-23-Boostcamp22/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Graph</title>
      <link>https://jo-member.github.io/2021/02/22/2021-02-22-Boostcamp21.1/</link>
      <guid>https://jo-member.github.io/2021/02/22/2021-02-22-Boostcamp21.1/</guid>
      <pubDate>Sun, 21 Feb 2021 15:00:00 GMT</pubDate>
      
      <description>&lt;p&gt;그래프란 정점과 간선으로 이루어진 구조&lt;/p&gt;
&lt;p&gt;하나의 간선은 반드시 두개의 정점을 연결한다&lt;/p&gt;
&lt;p&gt;정점 : vertex,node&lt;/p&gt;
&lt;p&gt;간선 : Edge,link&lt;/p&gt;
&lt;p&gt;우리의 사회및 모든 다양한 것들은 구성요소간의 복잡한 살호작용으로 이루어진 복잡계이다&lt;/p&gt;
&lt;p&gt;이것을 표현하는 방식이 바로 그래프이다&lt;/p&gt;
&lt;p&gt;그래프란 복잡계를 간단하게 표현하는 방식이다&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>그래프란 정점과 간선으로 이루어진 구조</p><p>하나의 간선은 반드시 두개의 정점을 연결한다</p><p>정점 : vertex,node</p><p>간선 : Edge,link</p><p>우리의 사회및 모든 다양한 것들은 구성요소간의 복잡한 살호작용으로 이루어진 복잡계이다</p><p>이것을 표현하는 방식이 바로 그래프이다</p><p>그래프란 복잡계를 간단하게 표현하는 방식이다</p><span id="more"></span><p>정점 분류문제 (node classification) ex) 어떠한 계정이 어떠걸 리트윗했는지를 간선으로 표현. 사람(node)의 보수성, 진보성을 판별하는 </p><p>랭킹 및 정보검색문제 : 웹이라는 거대한 그래프로부터 어떻게 중요한 웹페이지를 찾아낼까?</p><p>군집분석문제 : 연결관계로 부터 사회적 무리(군집)을 찾아낼 수 있을까?</p><p>정보전파 &amp; 바이럴 마케팅 문제 : 정보라는 것이 어떻게 네트워크를 통해 전파가 될까?</p><p>본강의에서는 위의 문제들을 해결하는 기술들을 배우게될 예정</p><p>1주일이라는 짧은 시간이라 기초를 배우고 직관적인 방법론적인 설명</p><p>간선에 방향이 있는 directed graph vs undirected graph</p><p>협업관계그래프, 페이스북 친구그래프 : undirectied</p><p>인용그래프, 트윈터 팔로우 그래프 : directed</p><p>간선에 가중치가 있는 그래프 : 전화그래프, 유사도 그래프</p><p>간선에 가중치가 없는 그래프 : 페이스북 친구 그래프, 웹그래프</p><p>동종 그래프 vs 이종그래프</p><p>이종그래프는 두종류의 node를 가진다 . 서로다른 정점 사이에만 간선이 연결된다. Ex) 사용자,상품사이의 전자상거래 내역</p><p>동종그래프는 단일종류의 정점을 가진다</p><p>node의 집합 V, edge의 집합 : E, </p><p>G = (V,E)</p><p>N<del>out</del>(1), N<del>in</del>(1)이런거</p><p>그래프의 표현 및 저장</p><p>Networkx를 사용하여 그래프를 표현</p><p>snap.py라는 라이브러리도 많이 사용한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><p>인접 리스트</p><p>1 : [2.5]</p><p>2 : [1,3,5]</p><p>3: [2]</p><p>5: [1,2]</p><p>인접행렬</p><p>간선이 있으면 1, 없으면 0</p><p>방향성이 없으면 대각으로 대칭</p><p>있으면 다름</p><p>희소행렬을 사용하면 저장공간을 절약할 수 있음 (대부분의 원소가 0일때)</p><h2 id="1-실제그래프-vs-랜덤그래프"><a href="#1-실제그래프-vs-랜덤그래프" class="headerlink" title="1. 실제그래프 vs 랜덤그래프"></a>1. 실제그래프 vs 랜덤그래프</h2><p>실제 그래프란 다양한 복잡계로부터 얻어진 그래프를 의미한다</p><p>본수업에서는 MSN 메신저 그래프를 실제 그래프의 예시로 사용하겠다.</p><p>랜덤그래프란 확률적 과정을 통해 생성된 그래프를 의미한다</p><h3 id="에르되스-레니-랜덤그래프"><a href="#에르되스-레니-랜덤그래프" class="headerlink" title="에르되스-레니 랜덤그래프"></a>에르되스-레니 랜덤그래프</h3><p>임의의 두 node사이의 간선 존재여부가 동일한 확률분포로 나타내어짐</p><p>G(n,p)는 n개의 정점, 두개의 정점 사이에 간선이 존재할 확률 = p</p><h2 id="2-작은-세상-효과"><a href="#2-작은-세상-효과" class="headerlink" title="2. 작은 세상 효과"></a>2. 작은 세상 효과</h2><p>정점 u와 v사이의 경로란</p><p>u에서 시작해서 v에서 끝나야 한다</p><p>순열에서 연속된 정점은 반드시 간선으로 연결되어있어야 한다.</p><p>경로, 거리, 및 지름</p><p>경로는 여러가지지만 이중 가장 짧은 경로의 길이가 거리이다</p><p>그래프에서 지름은 정점간 거리의 최댓값이다.</p><p>작은 세상 효과</p><p>임의의 두사람을 골랐을 때 이들은 몇단계의 지인을 거쳐야 연결되는가?</p><p>위치타에서 보스턴까지 지인을 6단게거치면 가능</p><p>MSN에서도 정점간의 평균거리는7정도밖에 되지 않는다</p><p>단 거대연결구조만을 고려하였다.</p><p>이러한 현상을 작은세상효과라고 한다.</p><p>작은 세상효과는 높은 확률로 랜덤그래프에도 존재한다.</p><p>체인 사이클 격자그래프에는 이 작은세상그래프효과가 존재하지 않는다.</p><h2 id="연결성에-두터운-꼬리분포"><a href="#연결성에-두터운-꼬리분포" class="headerlink" title="연결성에 두터운 꼬리분포"></a>연결성에 두터운 꼬리분포</h2><p>연결성?</p><p>정점의 Degree란 그정점과 연결된 간선의 수 : |N(v)| 라고 표현하기도 함</p><p>랜덤그래프의 연결성 분포는 높은 확률로 정규분포와 유사하다</p><p>실제 그래프는 연결성이 두터워서 hub 정점이 존재할 수있는데</p><p>랜덤그래프에서는 정규분포를 띌 가능성이 높다</p><h2 id="연결요소"><a href="#연결요소" class="headerlink" title="연결요소"></a>연결요소</h2><ol><li>연결요소에 속하는 정점들은 경로로 연결될 수 있습니다.</li><li>1의 조건을 만족하면서 정점을 추가할 수 없다.</li></ol><p>실제그래프에는 대다수의 정점을 포함하는 거대연결요소가 존재한다</p><p>MSN메신저 그래프에는 99,9%의 정점이 하나의 거대연결요소에 포함된다</p><p>정점들의 평균 연결성이 1보다 충분히 큰경우, 랜덤그래프에도 높은 확률로 거대연결 요소가 존재한다.</p><p>군집이란 정점들의 집합</p><p>같은 군집안에서의 정점 사이에는 많은 edge가 존재</p><p>지역적 군집 계수 : 그 정점이 군집을 형성하려는 정도</p><p>C<del>i</del> = 정점 i의 이웃쌍중 간선으로 직접 연결된 것의 비율</p><p>정점i의 지역적 군집계수가 높으면 이웃들이 연결되어있다.-&gt; 정점 i와 이웃들이 군집을 형성한다</p><p>전역 군집 계수</p><p>전체 그래프에서 군집의 형성정도를 측정</p><p>각 정점에서 지역적 군집계수의 평균이다. 단 지역적 군집계수가 정의가 안되면 짤</p><p>세상에는 많은 군집이 존재한다 </p><ol><li><p>homophily : 유사한 정점끼리는</p></li><li><p>공통이웃이 있는경우 공통이웃이 두정점을 매개하는 역할</p></li></ol><p><img src="/images/image-20210222131012470.png" alt="image-20210222131012470"></p>]]></content:encoded>
      
      
      <category domain="https://jo-member.github.io/categories/Boostcamp/">Boostcamp</category>
      
      
      <category domain="https://jo-member.github.io/tags/Graph/">Graph</category>
      
      
      <comments>https://jo-member.github.io/2021/02/22/2021-02-22-Boostcamp21.1/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Transformer심화</title>
      <link>https://jo-member.github.io/2021/02/18/2021-02-18-Boostcamp19.1/</link>
      <guid>https://jo-member.github.io/2021/02/18/2021-02-18-Boostcamp19.1/</guid>
      <pubDate>Wed, 17 Feb 2021 15:00:00 GMT</pubDate>
      
      <description>&lt;h1 id=&quot;Transformer&quot;&gt;&lt;a href=&quot;#Transformer&quot; class=&quot;headerlink&quot; title=&quot;Transformer&quot;&gt;&lt;/a&gt;Transformer&lt;/h1&gt;&lt;br/&gt;

&lt;h2 id=&quot;Self-Attention&quot;&gt;&lt;a href=&quot;#Self-Attention&quot; class=&quot;headerlink&quot; title=&quot;Self-Attention&quot;&gt;&lt;/a&gt;Self-Attention&lt;/h2&gt;&lt;p&gt;ex) I go home&lt;/p&gt;
&lt;p&gt;I에 대한 input vector가 hidden state처럼 역할을 하여서&lt;/p&gt;
&lt;p&gt;I와 각각의 단어에 대한 내적을 한후 이에대한 softmax를 구하여 가중평균을 구한다.&lt;/p&gt;
&lt;p&gt;이렇게 encoding vector값을 구하게 되면 결국 자기자신과 내적한 값이 큰값을 가져, 자기 자신에 대한 특성만이 dominant하게 담길것이므로, 이를 해결해주기 위해 다른 architecture를 쓴다&lt;/p&gt;
&lt;p&gt;각 vector들이 3가지의 역할을 하고있는 것이다. 동일한 set의 vector에서 출발했더라도 각혁할에 따라 vector가 서로다른형태로 변환할수있게해주는 linear transformation matrix가 있다.&lt;/p&gt;
&lt;p&gt;한마디로 각각의 input이 서로다른 matrix에 적용이되어 각각이 key, quary,value가 된다는 의미이다.&lt;/p&gt;
&lt;p&gt;I 라는 word가 서로다른 matrix에 따라 quart, key, value값이 만들어지고 쿼리는 1개이고 이 쿼리 벡터와 각각의 key vector와의 내적값을 구하고 결과를 softmax에 통과시켜 가중치를 구한후 , 이값과 value vector를 각각 곱해주어 이들의 가중평균으로 최종적인 vector를 구하다. 결국 이 vector가 feature들이 담긴 encoding vector이다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/image-20210218113022613.png&quot; alt=&quot;image-20210218113022613&quot;&gt;&lt;/p&gt;
&lt;p&gt;이러하게 행렬연산으로 위의 과정을 한번에 처리할 수 있다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><br/><h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><p>ex) I go home</p><p>I에 대한 input vector가 hidden state처럼 역할을 하여서</p><p>I와 각각의 단어에 대한 내적을 한후 이에대한 softmax를 구하여 가중평균을 구한다.</p><p>이렇게 encoding vector값을 구하게 되면 결국 자기자신과 내적한 값이 큰값을 가져, 자기 자신에 대한 특성만이 dominant하게 담길것이므로, 이를 해결해주기 위해 다른 architecture를 쓴다</p><p>각 vector들이 3가지의 역할을 하고있는 것이다. 동일한 set의 vector에서 출발했더라도 각혁할에 따라 vector가 서로다른형태로 변환할수있게해주는 linear transformation matrix가 있다.</p><p>한마디로 각각의 input이 서로다른 matrix에 적용이되어 각각이 key, quary,value가 된다는 의미이다.</p><p>I 라는 word가 서로다른 matrix에 따라 quart, key, value값이 만들어지고 쿼리는 1개이고 이 쿼리 벡터와 각각의 key vector와의 내적값을 구하고 결과를 softmax에 통과시켜 가중치를 구한후 , 이값과 value vector를 각각 곱해주어 이들의 가중평균으로 최종적인 vector를 구하다. 결국 이 vector가 feature들이 담긴 encoding vector이다.</p><p><img src="/images/image-20210218113022613.png" alt="image-20210218113022613"></p><p>이러하게 행렬연산으로 위의 과정을 한번에 처리할 수 있다.</p><span id="more"></span><h2 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h2><p><img src="/images/image-20210218115609302.png" alt="image-20210218115609302"></p><p>쿼리,key,value를 만들때 여러 set의 matrix를 적용하여 여러 attention을 수행한다. 이러한 서로다른 선형변환 matrix를 head라고 부른다. 동일한 sequence에서 특정한 quary에 대해서 여러측면으로 정보를 뽑아야하는 경우가 있다. </p><p><img src="/images/image-20210218115717813.png" alt="image-20210218115717813"></p><p><img src="/images/image-20210218120149327.png" alt="image-20210218120149327"></p><p>이후 하나의 선형변환 layer를 추가하여 우리가 원하는 shape의 output을 얻어낸다. 왜 이러한 shape으로 변환해야할까? for residual connection</p><p>Residual connection을 사용했다. 이는 CV에서 널리쓰이던 Resnet에서 사용한 residue개념을 활용하여, attention 결과의 encoded vector와 원래 입력 vector를 더한다. 이러한 과정을 통해 gradient vanishing과 학습의 속도를 해결하였다.</p><p>Layer Normalization</p><p>주어진 sample에 대해서 그값들의 평균을 0 분산을 1로 만들어준후 우리가 원하는 평균과 분산을 만들어주는 선형변환으로 이루어져있다.<img src="/images/image-20210218122035821.png" alt="image-20210218122035821"></p><p>표준화된 평균과 분산으로 만들어줌. 이후 affine transformation (ex : y = 2x+3)을 수행할 경우 평균은 3 분산은 4가 된다. 여기서의 2와 3은 NN이 optimize해야하는 paramter가 된다. 위의 transformer에도 이런식으로 적용이된다</p><p><img src="/images/image-20210218122412734.png" alt="image-20210218122412734"></p><p>affine transformation은 이제 parameter이라 학습하는? 왜성능이 올라갈까?</p><p>Transformer에서의 self attention은 순서의 정보를 담고있지 않기 때문에 추가적인 작업이 필요하다. 이 작업을 transformer은 postition encoding에 sinusodial function을 적용하였다.</p><p>Optimizer은 graident descent가 아닌 Adam을 사용하였다. Learning rate을 고정한 값을 사용하지 않고 학습중에 lr을 변경시켜주었다. </p><h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p><SOS>, 나는, 집에, 간다</p><p>Masked Multi-Head Attention의 결과가가 얻어졌다면 이를 다시 multi-head attention에 넣어준다. 그데 이제 quary에만 이게 사용되고 encoding단의 encoded vector가 이제 key와 value값에 들어가게 된다. 이제 target language의 vocab size에 맞는 vector를 생성하는 linear transformation을 걸어준다. 그곳에 soft max를 취해서 다음 word를 찾아낸다. 이제 ground truth와의 softmax loss를 구해서 backpropagation으로 학습해 나간다.</p><p><strong>Masked Self Attention</strong></p><p>전체 sequence에 대한 정보를 허용하게 되면 첫번째 time step에서 SOS만이 주어졌는데 나는이라는 단어를 예측해야 하는데 나는이라는 값과 sos사이의 행렬곱값이 있기 때문에 이상황에는 이를 masking해주어야 한다.</p><p><img src="/images/image-20210218135058123.png" alt="image-20210218135058123"></p><p>이렇게 masking해준후 normalize를 해주게 된다. 가중평균의 합이 1이 되도록</p>]]></content:encoded>
      
      
      <category domain="https://jo-member.github.io/categories/Boostcamp/">Boostcamp</category>
      
      
      <category domain="https://jo-member.github.io/tags/Transformer/">Transformer</category>
      
      <category domain="https://jo-member.github.io/tags/NLP/">NLP</category>
      
      
      <comments>https://jo-member.github.io/2021/02/18/2021-02-18-Boostcamp19.1/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Sequence to sequence with Attention</title>
      <link>https://jo-member.github.io/2021/02/17/2021-02-15-Boostcamp18.1/</link>
      <guid>https://jo-member.github.io/2021/02/17/2021-02-15-Boostcamp18.1/</guid>
      <pubDate>Tue, 16 Feb 2021 15:00:00 GMT</pubDate>
      
      <description>&lt;h1 id=&quot;Sequence-to-sequence&quot;&gt;&lt;a href=&quot;#Sequence-to-sequence&quot; class=&quot;headerlink&quot; title=&quot;Sequence to sequence&quot;&gt;&lt;/a&gt;Sequence to sequence&lt;/h1&gt;&lt;br/&gt;

&lt;p&gt;\&lt;/p&gt;
&lt;h2 id=&quot;Seq2Seq-Model&quot;&gt;&lt;a href=&quot;#Seq2Seq-Model&quot; class=&quot;headerlink&quot; title=&quot;Seq2Seq Model&quot;&gt;&lt;/a&gt;Seq2Seq Model&lt;/h2&gt;&lt;p&gt;Ex) Are you free tomorrow?&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/image-20210217110021239.png&quot; alt=&quot;image-20210217110021239&quot;&gt;&lt;/p&gt;
&lt;p&gt;서로 paramter를 share하지 않는 2개의 별개의 RNN model을 (보통 LSTM) 쓴다. 각각의 RNN을 Decoder, Encoder로 사용한다.&lt;/p&gt;
&lt;p&gt;Encoder의 마지막단의 output을 vertorize 시켜준후 decoder의 input에는 SOS token, hidden state에는 encoder의 output을 넣어준다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="Sequence-to-sequence"><a href="#Sequence-to-sequence" class="headerlink" title="Sequence to sequence"></a>Sequence to sequence</h1><br/><p>\</p><h2 id="Seq2Seq-Model"><a href="#Seq2Seq-Model" class="headerlink" title="Seq2Seq Model"></a>Seq2Seq Model</h2><p>Ex) Are you free tomorrow?</p><p><img src="/images/image-20210217110021239.png" alt="image-20210217110021239"></p><p>서로 paramter를 share하지 않는 2개의 별개의 RNN model을 (보통 LSTM) 쓴다. 각각의 RNN을 Decoder, Encoder로 사용한다.</p><p>Encoder의 마지막단의 output을 vertorize 시켜준후 decoder의 input에는 SOS token, hidden state에는 encoder의 output을 넣어준다.</p><span id="more"></span><h2 id="Seq2Seq-with-Attention"><a href="#Seq2Seq-with-Attention" class="headerlink" title="Seq2Seq with Attention"></a>Seq2Seq with Attention</h2><p>앞에서의 RNN을 사용한 model은 hidden state vector의 dimesion이 정해져 있어서 입력문장의 길이가 길어지면 마지막 time step에 있는 hiddenstate vector에 앞서 나왔던 많은 정보들이 잘 담겨져 있지 않다.</p><p>아무리 이 LSTM에서 longterm dependency를 해결하려 해도구조상의 문제 때문에 해결하기에 매우힘들다</p><p>따라서 이러한 문제를 해결하기 위해서 seq2seq에서 Attention을 활용할 수 있다. Attention은 encoder의 각각의 hidden state vector를 전체적으로 decoder에 제공해주고 decoder에서는 그때그때 필요한 encoder의 hidden state vector를 가져가서 사용한다</p><p>decoder의 hidden state vector가 encoder의 어떤 hidden state vector를 가져올지를 결정하게 된다. 이거는 각각을 내적해보아서, 내적에 기반한 유사도를 판별하게 되고 이결과를 softmax에 통과 시켜서 확률값을 얻어내고 이를 각각의 가중치로 사용하여 이들의 가중평균으로서 나오는 하나의 encoding vector를 얻어낼수 있다!!!!!! 이러한 가중평균으로 나온 하나의 vector를 우리는 context vector라고 부른다.  </p><p><img src="/images/image-20210217111855000.png" alt="image-20210217111855000"></p><p>이후에 decoder hidden state vector와 context vector가 concatnate 되어 output layer의 입력으로 들어가게 되고 다음나올 단어를 예측할 수 있게 된다</p><p>이러한 과정들을 EOS가 나올때 까지 반복한다.</p><p>잘못된 단어를 전단계에서 예측을 하더라도 다음단계에는 올바른 ground truth를 넣어주기 떄문에 하나가 틀려도 이후가 망가지지 않는다.  학습이 끝난후 이 잘못된 단어를 다시 넣어준다. 또한 It’s teacher forcing.</p><p>Teacher forcing이 아닌 방식이 학습후에 우리가 실제로 사용할때와 비슷하다.</p><p>Teacher forcing때는 ground truth를 넣어주어야 하기 때문에학습속도가 빠르다</p><p>학습의 전반부에는 teacher forcing을 사용후  어느정도 학습이 되면, 이전의 output을 다시 입력으로 넣어주는 방식으로 진행한다.</p><br/><p><img src="/images/image-20210217120519836.png" alt="image-20210217120519836"></p><p>이처럼 유사도를 측정하는 과정에서 사용되는 내적은, 3가지의 종류로 계산해 낼 수 있다.</p><p>2번째인 general 방식으로 게산하는 것을 행렬으로 생각해보자.내적을 기반한 계산을 행렬의 곱으로 생각해보면,  </p><p><img src="/images/image-20210217120331715.png" alt="image-20210217120331715"></p><p>대각행렬의 성분들은 같은 차원끼리의 가중치를 나타내고, 나머지 값들은 다른 차원끼리의 곱해진 값들의 가중치를 나타낸다</p><p>이처럼 간단한 내적으로 정의된 형태의 유사도를 그가운데 학습가능한 parameter를 추가함으로서 새롭게 score를 계산했다.</p><p>이게 바로 general한 dot product이다.</p><br/><p>다음으로 concat을 사용한 score 측정 방식을 보자</p><p><img src="/images/image-20210217120926535.png" alt="image-20210217120926535"> </p><p>이처럼 2개의 vector를 concat시켜 MLP의 입력으로 넣어준 후 non linear activation function을 적용하여 값을 구해낸다.</p><p><img src="/images/image-20210217121132215.png" alt="image-20210217121132215"></p><p>이수식을 간단하게 보면 Wa는 1번째 layer의 가중치, 그이후에 tanh를 적용한 후 v를 곱해주는데 이는 우리가 최종적으로 얻어야할 output이 scalar값이기 떄문에 v는 row의 형태를 띄어야 한다. 따라서 tranpose를 시켜준것을 확인 할 수 있다.</p><br/><p>그렇다면 이들의 paramter은 어떠힌 방식으로 update될까?</p><p>결국은 이러한 유사도를 구하는데 필요한 parameter들또한 backpropagation을 통하여 선형변환 행렬들이 학습되게 된다.</p><br/><h2 id="Attention-is-great"><a href="#Attention-is-great" class="headerlink" title="Attention is great"></a>Attention is great</h2><ul><li><p>Attention significantly impoves NMT performace</p><p>어떠한 한 부분에 집중할 수 있게 해주었다</p></li><li><p>It solves bottle neck problem</p><p>encoder의 마지막을 사용했어야 해서 생기는 long term dependency를 해결</p></li><li><p>Gradient vanishing의 문제를 해결하였다.</p></li><li><p>Attention provides some interpretability</p><p>우리가 transform과정에서 모델이 어떠한 부분에 집중 했는지를 확인 할 수 있다. Allignment를 NN이 스스로 배우는 현상을 보여주게 된다.</p></li></ul><h1 id="Beam-search"><a href="#Beam-search" class="headerlink" title="Beam search"></a>Beam search</h1><ul><li>test과정에서 더 좋은 결과를 얻을수 있게 해주는 하나의 방법</li></ul><h2 id="Greedy-decoding"><a href="#Greedy-decoding" class="headerlink" title="Greedy decoding"></a>Greedy decoding</h2><p>가장 높은 확률을 가지는 단어 1개를 선택하는 방법</p><p>이렇게 되면 어떠한 단어를 잘못 생성해내었을때 다시 뒤로 돌아갈수 없어 최적의 예측값을 내지 못하게 된다</p><p>이를 해결하기 위해서 다양한 방법들이 제시된다</p><br/><h2 id="Exhaustive-Search"><a href="#Exhaustive-Search" class="headerlink" title="Exhaustive Search"></a>Exhaustive Search</h2><p>첫번째 생성하는 단어가 가장큰 확률이였다고 해도 뒷부분에서 나오는 확률값 가장큰 확률값이 아닌 경우가 발생될수가 있다.</p><p><img src="/images/image-20210217125708440.png" alt="image-20210217125708440"></p><p>이는 결국 time step t 까지의 가능한 모든경우를 따져서 이는 곧 vocab가지수가 되고 V^t^가 가능한 모든 경우의 수이다. 이는 너무 큰 숫자이기 때문에 beam search를 쓰게된다</p><br/><h2 id="Beam-search-1"><a href="#Beam-search-1" class="headerlink" title="Beam search"></a>Beam search</h2><p>매 time step마다 모든 경우의 수를 고려하는게 아니라, 우리가 정해놓은 k개의 가능하 가짓수를 고려하고 마지막까지 decoding을 진행한후 k개의 candidate중에서 가장확률값이 높은걸 선택하는 방식이다.</p><p>이를 우리는 hypothesis (가설)이라고 부른다</p><p>k는 beam size이 일반적으로 5~10으로 설정하게 된다.</p><p> <img src="/images/image-20210217130106952.png" alt="image-20210217130106952"></p><p>확률들의 곱셈 앞에 log를 붙이게 되면 곱들이 모두 덧셈이 된다. 여기서 log함수 단조증가이기 때문에, 큰값이 큰값을 가진다.</p><p>ex) k = 2</p><ol><li> k가 2이기 때문에 가장 확률값이 높은 2개의 단어를 뽑는다</li></ol><p><img src="/images/image-20210217130719246.png" alt="image-20210217130719246"></p><ol start="2"><li>이중 값이 큰걸 계속해서 선택해 나감</li></ol><p><img src="/images/image-20210217130908355.png" alt="image-20210217130908355"></p><ul><li>greedy의 경우 end token이 나왔을때가 종료이지만, beam search에서는 서로다른 시점에서 end token이 생성되기 때문에, 각각이 끝날때마다 한곳에 저장해준다.</li></ul><p>우리가정한 T라는 시간까지 수행하거나, 완료된 hypothesis가 n개가 되었을때 beam search를 중단한다.</p><p>우리가 고려하는 hypotheses의 길이가 다를때는 상대적으로 짧은 길이의 확률이 높은것이고, 길면 낮을것이다. </p><p>이를 고려해 주기 위해서는 각 joint prob을 문장의 길이로 나눔으로서 해결해줄 수 있다.</p><h2 id="BLEU-score"><a href="#BLEU-score" class="headerlink" title="BLEU score"></a>BLEU score</h2><ul><li>생성 model의 점수를 평가하기 위한 척도</li><li>고정된 위치에서 정해진 단어가 나와야 된다는 평가방식은 매우 나쁜 방식이다.</li></ul><p>ex)</p><p>Reference : Half of my heart is in Havana ooh na na</p><p>Predicted :  Half as my heart is in Obama ohh na</p><p>Precision(실제로 위치상관없이 겹치는 단어가 몇개인가) = #(correct words)/length_of_prediction = 7/9</p><p>Recall(재현률)  = #(correct words)/length_of_reference = 7/10</p><p>F-measure = (precision x recall) / 0.5(precision + recall) (두 값들의 조화평균)</p><p>보다 작은 작은 값에 가깝게 구하는 방식 -&gt; 조화평균</p><p>이렇게 구한 값들은 순서를 보장하지 않기 때문에 BLEU가 나왔다.</p><h3 id="BiLingual-Evaluation-Understudy"><a href="#BiLingual-Evaluation-Understudy" class="headerlink" title="BiLingual Evaluation Understudy"></a>BiLingual Evaluation Understudy</h3><p><strong>Ngram</strong>이란걸 사용했다. 연속된 N개의 단어로 이루어진 문구를 matching하여점수로 반영하였다.</p>]]></content:encoded>
      
      
      <category domain="https://jo-member.github.io/categories/Boostcamp/">Boostcamp</category>
      
      
      <category domain="https://jo-member.github.io/tags/RNN/">RNN</category>
      
      <category domain="https://jo-member.github.io/tags/NLP/">NLP</category>
      
      
      <comments>https://jo-member.github.io/2021/02/17/2021-02-15-Boostcamp18.1/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>RNN심화1</title>
      <link>https://jo-member.github.io/2021/02/16/2021-02-15-Boostcamp16.1/</link>
      <guid>https://jo-member.github.io/2021/02/16/2021-02-15-Boostcamp16.1/</guid>
      <pubDate>Mon, 15 Feb 2021 15:00:00 GMT</pubDate>
      
      <description>&lt;br/&gt;

&lt;h1 id=&quot;RNN&quot;&gt;&lt;a href=&quot;#RNN&quot; class=&quot;headerlink&quot; title=&quot;RNN&quot;&gt;&lt;/a&gt;RNN&lt;/h1&gt;&lt;p&gt;서로다른 time step에서 들어오는 입력 데이터를 처리할때, 매번 반복되는 동일한 rnn module을 호출한다.&lt;img src=&quot;/images/image-20210216103443317.png&quot; alt=&quot;image-20210216103443317&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/image-20210216103929384.png&quot; alt=&quot;image-20210216103929384&quot;&gt;&lt;/p&gt;
&lt;p&gt;각 단어별로 품사를 예측해야 되는 경우 -&amp;gt; 매 time step마다 y를 output으로&lt;/p&gt;
&lt;p&gt;어떠한 문장의 긍부정을 판별하는 경우 -&amp;gt; 최종 time step의 y만이 output으로&lt;/p&gt;
&lt;p&gt;모든 time step에서 같은 parameter W를 공유한다&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<br/><h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p>서로다른 time step에서 들어오는 입력 데이터를 처리할때, 매번 반복되는 동일한 rnn module을 호출한다.<img src="/images/image-20210216103443317.png" alt="image-20210216103443317"></p><p><img src="/images/image-20210216103929384.png" alt="image-20210216103929384"></p><p>각 단어별로 품사를 예측해야 되는 경우 -&gt; 매 time step마다 y를 output으로</p><p>어떠한 문장의 긍부정을 판별하는 경우 -&gt; 최종 time step의 y만이 output으로</p><p>모든 time step에서 같은 parameter W를 공유한다</p><span id="more"></span><br/><p>주어진 vector가 3차원의 입력벡터로 주어졌을때 h<del>t-1</del>은 2차원이라고 가정하자 </p><p>x<del>t</del>와 h<del>t-1</del>를 같이 입력으로 받아서 f<del>W</del>에 넣어주면, h<del>t</del>가 나오게 된다<img src="/images/image-20210216111255186.png" alt="image-20210216111255186"></p><p>현재 timestep t에서추가적인 outputlayer를 만들고 h<del>t</del>에 W<del>hy</del>를 곱해서 y<del>t</del>를 얻어낸다.</p><h2 id="Types-of-RNN"><a href="#Types-of-RNN" class="headerlink" title="Types of RNN"></a>Types of RNN</h2><p><strong>One-to-one</strong></p><p>입출력 모두가 sequence data인 경우에 입출력이 단 1개인</p><p><strong>one-to-many</strong></p><p>image captioning에서 이러한 구조를 띈다. </p><p>초기에 입력이 한번 들어가고 이후 입력으로는 0으로 채워진 tensor를 입력으로 주게된다</p><p><strong>many-to-one</strong></p><p>최종값을 마지막에서야 내주는</p><p>ex) I love movie에서 RNN이 처리한후 마지막의 h<del>t</del>를 봄으로서 긍부정을 예측하게 된다. 길이가 달라진다면 RNN CELL이 그만큼 확장이된다</p><p><strong>many-to-many</strong></p><ol><li>ex) machine translation</li></ol><img src="/images/image-20210216120729609.png" alt="image-20210216120729609" style="zoom:50%;" /><ol start="2"><li>Ex) POS, vidio의 frame이 sequence대로 주어질때</li></ol><h2 id="Character-level-Language-Model"><a href="#Character-level-Language-Model" class="headerlink" title="Character-level Language Model"></a>Character-level Language Model</h2><ul><li>Example of training sequence “hello”</li><li>vocab = [h,e,l,o]</li><li>각각의 character은 one-hot-vector로 표현이 가능하다</li></ul><img src="/images/image-20210216121012818.png" alt="image-20210216121012818" style="zoom:50%;" /><h2 id="Back-propagation-through-time-BPTT"><a href="#Back-propagation-through-time-BPTT" class="headerlink" title="Back propagation through time (BPTT)"></a>Back propagation through time (BPTT)</h2><p>Whh,Why,Wxh 와 같은 parameter들을 학습한다</p><p>sequence전체를 한번에 학습하기에는 physical적인 한계가 존재하기 때문에 군데군데 짤라서 제한된 길이의 sequenc 만으로 학습을 진행한다</p><p>매  time step마다 hidden state vector가 거의 모든 정보를 담고 있다. 그렇다면 만약에 hidden state의 차원이 3차원이라면, 우리가 원하는 정보가 그중 어느 node에 담겨져 있을까? 이걸 역추적. 첫번째 ht의 node를 고정해 놓고 이후의 변화들을 봄</p><br/><p>정작 지금까지 배운 vanila RNN은 잘 활용하지 않는다. 이유는 만약 긴거리에 있는 정보가 매우 중요할 경우 back propagtion으로 구해지기 때문에 gradient vanishing이나 gradient explode가 일어나게 된다. </p><p><img src="/images/image-20210216124337509.png" alt="image-20210216124337509"></p><p>gradient값이 증폭되고있다</p><br/><h1 id="LSTM-amp-GRU"><a href="#LSTM-amp-GRU" class="headerlink" title="LSTM &amp; GRU"></a>LSTM &amp; GRU</h1><h2 id="Long-short-term-Memory"><a href="#Long-short-term-Memory" class="headerlink" title="Long short-term Memory"></a>Long short-term Memory</h2><p>보다 효과적으로  long term dependency를 처리할수 있게끔하기 위해</p><p>h<del>t</del>를 단기 기억소자로 생각할 수 있으며, 이러한 단기기억을 얼마나 길게 끌고갈 것이지를 판별해주는 역할들을 가진 gate들로 이루어져 있다</p><p>전 time step에서 넘어오는 정보가 2가지의 서로다른 vector가 들어오게 된다.</p><p>위에 들어오는 vector : C<del>t</del></p><p>아래쪽에 들어오는 vecor : h<del>t</del></p><p>Í<img src="/images/image-20210216125806943.png" alt="image-20210216125806943"></p><p>C<del>t-1</del> 이전 cell state와 이전 state의 hidden state를 입력으로 받아 현재의 cs와 ss를 내준다. Hidden state vector은 cell state vector중에 노출되는 정보를 담은, 한번 필터링 된 vector이다.</p><p><img src="/images/image-20210216130159755.png" alt="image-20210216130159755"></p><p>여기서 sigmoid의 결과와 곱해지면 얼마만큼 이전의 원래값을 반영할지를 결정하는 역할을 한다. 마지막 tanh를 통해 나오는 값은 현재 time step에서 LSTM에서 계산되는 유의미한 정보라고 생각할 수 있다.</p><ol><li><p>Forget gate</p><p><img src="/images/image-20210216131518759.png" alt="image-20210216131518759"></p></li></ol><p>위를 보면 이전의 hidden state와 현재의 x<del>t</del>를 입력으로 받아 sigmoid 적용후 3차원의 vector가 나오게 되었다. 이렇게 나온 vector와 이전의 cell state의 element wise product를 해주어서 이전의 cell state를 얼마만큼 반영할지를 게산해 주었다.</p><ol start="2"><li>Gate gate</li></ol><p><img src="/images/image-20210216132213172.png" alt="image-20210216132213172"></p><p>C<del>t</del>에 더해주어야 하는 값을 바로 더해주지 않고 i<del>t</del>를 곱해서 더해준다</p><ol start="3"><li>Output gate</li></ol><p><img src="/images/image-20210216132521862.png" alt="image-20210216132521862"></p><p>이제 cell state vector C<del>t</del>로 hidden state vector h<del>t</del>를 만들어준다. 앞서 sigmoid를  적용한 값또한 tanh를 거친 Celll state에 곱한값에 곱해주어 적절한 비율만큼 값을 작게 만들어주어 최종적인 h<del>t</del>를 만들어주게 된다.</p><p>h<del>t</del>는 다음 rnn의 hidden state로 들어가는 동시에 현재 time step에서 예측을 수행할때 이걸 output layer에 넘겨주어 예측값을 생성해 낸다</p><h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><p>LSTM에서 2가지 종류의 vector로 존재하던 cell state와 hidden state vector를 일원하 하여 하나의 vector만이 존재하게 한다는게 특징이다. 하지만 전체적인 동작원리는 거의 비슷</p><p><img src="/images/image-20210216142225794.png" alt="image-20210216142225794"></p><p>forget gate대신 1-z<del>t</del>를 사용, i<del>t</del>대신 z<del>t</del>를 사용</p><p>input gate가 커질수록 forget gate의 값이 점차 작아지게 되어 결과적으로 이전 hidden state vector를 더 적게 반영하는 것이고, vice versa</p><ol><li>hidden state를 일원화 하였다</li><li>2개의 독립된 gate를 통하여 동작되었던 model을 하나의 gate만으로 줄여 계산량과 메모리 사용량을 줄였다.</li></ol><p>정보를 주로담는 cell state가 update되는 과정이 행렬의 계속적인 곱의 연산이 아니라 그때그때 서로다른 gate를 거쳐가며 update되기 때문에 gradient vanishing이 사라진다. 덧셈연산은 이전의 state를 복사해주어 gradient를 유지하는 역할을 한다고 볼 수도 있다. RNN은 다양한 길이를 가질수 있는 유연한 형태의 deep learning구조.</p>]]></content:encoded>
      
      
      <category domain="https://jo-member.github.io/categories/Boostcamp/">Boostcamp</category>
      
      
      <category domain="https://jo-member.github.io/tags/RNN/">RNN</category>
      
      <category domain="https://jo-member.github.io/tags/NLP/">NLP</category>
      
      
      <comments>https://jo-member.github.io/2021/02/16/2021-02-15-Boostcamp16.1/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Improving Language Understanding by Generative Pre-Training</title>
      <link>https://jo-member.github.io/2021/02/14/2021-02-14-GPT/</link>
      <guid>https://jo-member.github.io/2021/02/14/2021-02-14-GPT/</guid>
      <pubDate>Sat, 13 Feb 2021 15:00:00 GMT</pubDate>
      
      <description>&lt;br/&gt;

&lt;p&gt;이번에는 openai에서 발표한 논문인 GPT를 review해보겠다&lt;/p&gt;
&lt;p&gt;GPT3는 이전에 review한 transformer구조를 활용하여 Language understanding을 효과적으로 만들었다.&lt;/p&gt;
&lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;br/&gt;

&lt;p&gt;자연어를 이해는 text추론, 질문에 대한 대답, 의미의 유사성 평가, 문서분류등을 포함하고 있다. 라벨링 되지 않은 text들을 매우 넘처나지만, 특정 task의 학습을 위해 labed된 text들은 매우 적기때문에 좋은 모델을 학습시키는것은 매우 힘들다.  Language 모델을 unlabled된 text로 &lt;em&gt;generative pretrain&lt;/em&gt;을 한이후 각각의 task에 맞게 fine-tunning을 하였다.  이러한 많은 unlabed text를 사용하여 학습하였다. 이전의 연구와는 달리,필요한 task에 fine-tuning하여 응용하는 것이 매우 효과적이다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<br/><p>이번에는 openai에서 발표한 논문인 GPT를 review해보겠다</p><p>GPT3는 이전에 review한 transformer구조를 활용하여 Language understanding을 효과적으로 만들었다.</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><br/><p>자연어를 이해는 text추론, 질문에 대한 대답, 의미의 유사성 평가, 문서분류등을 포함하고 있다. 라벨링 되지 않은 text들을 매우 넘처나지만, 특정 task의 학습을 위해 labed된 text들은 매우 적기때문에 좋은 모델을 학습시키는것은 매우 힘들다.  Language 모델을 unlabled된 text로 <em>generative pretrain</em>을 한이후 각각의 task에 맞게 fine-tunning을 하였다.  이러한 많은 unlabed text를 사용하여 학습하였다. 이전의 연구와는 달리,필요한 task에 fine-tuning하여 응용하는 것이 매우 효과적이다.</p><span id="more"></span><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><br/><p>Raw text를 사용하여 효과적인 NLP 학습을 하기위해서는 지도학습에 대한 의존성을 완화해야 한다. 많은 딥러닝 방법들은 labeled된 data를 사용해야 해서 한계가 존재한다. 이러한 상황에서 unlabed된 data는 시간과 노력이 필요한 annotation을 모으는 작업들을 대체할 수 있다. 만약 고려가능한 지도가 가능한 상황이라면, unsupervised 방법은 model의 성능을 증가시킬 수 있다. 이러한 방식은 pretrained된 word embedding을 사용하여 성능을 높이는것과 비슷한 이유이다.</p><p>unlabed된 data로 word-level의 정보보다 많은 정보를 활용하는것은 2가지 이유에서 매우 어렵다</p><ol><li>어떠한 종류의 optimization objective가 가장 효과적으로 text를 표현할수 있을까 가 매우 unclear하다</li><li>우리가 원하는 특정 task에 효과적으로 적용하는 방법에 대한 의견이 일치가 되징 않았다. 현재 존재하는 방법은 model에 특정한 task-specific한 변화를 가하는 것과, 복잡한 학습방법,그리고 학습을 도와주는 몇몇 learning objective들을 넣어주는, 이러한 방법들의 combination이다</li></ol><p>이러한 불확실성은 language processing에서의 효과적인 semi-supervised learning을 발전시키기 힘들게 만든다.</p><br/><p>이 논문에서는 unsupervised pre-training과 supervised fine-tunning을 조합한 방법을 사용하여 semi-supervised approach를 하였다. 목적은 가장 보편적으로 학습하여 약간의 응용으로 다양한 분야에 적용시키는것이다. </p><p>2-stage로 나누어 train하였다</p><ol><li>초기 parameter를 학습하기 위해 unlabeled data를 사용하여 pre-train 하였다 / with transformer</li><li>우리는 이 parameter들을 특정한 task에 맞는 supervised objective 학습에 사용하였다.</li></ol><p>또한 model에서 <em>Transformer</em>를 사용하여 long-term dependencies를 해결하였다.</p><br/><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p><strong>Semi-supervised learning for NLP</strong></p><p>우리의 work는 Semi-supervied learning의 범주안에있다. 이 ssl은 sequence labeling, text 분류등에 쓰이면서 큰 관심을 받고 있다. 가장 초기에는 unlabeled data를 supervised learning의 feature로 사용하여 word나 phrase level의 통계를 계산하는데 사용되었다. 최근 몇년동안 word-embedding이 얼마나 좋은지 밝혀냈다. 이러한 접근은 word-level의 정보를 특정한 high-level에 맞추어 준다. 최근에는 word-level이 아닌 phrase나 sentence level의 embedding을 사용하여 text를 다양한 target task의 vector representation을 나타내 주었다.</p><br/><p><strong>Unsupervised pre-training</strong></p><p>Unsupervised pre-training은 supervised learning을 바꾸는거 보다는 좋은 initialization을 찾는게 목적이다. 각각의 연구들은 image classification과 regression task의 기술이 사용되었다. Pre-training은 정규화 과정에서 generalization성능을 올려준다. </p><p>우리의 연구는 language modeling으로 model을 pre-train한후 task에 맞게 fine-tuning해주는 것이다. Pre-training이 언어적인 정보를 잘 잡아낼수 있지만,이전연구에서 사용된 LSTM을 사용하는 것은 긴 data를 해석하지 못한다는 단점이 존재한다. 따라서 우리는 Transformer를 사용하였다.또다른 연구에서는 몇몇 보조적인 feature들을 삽입해주어 성능을 향상시켰지만, 이는 새로운 parameter의 증가를 야기한다. 우리의 GPT는 transfer과정에서 최소한의 수정만을 필요로 한다.</p><br/><p><strong>Auxiliary training objectives</strong></p><p>여러 보조적인 unsupervised training은 semi-supervised learning의 대채적인 형태이다. 이전의 연구에서는 다양한 종류의 보조적인 NLP방법론(POS tagging, chunking,등등등)을 사용하였다. 최근 또다른 연구는 보조적인 language model를 추가하여 sequence labeling의 성능향상을 이야기 하였다. </p><br/><h2 id="3-Framework"><a href="#3-Framework" class="headerlink" title="3. Framework"></a>3. Framework</h2><p>학습과정은 2개의 stage로 나누어져 있다</p><ol><li>unlabeled된 큰 말뭉치를 사용하여 가장 범용적인 language model을 학습하는 stage</li><li>이후 labeled data를 사용한 fine-tuning stage</li></ol><br/><p><strong>3.1 Unsupervised pre-training</strong></p><p>unsupervised의 token들 = <img src="/images/image-20210214174726972.png" alt="image-20210214174726972">이 주어지고, 이어지는 likelihood를 maximize하기위해 보편적인 language model을 사용한다.</p><img src="/images/image-20210214174902969.png" alt="image-20210214174902969" style="zoom:150%;" /><p>k는 context의 size이고, conditional prob P는 NN을 사용하여 modeled</p><p>이들은 모두 SGD를 사용하여 training했다.</p><p><em>multi-layer Transformer decoder</em>를 사용했다.</p><p>이 model은 input context tokens에 multi-headed self-attention을 활용하였고, 이후에 position-wise feedforward layer를 적용하여 target token에대한 output distribution을 구한다.</p><p><img src="/images/image-20210214180732689.png" alt="image-20210214180732689"></p><p>U는 token의 context vector이고, n은 layer의 숫자, W<del>e</del>는 token embedding matrix, W<del>p</del>는 position embedding matrix이다.</p><br/><p><strong>3.2 Supervised fine-tuning</strong></p><p>model을 train한후, supervised target test에 맞추어서 parameter를 적용한다. labeled된 dataset C(각각은 input token의 sequence로 이루어짐 ((x^1^,…,x^m^) and label y )) </p><p>Input은 pre-trained된 model을 통과하여 최종 transformer block의 activation인 h<del>l</del>^m^을 얻어내고, 이후에 linear output layer에 W<del>y</del>와 함께 들어간다. </p><p><img src="/images/image-20210214181504691.png" alt="image-20210214181504691"></p><p>이는 이후의 objective를 maximize하게 한다.</p><p><img src="/images/image-20210214181541269.png" alt="image-20210214181541269"></p><p>보조적인 장치로 language modeling을 사용하여 fine-tuning을 하는것은 (1) generalization성능을 높힌다 (2) 수렴속도를 높힌다. 우리는 아래의 objective를 optimize한다</p><p><img src="/images/image-20210214181938559.png" alt="image-20210214181938559"></p><p>Fine-tuning중에 유일한 extra parameter은 W<del>y</del>와 구분token을 위한 embedding이다.</p><p><img src="/images/image-20210214182200590.png" alt="image-20210214182200590"></p><p><strong>3.3 Task-specific input transformations</strong></p><p>text classification가 같은 몇몇 분야에서, 위에서 묘사했던대로 우리의 model을 fine-tune할 수 있었다. 질의응답과, textual entailment와 같은 문제에는 input을 ordered sentence pairs, triplets of document, question, answer으로 해주었다. 우리의 pre-trained model이 연속적인 sequence에서 학습되었기 때문에, 이러한 문제들에는 약간의 맞춤 수정이 필요하다.  이전의 연구들은 transffered representation위에 특정 architecture를 삽입하는 형태로 학습해왔다. 이는 많은양의 cutomization이 필요하며 이러한 추가적인 특정 architecture에는 transfer learning을 사용하지 않았다. 대신 우리는 traveral-stple approach(input을 정렬된 sequence로 만들어)를 사용하여 우리의 pre-trained model이 학습할 수 있게 하였다. 이러한 input의 조정은 문제상황에 따라 architecture의 큰 수정을 하지 않아도 되게 한다. 모든 transformation은 randomly initialized된 start,ending token을 포함한다.</p><br/><ol><li>Textual entailment(문장의 포함관계) : 전제 p와 가설 h 중간에 delimiter token $를 삽입하여 합쳐주었다.</li><li>Similarity (문장의 유사도 평가) : 두개의 비교대상은 순서가 딱히 없다. 한마디로 동등한 level에서 비교해야 되기 때문에 모든 가능한 순서를 사용하고 transformer이후에 나오는2개의 h<del>l</del>^m^  을 합쳐준다.</li><li>Question Answering and Commonsense Reasoning (질의응답) : </li></ol><h2 id="3-Model-Atchitecture"><a href="#3-Model-Atchitecture" class="headerlink" title="3. Model Atchitecture"></a>3. Model Atchitecture</h2><br/><p>language model -&gt; label이 필요가 없다</p><p>주어진 단어들을 가지고 다음단어를 예측하는</p><ol><li><p>Generative model</p><p>Generative model</p><p>data가 많아 질수록 정확도가 높아진다</p></li><li><p>Discriminative model</p><p>타이타닉같은</p><p>데이터가 많지 않을때 패턴파악이 쉬워서 많이들 사용한다</p><p>한정된 data에 과적합 되기가 쉽다</p></li><li><p>sample된 data로는 왜곡된 판단을 할 수 있다</p></li></ol><p>GPT는 unlabeled된 data로</p><ol><li><p>Pretraining LM</p></li><li><p>finefuning</p><p>데이터만 task관련데이터로 학습 model은 그대로</p></li></ol><p>Naural Language Inference -&gt; entailment contradiction파악</p><p>질의응답</p><p>비슷한 문장 판별</p><p>주어진 문장을 그룹으로 분류하는</p><p>비지도 학습 label이 있는 data로 fine tunning한다.</p><p>기존 language model 학습 공식과 같다</p><p>transformer의 decoder로 구성</p><p>layer추가없이 pretrained LM</p><p>byte pair embedding을 사용하였다</p><p>신조어 오탈자에 약한 word embedding이 아닌</p><p>byte pair. —–&gt; hack,able, deep, learn, ing</p><p>이런식으로 embedding을 하였다.</p><p>data가 주어졌을떄</p>]]></content:encoded>
      
      
      <category domain="https://jo-member.github.io/categories/PaperReview/">PaperReview</category>
      
      
      <category domain="https://jo-member.github.io/tags/Transformer/">Transformer</category>
      
      <category domain="https://jo-member.github.io/tags/GPT/">GPT</category>
      
      <category domain="https://jo-member.github.io/tags/NLP/">NLP</category>
      
      
      <comments>https://jo-member.github.io/2021/02/14/2021-02-14-GPT/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Generative Models</title>
      <link>https://jo-member.github.io/2021/02/05/2021-02-05-Boostcamp15.1/</link>
      <guid>https://jo-member.github.io/2021/02/05/2021-02-05-Boostcamp15.1/</guid>
      <pubDate>Thu, 04 Feb 2021 15:00:00 GMT</pubDate>
      
      <description>&lt;br/&gt;



&lt;h1 id=&quot;Generative-Models&quot;&gt;&lt;a href=&quot;#Generative-Models&quot; class=&quot;headerlink&quot; title=&quot;Generative Models&quot;&gt;&lt;/a&gt;Generative Models&lt;/h1&gt;&lt;br/&gt;

&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;What I can not create, I do not understand&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&quot;https://deepgenerativemodels.github.io/&quot;&gt;https://deepgenerativemodels.github.io/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;What does it mean to learn a generative model&lt;/li&gt;
&lt;li&gt;generative model은 단순히 생성모델이 아니다 &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Suppose we have some images of dogs&lt;/p&gt;
&lt;p&gt;We want to learn a probability distribution p(x) such that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Generation : If we sample x&lt;del&gt;new&lt;/del&gt; ~ p(x), x&lt;del&gt;new&lt;/del&gt; should look like a dog&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;implicit models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Density estimation :p(x) should be high if x look like a dog (어떤이미지의 확률을 계산함)&lt;/p&gt;
&lt;p&gt;이건 마치 image classification&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;explicit models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Unsupervised representation learning &lt;/p&gt;
&lt;p&gt;특정 image가 어떤 특징을 가지고있는지를 학습&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
      
      
      
      <content:encoded><![CDATA[<br/><h1 id="Generative-Models"><a href="#Generative-Models" class="headerlink" title="Generative Models"></a>Generative Models</h1><br/><br/><ul><li>What I can not create, I do not understand</li></ul><p><a href="https://deepgenerativemodels.github.io/">https://deepgenerativemodels.github.io/</a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><br/><ul><li>What does it mean to learn a generative model</li><li>generative model은 단순히 생성모델이 아니다 </li></ul><p>Suppose we have some images of dogs</p><p>We want to learn a probability distribution p(x) such that</p><ul><li><p>Generation : If we sample x<del>new</del> ~ p(x), x<del>new</del> should look like a dog</p><ul><li>implicit models</li></ul></li><li><p>Density estimation :p(x) should be high if x look like a dog (어떤이미지의 확률을 계산함)</p><p>이건 마치 image classification</p><ul><li>explicit models</li></ul></li><li><p>Unsupervised representation learning </p><p>특정 image가 어떤 특징을 가지고있는지를 학습</p></li></ul><span id="more"></span><h2 id="How-can-we-represent-p-x"><a href="#How-can-we-represent-p-x" class="headerlink" title="How can we represent p(x)??????"></a><strong>How can we represent p(x)??????</strong></h2><br/><ul><li><p>Bernoulli distribution</p><ul><li><p>D = {Heads, Tails}</p></li><li><p>Specify P(X = Head) = p, P(X = Tails) = (1-p)</p></li></ul></li><li><p>Categorical distribution</p></li></ul><p>ex) Modeling and RGB joint distribution</p><ul><li>(r,g,b) ~ p(R,G,B)</li><li>number of case = 256x256x256</li><li>parameters = 255x255x255 개가 필요</li></ul><p>하나의 RGB pixel만해도 parameter를 표현하려면 어마어마한 숫자의 parameter가 필요하다</p><h3 id="Structure-Through-Independence"><a href="#Structure-Through-Independence" class="headerlink" title="Structure Through Independence"></a>Structure Through Independence</h3><p>What if X1,….,Xn are independent and binary pixels</p><p>p(x1,…,xn) = p(x1)p(x2)…p(xn)</p><p>possible state : 2^n^</p><p>parameter : n개만 필요</p><p>만약 각각의 pixel이 독립적이라고 가정한다면 이렇게 parameter수가 줄어든다</p><p>근데 이건 너무 말이 안된다</p><p>따라서 Independence와 fully dependent사이의 절충안???</p><h3 id="Conditional-Independence"><a href="#Conditional-Independence" class="headerlink" title="Conditional Independence"></a>Conditional Independence</h3><p>Three Important Rule</p><p><img src="/images/image-20210205102341270.png" alt="image-20210205102341270"></p><p>n개의 joint distrubution을 n개의 conditional distribution으로 바꾸고</p><p>z가 주어졌을때 x,y는 independent하다 -&gt;이게 가정 완전 xy가 independent한게 아니라 z가 주어졌을때 </p><p>y는 상관이없다 이런느낌</p><h3 id="Conditional-Independence-1"><a href="#Conditional-Independence-1" class="headerlink" title="Conditional Independence"></a>Conditional Independence</h3><p>Using the chain rule</p><p><img src="/images/image-20210205103330254.png" alt="image-20210205103330254"></p><p>이 수식 도출에서 어떠한 수학적인 가정이 없이 chain rule만으로 구한 수식이다 따라서 fully independent와 parameter 개수는 같다</p><ul><li><p>p(x1) :1개</p></li><li><p>p(x2|x1) : 2개 (one per for p(x2|x1 = 0) and p(x2|x1 = 1))</p></li><li><p>p(x3|x1,x2) : 4개</p></li><li><p>Hence 1+2+2^2^+…+2^n-1^ = 2^n^-1</p></li></ul><p>i+1번쨰 pixel은 i번째 pixel에만 dependent하다 가정 : markov assumption</p><p><img src="/images/image-20210205103602281.png" alt="image-20210205103602281"></p><p><img src="/images/image-20210205111200710.png" alt="image-20210205111200710"></p><p>그 중간에 있는 걸 conditional independence를 잘 활용해서 중간의 parameter값을 얻어냈다</p><h2 id="Auto-regressive-Model"><a href="#Auto-regressive-Model" class="headerlink" title="Auto-regressive Model"></a>Auto-regressive Model</h2><ul><li><p>suppose we have 28x28 binary pixels</p></li><li><p>goal : p(x) = p(x1,x2….,x784)</p></li><li><p>how can we parametrize p(x)</p></li><li><p>use chain rule to get joint distribution</p></li><li><p>p(x<del>1:784</del>) = p(x<del>1</del>)p(x<del>2</del>|x<del>1</del>)p(x<del>2</del>|x<del>1:2</del>)……</p></li><li><p>이게 바로 auto-regressive model (i번째 pixel이 1~i-1까지 모든 history에 dependent한)</p></li><li><p>가장 중요한게 순서를 매기는 과정</p><p><strong>이미지에 순서???? —-&gt; 순서에 따라 성능이나 방법론이 달라질수 있다</strong></p></li></ul><h2 id="NADE-Neural-Autoregressive-Density-Estimator"><a href="#NADE-Neural-Autoregressive-Density-Estimator" class="headerlink" title="NADE : Neural Autoregressive Density Estimator"></a>NADE : Neural Autoregressive Density Estimator</h2><ul><li>p(x<del>i</del>|x<del>1:i-1</del>) = </li></ul><p>i번째 pixel을 1~i-1에 dependent하게 만든다  —–&gt; </p><p><strong>dependent 하다 ?</strong> 1-i-1번째 pixel값을 입력으로 받고 network를 통과시켜서 나온 output에 sigmoid를 통과해서 확률이 나오도록하는것</p><p><img src="/images/image-20210205111934634.png" alt="image-20210205111934634">    </p><p>neural network의 weight의 차원값은 지속해서 늘어남이전입력들이 계속해서 늘어나기 때문에</p><ul><li>NADE is explicit model</li><li>Suppose we have 784개의 binary pixel</li></ul><p>알고있는 값들을 집어넣은뒤 계산하게 되면 확률값이 나옴</p><p><img src="/images/image-20210205112108078.png" alt="image-20210205112108078"></p><p>Density estimate : 확률적으로 무언가의 확률을 explit하게 계산한다</p><p>Continous한 r.v를 modeling할때는 Gaussian이 사용이 된다</p><br/><h2 id="Pixel-RNN"><a href="#Pixel-RNN" class="headerlink" title="Pixel RNN"></a>Pixel RNN</h2><br/><ul><li>Use RNNs to define an auto regressive model</li><li>이전에 봤던 NADE는 dense layer을 사용함 하지만 Pixel RNN은 RNN을 통해 generate한다</li><li><img src="/images/image-20210205112323136.png" alt="image-20210205112323136"><ul><li><p>ordering의 순서에 따라</p><p>Row LSTM</p><p>Diagonal BiLTM</p><p><img src="/images/image-20210214141351023.png" alt="image-20210214141351023"></p><br/></li></ul></li></ul><br/><h1 id="Latent-Variable-Models"><a href="#Latent-Variable-Models" class="headerlink" title="Latent Variable Models"></a>Latent Variable Models</h1><h2 id="Variational-Auto-encoder"><a href="#Variational-Auto-encoder" class="headerlink" title="Variational Auto-encoder"></a>Variational Auto-encoder</h2><ul><li><p>Is an autoencoder generative model??</p><p>autoencoder은 input을 재정의하는 과정이지 generative model은 아니다</p><p>과연 무엇때문에 Variational Auto-Encoder은 generation 모델인가?</p></li><li><p>Variational inference (VI)</p><ul><li>The goal of VI is to optimize the variational distribution that best matches the <strong>posterior distribution</strong></li></ul></li><li><p>posterior distribution : observation이 주어졌을때 내가 관심있어하는 r.v의 확률분포</p><ul><li>posterior distribution을 계산하는건 매우 힘들기 때문에 Variational distribution을 근사한다</li></ul></li></ul><p>KL divergence를 사용해서 Variational distribution과 Posterior distribution의 차이를 줄여보겠다</p><p><img src="/images/image-20210214142712060.png" alt="image-20210214142712060"></p><p><strong>How?</strong></p><p><img src="/images/image-20210214142913172.png" alt="image-20210214142913172"></p><p>원해는 KL divergence를 줄이는게 목적이지만 이게 불가능하기 때문에 ELBO라고 불리는 term을 최대화 한다</p><br/><ul><li><p><strong>ELBO can further be decomposed into</strong></p><p><img src="/images/image-20210214144517622.png" alt="image-20210214144517622"></p></li></ul><p>Reconstruction Term</p><p>x라는 입력을 latent space로 보냈다가 Decoder로 돌아오는 Reconstruction loss를 줄이는 term</p><p>Latent space에 올려놓은 점들이 이루는 분포가 Latent space의 prior distribution와 비슷하다? implicit한 model</p><br/><p>Decoder이후의 output domain의 값들이 generation result이다</p><p>Auto encoder은 이게 아니라 generation model이 아니다</p><br/><p>Key limitation</p><ul><li>Interactable model (hard to evaluate likelihood)</li><li>reconstruction term은 상관없는데 KL divergence를 사용한 prior distribution에는 무조건 미분이 가능한 distribution (like Gaussian)을 사용해야 한다. 따라서 diverse한 latent prior distributions에는 사용을 하기에 힘들다</li><li>In most cases, we use an isotropic Gaussian</li><li><img src="/images/image-20210214145657271.png" alt="image-20210214145657271"></li></ul><br/><h2 id="Adversarial-Auto-encoder"><a href="#Adversarial-Auto-encoder" class="headerlink" title="Adversarial Auto-encoder"></a>Adversarial Auto-encoder</h2><br/><ul><li><p>It allows us to use any arbitrary latent distributions that we can sample</p><p><img src="/images/image-20210214145814068.png" alt="image-20210214145814068"></p><p>Prior fitting term을 gan을 사용하여 분포를 맞추어줌</p><p>sampling이 가능한 어떠한 분포도 맞출수있다는 장점이 있다</p></li></ul><h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><p><img src="/images/image-20210214151437158.png" alt="image-20210214151437158"></p><p>discriminator가 점차 발전해 나가면서 generator도 따라서 성능이 올라가는 상생의?</p><h3 id="GAN-vs-VAE"><a href="#GAN-vs-VAE" class="headerlink" title="GAN vs VAE"></a>GAN vs VAE</h3><p><img src="/images/image-20210214151545975.png" alt="image-20210214151545975"></p><h3 id="GAN의-Objective"><a href="#GAN의-Objective" class="headerlink" title="GAN의 Objective"></a>GAN의 Objective</h3><ul><li><p>For discriminator<img src="/images/image-20210214151646421.png" alt="image-20210214151646421"></p><p>where the optimal discriminator is <img src="/images/image-20210214151813712.png" alt="image-20210214151813712"></p></li><li><p>For generator</p><p><img src="/images/image-20210214151923764.png" alt="image-20210214151923764"></p><p><strong>GAN의 objective는 나의 true generative distribution과 내가 학습하고자하는 generator사이의 Jenson-Shannon Divergence를 최소화 하는것이다</strong></p></li></ul><h3 id="DCGAN"><a href="#DCGAN" class="headerlink" title="DCGAN"></a>DCGAN</h3><p><img src="/images/image-20210214152052429.png" alt="image-20210214152052429"></p><h3 id="Info-GAN"><a href="#Info-GAN" class="headerlink" title="Info-GAN"></a>Info-GAN</h3><p><img src="/images/image-20210214152117180.png" alt="image-20210214152117180"></p><p>학습시에 class라는 random한 one-hot vector를 매번 집어 넣어준다</p><p>generation시에 gan이 특정모드에 집중할 수 있게끔해준다</p><h3 id="Text2Image"><a href="#Text2Image" class="headerlink" title="Text2Image"></a>Text2Image</h3><p><img src="/images/image-20210214152233370.png" alt="image-20210214152233370"></p><p>텍스트로 이미지를 generate하는 연구</p><p>model이 매우 복잡하다…….</p><br/><p>CycleGAN</p><p><img src="/images/image-20210214152414654.png" alt="image-20210214152414654"></p><p>이 cycle consistency loss가 매우 중요하다</p>]]></content:encoded>
      
      
      <category domain="https://jo-member.github.io/categories/Boostcamp/">Boostcamp</category>
      
      
      <category domain="https://jo-member.github.io/tags/Basic/">Basic</category>
      
      <category domain="https://jo-member.github.io/tags/GAN/">GAN</category>
      
      
      <comments>https://jo-member.github.io/2021/02/05/2021-02-05-Boostcamp15.1/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Attention Is All You Need</title>
      <link>https://jo-member.github.io/2021/02/05/2021-02-05-Attention/</link>
      <guid>https://jo-member.github.io/2021/02/05/2021-02-05-Attention/</guid>
      <pubDate>Thu, 04 Feb 2021 15:00:00 GMT</pubDate>
      
      <description>&lt;br/&gt;

&lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;p&gt;Seuence transduction model들은 현재 복잡한 recurrent한 구조 (RNN) 이나 encoder decoder를 포함한 CNN이 주를 이룬다. 가장 좋은성능을 내는 model또한 attention mechanism을 이용하여 encoder와 decoder를 연결하는 형태이다.&lt;/p&gt;
&lt;p&gt;이 논문에서는 새로운 방법인 Transformer를 제안&lt;/p&gt;
&lt;p&gt;이는 오로지 attention mechanism만을 사용!&lt;/p&gt;
&lt;p&gt;이는 RNN이나 CNN보다 더 &lt;strong&gt;병렬화가 가능하고 train하는데 적은 시간이 걸린다!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;WMT 2014 English to-German data를 사용하여 BLEU라는 score에서 28.4점을 얻었다.(여러 논문을 읽다보면 자주 등장하는 이 BLUE score은 정리해 놓은게 있는데 추후에 posting )&lt;/p&gt;
&lt;p&gt;이는 앙상블을 포함한 이전의 가장 좋은 성능보다 2BLUE가 높다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<br/><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Seuence transduction model들은 현재 복잡한 recurrent한 구조 (RNN) 이나 encoder decoder를 포함한 CNN이 주를 이룬다. 가장 좋은성능을 내는 model또한 attention mechanism을 이용하여 encoder와 decoder를 연결하는 형태이다.</p><p>이 논문에서는 새로운 방법인 Transformer를 제안</p><p>이는 오로지 attention mechanism만을 사용!</p><p>이는 RNN이나 CNN보다 더 <strong>병렬화가 가능하고 train하는데 적은 시간이 걸린다!</strong></p><p>WMT 2014 English to-German data를 사용하여 BLEU라는 score에서 28.4점을 얻었다.(여러 논문을 읽다보면 자주 등장하는 이 BLUE score은 정리해 놓은게 있는데 추후에 posting )</p><p>이는 앙상블을 포함한 이전의 가장 좋은 성능보다 2BLUE가 높다.</p><span id="more"></span><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><br/><p>RNN모델 (LSTM이나 GRU)는 machine translation과 같은 sequence modeling의 State of the art한(최신의 가장 좋은 성능의) 접근방식으로 알려져있다. RNN은 input과 output의 위치를 계산한 결과를 담고있다. 계산하는 시간이나 순서에 의해 정렬된 위치들은 이전의 hidden state h<del>t-1</del>로 표현된 연속적인 hidden state h<del>t</del>를 생성한다. 이것은 본질적으로 training examples의 병렬화를 배제하며, 이로인해 memory의 한계로 인한 batch size의 한계 때문에 긴 sequence length에 굉장히 critical한 요소로 작용한다. 최근의 연구들은 factorization과 conditional computation(이것에 대한 논문: Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks, Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer )을 이용한 계산과정의 효율화로 큰 발전을 이루어냈다. 하지만 순차적인 계산에 의한 제약은 아직 남아있다</p><p>Attention mechanism은 input과 out사이의 길이에 상관없이 dependencies를  modeling할수있다는 부분에서 sequence modeling과 transduction modeling의 필수적인 부분이 되었다. 하지만 몇몇 경우에서는 아직 attention mechanism과 RNN을 합쳐서 사용하고 있다.</p><p>이 연구에서는 Transformer라는 attention mechanism에만 의존하여 input과 output의 dependency를 이끌어내는 architecture을 제안한다. Transformer은 병렬화를 가능하게 하고, 성능을 더욱 향상시킬수 있다</p><br/><p>요약 : 우리의 transformer가 training example의 병렬화로 인한 속도 향상과 좋은 점수를 낸다.</p><br/><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>encoder , decoder에대한 background</p><p>중간의 latent space는 input이나 output보다 훨씬 최소화된 vector이다. Sequential한 계산을 줄이는 목표는 기본적인 building block에서 CNN을 사용하여 병렬적으로 input과 output의 hidden representation을 계산하는 ByteNet이나 ConvS2S의 기반을 이루고있다. 위와 같은 model에서는 2개의 input이나 output의 길이가 증가할수록 계산량이 늘어난다.(ByteNet은 log적으로, ConvS2S는 linear하게). 이러한 결과는 거리에 따른 dependencies들을 학습하기에 더욱 어렵게 만든다. </p><br/><p>※ (input과 output사이의 길이가 길어지면 계산량이 증가해 서로의 연관관계를 학습하기가 어렵다는 뜻</p><p>cnn은 한번에 kernel size를 진짜 커봤자 최대 7x7을 쓰기 때문에 만약 input이 엄청 길다면 CNN연산시 계산량이 증가하게 되고 위치에 대한 정보의 일부만이 담기게됨. 예를 들어 간단하게 she is pretty and good at playing piano with her own piano와 같은 문장에서 뒤에 her과 처음 she는 긴 거리를 가지게 되어서 이 정보를 담기에 CNN은 부적절?).</p><br/><p>Transformer에서는 linear나 logmatric하게 계산량이 증가하지 않고 constant한 number로 증가한다.Attention-weigheted position의 <strong>평균</strong>을 사용하여  Effective한 **해상도?**가 감소함에도Multi-Head Attention과 상호작용 함으로서 계산량을 줄였다.</p><p>Self-attention은 서로 다른 position에 있는 sequence를 표현하기 위해 서로를 relating한다. </p><p>Self-attention은 reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representation과 같은 분야에서 성공적으로 사용되어져 왔다.</p><p>End-to-end memory network은 순서에 따라 정렬된 recurrence가 아닌 recurrent attention mechanism을 기반으로 하고있고, 좋은 성능을 보여주고 있다.</p><p>Transformer은 처음으로 RNN을 사용하지 않고 오로지 self-attention만이 쓰인 첫번째 변역 model이다.</p><br/><br/><p>요약 :  Sequence한 문제에서의 모델</p><p>​    RNN의 단점 : 병렬화의 어려움으로 인한 계산의 복잡도 증가, train 시간의 증가,</p><p>​    (그리고 강의에서 만약     sequential한 데이터중 중간에 어느 하나가빠진다면 해결하기가 어렵다고 했다)</p><p>​    CNN의 단점 : 병렬적인 계산은 이루어 지지만, input이나 output의 길이가 증가할수록 계산도 많고 단어간의 관    계파악이 빡셈</p><p>​    따라서 Attention만 쓴 Transformer 짱</p><p><br/><br/></p><h2 id="3-Model-Atchitecture"><a href="#3-Model-Atchitecture" class="headerlink" title="3. Model Atchitecture"></a>3. Model Atchitecture</h2><p>가장 경쟁력이 좋은 neural sequence transduction model은 encoder-decoder 구조를 가지고 있다. Encoder은 입력 sequence를 x = (x<del>1</del>,….x<del>n</del>)으로 표현하였고 이를 z = (z<del>1</del>,….z<del>n</del>)으로 map한다. 주어진 z로 decoder가 output sequence인 (y<del>1</del>,…,y<del>m</del>)을 생성해 낸다.(보통 중간의 latent 층은 input과 output에 비해 작은 dimension을 가진다고 조교님께서 설명) 각 step마다 model은 <strong>auto-regressive</strong>하며, 문장을 생성할때 이전에 생성된 symbol을 additional input으로 가정한다.</p><p><br/><br/></p><br/><p>※ Auto regressive 복습</p><p>(고정된 길이인 $\tau$만큼의 시퀀스만 활용하는 경우 Autoregressive Model(자기회귀모델)이라고 부른다</p><p>직전과거의 정보랑 직전정보가 아닌 정보들을 H<del>t</del>로 묶어서 활용)</p><p><img src="/images/image-20210204114520892-1612513436394.png" alt="image-20210204114520892"></p><p><br/><br/></p><p><br/><br/></p><p>Transformer은 stacked된 self-attention을 사용하고 있고, encoder와 decoder부분에 모두 fully connected layer를 삽입하였다.</p><p><img src="/images/image-20210205125403138.png" alt="image-20210205125403138"></p><br/><h3 id="3-1-Encoder-and-Decoder-Stacks"><a href="#3-1-Encoder-and-Decoder-Stacks" class="headerlink" title="3.1 Encoder and Decoder Stacks"></a>3.1 Encoder and Decoder Stacks</h3><p><strong>Encoder</strong> : encoder은 N=6 (6개)인 각각의 identical한 layer들이 층층이 쌓여있다. 각각의 layer들은 2개의 sub-layer로 구성되어있다. 첫번째는 Multi-Head Attention이고, 두번째는 간단한 fully-connected된 feed-forward network이다.</p><p>우리는 각각의 sublayer에 <strong>residual connection</strong>을 들어 주었다.</p><br/><p>※여기서 과연 residual connection을 넣은 이유가 뭘까? overfitting 방지 like ResNet??</p><br/><p>각각의 sub-layer의 output은 <em>LayerNorm(x + Sublayer(x))</em>. 모든 sublayer model과 embedding layer의 output의 차원은 d<del>model</del> = 512 이다.</p><br/><p><strong>Decoder</strong> : Decoder또한 N=6인 각각의 identical한 layer들이 층층이 쌓여있다. Encoder의 2개의 각 sub-layer에 Decoder은 <strong>encoder stack의 output에 대한 multi-Head attention</strong>을 수행하는 층이 추가가 되었다.</p><p>Encoder와 같이 sublayer에 residual connection을 만들어 주었다. 하위 position이 attend하는것을 방지하기 위해 self-attention-layer을 약간 수정하였다**(이게 masking). 이 masking은 i위치의 예측이 i보다 과거의 것으로만 구해지게 하기 위함이다.**</p><br/><p>※ Masking은 NLP 문제에서 굉장히 많이 쓰인다고 한다. 알아두자</p><br/><br/><h3 id="3-2-Attention"><a href="#3-2-Attention" class="headerlink" title="3.2 Attention"></a>3.2 Attention</h3><p>Attention function은 query와 set of key-value pair들을 output에 mapping하는 함수이다. (query,key,value,output은 모두 vector). Output은 value들의 weighted sum으로 계산하며, 이 weight는 query와 다른모든 key값들의 <strong>compatibility function</strong>으로 정해진다.</p><p>※여기서 compatibility function이란?</p><p>뒤에서 sum의 형태와 dot product로 나누어 진다. 이들의 차이점은 뒤에 기술</p><p><img src="/images/image-20210205131926013.png" alt="image-20210205131926013"></p><br/><h4 id="3-2-1-Scaled-Dot-Product-Attention"><a href="#3-2-1-Scaled-Dot-Product-Attention" class="headerlink" title="3.2.1 Scaled Dot-Product Attention"></a>3.2.1 Scaled Dot-Product Attention</h4><p>우리는 이러한 attention을 “Scaled Dot-Product Attention”이라고 부른다. Input은 d<del>k</del>의 차원을 가지는 keys와 queries(둘은 연산(내적)을 위해 같은 차원을 가진다)와  d<del>v</del>의 차원을 가지는 values로 이루어져 있다. 우리는 하나의 query 를 다은 모든 keys들과 내적하고(이 결과 값이 바로 강의에서 score) sqrt(d<del>k</del>)로 나누어 준다. 이후 softmax function을 적용하여 value의 weight를 얻어낸다.</p><p>※ i번째 단어에 대한 score vector 계산시 i의 쿼리 vector와 다른모든 key vectors 사이의 내적 (Matmul)</p><p>위의 과정들을 queries들을 Q matrix, keys and values를 각각 K and V라고 한다면 아래의 식으로 표현가능</p><p><img src="/images/image-20210205132622631.png" alt="image-20210205132622631"></p><p>가장 많이쓰는 attention function의 함수는 additive attention과 위와 같은 dot-product attention이다. 우리는 dot-product attention을 썼다. </p><p>Additive attention은 하나의 hidden layer와 feed forward network를 사용하여compatibility function을 계산한다. 이두가지는 복잡도 측면에서 비슷하지만, dot-product attention이 더빠르고 공간 절약적이다.(이유는 행렬의 계산으로 표현가능)</p><p>작은 값을 가지는 d<del>k</del>에서의  두 mechanism은 유사하겠지만, 큰 d<del>k</del>로 나누어 주지 않으면 additive attention이 dot-product의 성능을 넘는다. 큰  d<del>k</del>는  dot -product는 큰값을 가지게 되고, 이는 softmax function이 매우 작은 gradient를 가지게 한다. 이러한 영향을 줄이기 위해 sqrt(d<del>k</del>)로 나누어 주었다. (scale)</p><br/><h4 id="3-2-2-Multi-Head-Attention"><a href="#3-2-2-Multi-Head-Attention" class="headerlink" title="3.2.2 Multi-Head Attention"></a>3.2.2 Multi-Head Attention</h4><p>d<del>model</del>(max sequence)의 차원을 가지는 keys,values,queries으로 이루어진 single attention을 수행하는것이 아니라, 우리는 queries, keys, values들에 각각 h번 d<del>k</del>,d<del>k</del>,d<del>v</del>를 곱하여 project한값이 더욱 좋은것을 알아내었다.</p><p>이러한 <strong>projection</strong>을 거치면 attention function을 병렬적으로 수행 할 수있으며, d<del>v</del>의 차원을 가지는 output을 얻어낼 수 있다. 이 output은 다시 하나로 concatnated되어 projected된다.</p><p><img src="/images/image-20210205134008271.png" alt="image-20210205134008271"></p><p>Multi-Head Attention은 서로다른 위치에서의 서로다른 subspace의 표현을 jointly attend 하게 한다.</p><p><img src="/images/image-20210205134243929.png" alt="image-20210205134243929"></p><p>우리는 h = 8개의 parallel attention layer을 사용하였고,d<del>k</del>,d<del>v</del>,d<del>model</del>/h = 64</p><p><strong>각각의 head에서 차원을 줄임으로서 전체적인 계싼비용이 single head attention과 비슷하게 만들었다???????</strong></p><p>※ 한마디로 이제 d<del>model</del>의 차원을 h만큼 parrel layer에 나누어서 넣었으니 결국 single head attention과 비슷하다는 이야기인가??</p><br/><h4 id="3-2-3-Applications-of-Attention-in-our-Model"><a href="#3-2-3-Applications-of-Attention-in-our-Model" class="headerlink" title="3.2.3 Applications of Attention in our Model"></a>3.2.3 Applications of Attention in our Model</h4><p>Transformer은 multi-head attention을 3가지 다른 방식으로 사용하고 있다</p><ol><li><p>“Encoder - Decoder attention” layer에서 queries는 이전의 decoder layer에서 오고, encoder의 output에서 오는 key와 value들을 저장한다. 이것은 input sequence의 모든 position들을 모든 position의 decoder가 attend 하게 해준다.</p><p>한마디로 decoder을 쿼리만 들고있어도 된다.</p><br/></li><li><p>Self attention layer를 포함하는 encoder. Self attention layer에서는 이전 encoder의 layer의 결과에서부터 나온 위치와 같은 위치에서  모든 key, values, and queries가 나온다. encoder속의 각각의 위치들은  이전 encoder의 이전 layer의 모든 위치에 집중한다. (모든 현재 layer의 위치가 이전 layer의 모든 position 정보들을 가진다? 이런느낌?)</p><br/></li><li><p>비슷하게 decoder의 self attention layer또한 모든  decoder안의 모든(자기자신까지) position에 집중한다.<strong>Auto regressive 특성을 보존시키기 위해 왼쪽의 정보들이 decoder로 flow in 하는걸 막아주어야 한다. 우리는 이러한 걸 scaled dot product attention안의 softmax의 output 값에masking out함으로서 해결한다</strong></p><p>이걸 다시한번 생각해 보아야 겠다</p><p>Auto regressive한 특성이란 이전의 정보들 만으로 현재값을 도출해내는 특성</p><p>그니까 결국은 위에(1)식에서 softmax의 결과값에 미래의 정보들은 모두 masking 해준다는것이다.</p><p>그니까 결국엔 <img src="/images/image-20210205132622631-1612515516800.png" alt="image-20210205132622631"> </p><p>이식에서 미래의 정보들까지 K와 Q의 내적결과가 다담고 있으니까 미래의 정보는 마스킹 해준다.</p><br/></li></ol><h3 id="3-3-Position-wise-Feed-Forward-Networks"><a href="#3-3-Position-wise-Feed-Forward-Networks" class="headerlink" title="3.3 Position-wise Feed Forward Networks"></a>3.3 Position-wise Feed Forward Networks</h3><p>encoder와 decoder안의 각 layer에는 fully connected feed forward network를 가져야 한다.  이 <strong>fully connected feed forward network는 각각의 위치에 독립적으로 따로 적용된다</strong></p><p><img src="/images/image-20210205155145255.png" alt="image-20210205155145255"></p><p>위식을 보면 2개의 linear transformation과 ReLU activation을 그사이에 사용하였다.</p><p><strong>Linear transformation을 각각의 position에 같은 걸 적용해야 한다. 그리고 layer과 layer사이에는 다른 parameter를 적용해야 한다. 또다른 방법은 1의 kernel size를 가지는 2개의 convoltion을 사용하는 것이다.</strong></p><p>※ 그니까 하나의 layer에는 같은 weight를 적용하고 다른 layer사이에는 다른 weight를 적용한다는 뜻???</p><br/><p>d<del>model</del> = 512</p><p>inner-layer’s dimension d<del>ff</del> = 2048</p><br/><h3 id="3-4-Embedding-and-Softmax"><a href="#3-4-Embedding-and-Softmax" class="headerlink" title="3.4 Embedding and Softmax"></a>3.4 Embedding and Softmax</h3><p>여타 다른 번역 모델과 같이 여기서도 학습된 embadding을 사용했다.</p><p>Decode되어 나온결과도 학습된 linear transformation과 softmax함수를 사용하여 다음 token의 확률을 계산하였다.</p><p>embedding layer들에는 같은 weight와 presoftmax linear transformation을 사용, weight의 결과에 sqrt(d<del>model</del>)을 사용했다.</p><br/><h3 id="3-5-Positional-Encoding"><a href="#3-5-Positional-Encoding" class="headerlink" title="3.5 Positional Encoding"></a>3.5 Positional Encoding</h3><p>우리의 model은 recurrence도 없고 convolution도 없어서 각각의 model이 sequence의 순서를 사용하게 하여면 position 정보를 삽입해 주어야 한다.</p><p>따라서 </p><p><img src="/images/image-20210205094724396-1612516229839.png" alt="image-20210205094724396"></p><p>이와 같이 각각 embedding된 vector에 positional encoding된 vector를 더해준다</p><p>이 연구에서는 cos과 sin 함수를 사용했다</p><p><img src="/images/image-20210205181144831.png" alt="image-20210205181144831"></p><p>pos 는 position i는 차원</p><p>각각의 위치가 sinusodial하게 encoding 되도혹 하였다. 주기가 조올라 길어서 다른위친데 주기성 때문에 같은 값을 가지는 경우는 드물다</p><br/><h2 id="4-Why-Self-Attention"><a href="#4-Why-Self-Attention" class="headerlink" title="4. Why Self-Attention"></a>4. Why Self-Attention</h2><p>이 부분에서는 self attention layer를 RNN과 CNN에 더욱 자세히 비교한다.</p><p>앞에서 설명한거에 대한 보충설명</p><ol><li><p>한 layer에서 계산 복잡도에서의 이득</p></li><li><p>병렬화 될 수 있는계산의 총량</p></li><li><p>길이가 기이이일어졌을때 얼마나 network에 영향을 미치는지</p><p>길이가 졸라리 길어졌을때 dependencies는 매우 중요하다. 이에 가장 중요하게 미치는 영향중 하나가 signal하나가 network를 순회하는 길이이다?? 이게 짧아질수록 긴 길이에 대한 상호적인 관계가 더 잘 학습된다. 따라서 각 model마다 model에서 input과 output 사이의 거리? 뭐하이튼 그 얼마나 model이 compact한가</p></li></ol><p><img src="/images/image-20210205181818372.png" alt="image-20210205181818372"></p><p>위 table을 보면 computational한 성능을 높이기 위해 self attention 모델은 해당하는 output 위치의 오직 size r 만큼의 input sequence 주위를 고려하도록 제한되어있다.</p><p>이거에 대한 연구는 추후에 발표하겠다고 적혀있다. 그럼 이미 나와있겠지?</p><p>하이튼 위에꺼 비교해보면 모든 측면에서 self attention이 와따</p><br/><h2 id="5-Training"><a href="#5-Training" class="headerlink" title="5. Training"></a>5. Training</h2><h3 id="1-Training-data-and-batching"><a href="#1-Training-data-and-batching" class="headerlink" title="1. Training data and batching"></a>1. Training data and batching</h3><p>WMT 2014를 사용하여 train함</p><p>그리고 문장은 target vocab와 37000개를 공유하는 byte-pair encoding이라는 방식을 사용했음</p><p>각 traning batch는 25000개의 source token과 25000개의 target token을 포함하는 문장의 set으로 정해주었다.</p><h3 id="2-Optimizer"><a href="#2-Optimizer" class="headerlink" title="2. Optimizer"></a>2. Optimizer</h3><p>Adam을 썼고, lr을 단계적으로 변화시켰다.아래와 같은 수식으로</p><p><img src="/images/image-20210205182950061.png" alt="image-20210205182950061"></p><br/><h3 id="3-Regularization"><a href="#3-Regularization" class="headerlink" title="3. Regularization"></a>3. Regularization</h3><ol><li><p>Residual Dropout</p><p>더해지고 normalized 되기전에 각각의 sublayer의 output에 dropout을 적용하였다</p><p>그리고 또 embedding된 vector와 position의 합이후에도 적용하였다</p><p>P<del>drop</del>은 0.1 </p></li><li><p>Label Smoothing???</p><p>이런걸 적용했다고 하느네 이게 약간 예측불가능한걸 더해줘서 model이 더 새로운것을 배우게끔하는 거라하는데 걍 간단하게 나와있다</p><br/></li></ol><h2 id="6-Results"><a href="#6-Results" class="headerlink" title="6. Results"></a>6. Results</h2><p>BLEU score 잘나왔다 어쩌구 저쩌구 하다가</p><p>base model에서는 5개의 checkpoint를 만들어서 그것의 평균을 낸 하나의 model을 썼고, 각 check point는 10분마다 한번씩 interval을 주었다.</p><p>이게 내가 조교님한테 질문했던 부분과 좀 연관성이 있다. 이렇게 중간중간에 model을 기록하고 평균을 내는 방식도 있구나</p><p>그리고 이 beam search를 사용하였다고 한다</p><p>이건 이전의 논문들을 읽을때도 자주 사용했던 기법이다</p><p>간단하게 설명하면 가장 확률이 높은 K개을 선택하며 진행하는 것이다</p><p>greedy방법보다 효율적이고 score가 잘나온다고 들었다</p><h3 id="6-2-Model-Variations"><a href="#6-2-Model-Variations" class="headerlink" title="6.2 Model Variations"></a>6.2 Model Variations</h3><p>아래 table의 (A)를 보면 attention의 head의 개수와 key,value의 dimension을 변화시켜주었다. 너무 많은 head를 사용해도 안되고 하나만 사용해도 안됨</p><p>이건 너무 당연한거다 뭐든지 적당한게 좋다</p><p><img src="/images/image-20210205184119740.png" alt="image-20210205184119740"></p><h3 id="6-3-English-Constituency-Parsing"><a href="#6-3-English-Constituency-Parsing" class="headerlink" title="6.3 English Constituency Parsing"></a>6.3 English Constituency Parsing</h3><p>이 Transformer를 활용한 model은 통역에서 나아가서 영어 구문을 분석해주는 방법으로 발전시켜나가야 한다. </p><p>이건 별로 중요하지 않은것 같다</p><p>해보니까 RNN보다 좋은 성능을 나타내었다 끝</p><h2 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7. Conclusion"></a>7. Conclusion</h2><p>결론 </p><p>기존과 다르게 attention에만 기반을 둔 multi-headed self attention을 사용한 이 transformer은 다른 RNN이나 CNN보다 성능이 빠르며 이 Transfomer를 더욱큰 input과 output을 가지는 image나 비디오 오디오 등에적용시키는 것을 기대하고 있다.</p><p>Transformer 짱짱맨</p><p>그리고 조교님께서 관심 있으신 Neuroscience와 Attention사이의 관련</p><p><img src="/images/image-20210205185431936.png" alt="image-20210205185431936"></p><p>보면 우리 인간의 황반에서도 이 attention의 개념을 적용해서 사물을 인지하고 있으니, 잘되는게 어찌보면 당연하다</p><p>출처 : <a href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a></p><p>그리고 naver boostcamp</p>]]></content:encoded>
      
      
      <category domain="https://jo-member.github.io/categories/PaperReview/">PaperReview</category>
      
      
      <category domain="https://jo-member.github.io/tags/Transformer/">Transformer</category>
      
      <category domain="https://jo-member.github.io/tags/NLP/">NLP</category>
      
      
      <comments>https://jo-member.github.io/2021/02/05/2021-02-05-Attention/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>RNN1</title>
      <link>https://jo-member.github.io/2021/02/04/2021-02-04-Boostcamp14.1/</link>
      <guid>https://jo-member.github.io/2021/02/04/2021-02-04-Boostcamp14.1/</guid>
      <pubDate>Wed, 03 Feb 2021 15:00:00 GMT</pubDate>
      
      <description>&lt;p&gt;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h1 id=&quot;RNN&quot;&gt;&lt;a href=&quot;#RNN&quot; class=&quot;headerlink&quot; title=&quot;RNN&quot;&gt;&lt;/a&gt;RNN&lt;/h1&gt;&lt;hr&gt;
&lt;br/&gt;

&lt;h2 id=&quot;Sequence-Data-amp-Model&quot;&gt;&lt;a href=&quot;#Sequence-Data-amp-Model&quot; class=&quot;headerlink&quot; title=&quot;Sequence Data &amp;amp; Model&quot;&gt;&lt;/a&gt;Sequence Data &amp;amp; Model&lt;/h2&gt;&lt;br/&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;소리, 주가, 문자열 등의 데이터를 시퀀스 데이터로 분휴합니다&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;시계열 데이터는 시간순서에 따라 나열된 데이터로 시퀀스 데이터에 속한다&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;독립동등분포 가정을 잘 위해하기 때문에 순서를 바꾸거나 과거정보에 손실이 발생하면 데이터의 확률분포도 바뀌게 된다&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Markov model : first order autoregressive model&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;이들의 문제를 해결하기 위해 Latent autoregressive model&lt;/p&gt;
&lt;p&gt;hidden state가 과거의 정보들을 summerize한다&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p><br/><br/></p><h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><hr><br/><h2 id="Sequence-Data-amp-Model"><a href="#Sequence-Data-amp-Model" class="headerlink" title="Sequence Data &amp; Model"></a>Sequence Data &amp; Model</h2><br/><ul><li><p>소리, 주가, 문자열 등의 데이터를 시퀀스 데이터로 분휴합니다</p></li><li><p>시계열 데이터는 시간순서에 따라 나열된 데이터로 시퀀스 데이터에 속한다</p></li><li><p>독립동등분포 가정을 잘 위해하기 때문에 순서를 바꾸거나 과거정보에 손실이 발생하면 데이터의 확률분포도 바뀌게 된다</p></li><li><p>Markov model : first order autoregressive model</p></li><li><p>이들의 문제를 해결하기 위해 Latent autoregressive model</p><p>hidden state가 과거의 정보들을 summerize한다</p></li></ul><span id="more"></span><h3 id="다루는-법"><a href="#다루는-법" class="headerlink" title="다루는 법"></a>다루는 법</h3><ul><li>조건부 확률을 이용(과의 정보를 가지고 미래를 예측 )</li></ul><p><img src="/images/image-20210204112231880.png" alt="image-20210204112231880"></p><p>바로직전까지의 정보 S-1를 사용해서 현재인 S를 업데이트</p><p>반드시 모든 과거의 정보를 가지고 업데이트 하는 것은 아니다</p><p>따라서 조건부에 들어가는 데이터의 길이는 <strong>가변적이다</strong></p><br/><p>고정된 길이인 $\tau$만큼의 시퀀스만 활용하는 경우 Autoregressive Model(자기회귀모델)이라고 부른다</p><p>직전과거의 정보랑 직전정보가 아닌 정보들을 H<del>t</del>로 묶어서 활용</p><p><img src="/images/image-20210204114520892.png" alt="image-20210204114520892"></p><p>길이가 가변적이지 않고 이제 고정되기 때문에 여러가지 장점을 가지고 있다</p><p>사실은 과거의 모든 정보를 고려하기가 힘든 문제점을 고쳐서 이제 이전의 정보를 요약하는H<del>t</del>를 예측하는 모델 —-&gt; RNN</p><p><br/><img src="/images/image-20210204132225632.png" alt="image-20210204132225632"></p><h2 id="RNN-이해하기"><a href="#RNN-이해하기" class="headerlink" title="RNN 이해하기"></a>RNN 이해하기</h2><ul><li><p>기본적인 모형은 MLP와 유사하다</p></li><li><p><img src="/images/image-20210204115917697.png" alt="image-20210204115917697"></p></li><li><p>RNN의 역전파는 잠재변수의 연결그래프에 따라 순차적으로 계산한다</p><p>Back Propagation Through Time</p></li></ul><h3 id="BTTP를-살펴봅시다"><a href="#BTTP를-살펴봅시다" class="headerlink" title="BTTP를 살펴봅시다"></a>BTTP를 살펴봅시다</h3><p>BTTP를 통해 gradient를 계산해보면 미분의 곱으로 이루어진 항이 계산이 된다</p><p><img src="/images/image-20210204123130405.png" alt="image-20210204123130405"></p><p>길어지면 계산이 불안정해짐으로(gradient vanishing과 같은)문제가 있기 때문에</p><p>길이를 끊는것으로 truncated BPTT</p><br/><h3 id="Gradient-vanishing-문제의-해결"><a href="#Gradient-vanishing-문제의-해결" class="headerlink" title="Gradient vanishing 문제의 해결??"></a>Gradient vanishing 문제의 해결??</h3><ul><li>시퀀스 길이가 길어지는 경우에는 BTTP를 통한 역전파 알고리즘의 계산이 불안정해 지므로 길이를 끊는것이 중요하다</li><li>ex) LSTM, GRU …..</li></ul><p><img src="/images/image-20210204132428721.png" alt="image-20210204132428721"></p><p>RNN을 시간순으로 쭉 풀면 결국 fully connected layer network가 된다</p><p>가장어려운 ? 단점 ? —-&gt; 하나의 fixed rule로 이전의 정보들을 summerize하기 때문에 먼 과거의 정보들이 현재에서 살아남기가 힘들다!! 이게 short term dependencies</p><p><img src="/images/image-20210204132956315.png" alt="image-20210204132956315"></p><p>결국 먼 과거의 정보들은 많은 양의 activation function과 W곱의 결과로 vanishing or exploding되는 현상이 일어나게 된다</p><br/><br/><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p><img src="/images/image-20210204133224090.png" alt="image-20210204133224090"></p><p>LSTM의 전체적인 구조 </p><p><img src="/images/image-20210204133348991.png" alt="image-20210204133348991"></p><p>들어오는 입력이 3개 나가는게 3개</p><p>실제로 나가는건 h<del>t</del> (hidden state)</p><p><img src="/images/image-20210205094139431-1612486098130.png" alt="image-20210205094139431"></p><p><img src="/images/image-20210205094150152-1612486110711.png" alt="image-20210205094150152"></p><p><img src="/images/image-20210205094207767-1612486121259.png" alt="image-20210205094207767"></p>]]></content:encoded>
      
      
      <category domain="https://jo-member.github.io/categories/Boostcamp/">Boostcamp</category>
      
      
      <category domain="https://jo-member.github.io/tags/Basic/">Basic</category>
      
      <category domain="https://jo-member.github.io/tags/RNN/">RNN</category>
      
      <category domain="https://jo-member.github.io/tags/NLP/">NLP</category>
      
      
      <comments>https://jo-member.github.io/2021/02/04/2021-02-04-Boostcamp14.1/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>RNN2</title>
      <link>https://jo-member.github.io/2021/02/04/2021-02-04-Boostcamp14.2/</link>
      <guid>https://jo-member.github.io/2021/02/04/2021-02-04-Boostcamp14.2/</guid>
      <pubDate>Wed, 03 Feb 2021 15:00:00 GMT</pubDate>
      
      <description>&lt;p&gt;&lt;br/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h1 id=&quot;Transformer&quot;&gt;&lt;a href=&quot;#Transformer&quot; class=&quot;headerlink&quot; title=&quot;Transformer&quot;&gt;&lt;/a&gt;Transformer&lt;/h1&gt;&lt;br/&gt;

&lt;h2 id=&quot;Sequential-Model&quot;&gt;&lt;a href=&quot;#Sequential-Model&quot; class=&quot;headerlink&quot; title=&quot;Sequential Model&quot;&gt;&lt;/a&gt;Sequential Model&lt;/h2&gt;&lt;br/&gt;

&lt;p&gt;&lt;img src=&quot;/images/image-20210204173823314.png&quot; alt=&quot;image-20210204173823314&quot;&gt;&lt;/p&gt;
&lt;p&gt;위와 같은 문제로, RNN같이 sequential한 문제들을 해결할 때, 중간에 단어가 빠지거나 하면 해결하기가 어려움&lt;/p&gt;
&lt;p&gt;—&amp;gt; 여기서 나온게 Attention을 사용한 Transformer&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p><br/><br/></p><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><br/><h2 id="Sequential-Model"><a href="#Sequential-Model" class="headerlink" title="Sequential Model"></a>Sequential Model</h2><br/><p><img src="/images/image-20210204173823314.png" alt="image-20210204173823314"></p><p>위와 같은 문제로, RNN같이 sequential한 문제들을 해결할 때, 중간에 단어가 빠지거나 하면 해결하기가 어려움</p><p>—&gt; 여기서 나온게 Attention을 사용한 Transformer</p><span id="more"></span><br/><h2 id="Transformer-1"><a href="#Transformer-1" class="headerlink" title="Transformer"></a>Transformer</h2><br/><ul><li>Transformer is the first sequence transduction model based entirely on attention</li><li>RNN처럼 재귀적인게 아니라 based on attention</li></ul><p><img src="/images/image-20210204174108604.png" alt="image-20210204174108604"></p><ul><li>from a bird’s-eye view, this is what the Transformer does for machine translation tasks</li></ul><p><img src="/images/image-20210204185901783.png" alt="image-20210204185901783"></p><p>결국은 sequence to sequnece (불어 -&gt; 영어 )machine translation</p><p>입출력 sequence 는 숫자가 다를수 있다</p><p><img src="/images/image-20210204185951005.png" alt="image-20210204185951005"></p><p>모델은 하나임. 100개가 들어가도 100번 재귀적으로 들어가는게아니라 한번에 n개를 처리</p><p>generation할때는 1단어씩 만들게 된다</p><p>동일한구조를 가지지만 공유하지 않는 encoder와 decoder가 stack되어있다</p><p>encoder가 바뀔수 있는 n개의 단어를 어떻게 처리하는지</p><p>1개의 encoder에 n개의 단어가 한번에 들어간다</p><p>self attention + Feed Forward Neural Network -&gt;next encoder</p><p>Self- Attention is the cornerstone of Transformer</p><h2 id="Transformer-2"><a href="#Transformer-2" class="headerlink" title="Transformer"></a>Transformer</h2><p><img src="/images/image-20210205094243692.png" alt="image-20210205094243692"></p><p><img src="/images/image-20210205094305510.png" alt="image-20210205094305510"></p><p><img src="/images/image-20210205094323449.png" alt="image-20210205094323449"></p><p><img src="/images/image-20210205094338045.png" alt="image-20210205094338045"></p><p><img src="/images/image-20210205094354596.png" alt="image-20210205094354596"></p><p><img src="/images/image-20210205094421353.png" alt="image-20210205094421353"></p><p><img src="/images/image-20210205094434034.png" alt="image-20210205094434034"></p><p><img src="/images/image-20210205094446386.png" alt="image-20210205094446386"></p><p><img src="/images/image-20210205094508533.png" alt="image-20210205094508533"></p><p><img src="/images/image-20210205094524834.png" alt="image-20210205094524834"></p><p><img src="/images/image-20210205094724396.png" alt="image-20210205094724396"></p>]]></content:encoded>
      
      
      <category domain="https://jo-member.github.io/categories/Boostcamp/">Boostcamp</category>
      
      
      <category domain="https://jo-member.github.io/tags/Basic/">Basic</category>
      
      <category domain="https://jo-member.github.io/tags/RNN/">RNN</category>
      
      <category domain="https://jo-member.github.io/tags/Transformer/">Transformer</category>
      
      <category domain="https://jo-member.github.io/tags/NLP/">NLP</category>
      
      
      <comments>https://jo-member.github.io/2021/02/04/2021-02-04-Boostcamp14.2/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
