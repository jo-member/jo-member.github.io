{"pages":[{"title":"About Kinetic","text":"Intro.ì•ˆë…•í•˜ì„¸ìš”.","link":"/about/%E1%84%86%E1%85%AE%E1%84%8C%E1%85%A6.html"}],"posts":[{"title":"[ë°±ì¤€ 1157] ë‹¨ì–´ê³µë¶€","text":"ì•ŒíŒŒë²³ ëŒ€ì†Œë¬¸ìë¡œ ëœ ë‹¨ì–´ê°€ ì£¼ì–´ì§€ë©´, ì´ ë‹¨ì–´ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ì•ŒíŒŒë²³ì´ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë‚´ëŠ” í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•˜ì‹œì˜¤. ë‹¨, ëŒ€ë¬¸ìì™€ ì†Œë¬¸ìë¥¼ êµ¬ë¶„í•˜ì§€ ì•ŠëŠ”ë‹¤. ì²«ì§¸ ì¤„ì— ì•ŒíŒŒë²³ ëŒ€ì†Œë¬¸ìë¡œ ì´ë£¨ì–´ì§„ ë‹¨ì–´ê°€ ì£¼ì–´ì§„ë‹¤. ì£¼ì–´ì§€ëŠ” ë‹¨ì–´ì˜ ê¸¸ì´ëŠ” 1,000,000ì„ ë„˜ì§€ ì•ŠëŠ”ë‹¤. ì²«ì§¸ ì¤„ì— ì´ ë‹¨ì–´ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ì•ŒíŒŒë²³ì„ ëŒ€ë¬¸ìë¡œ ì¶œë ¥í•œë‹¤. ë‹¨, ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ì•ŒíŒŒë²³ì´ ì—¬ëŸ¬ ê°œ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ëŠ” ?ë¥¼ ì¶œë ¥í•œë‹¤. 123456789101112x =input().upper()used = {}for i in x: if i not in used: used[i] = 1 else: used[i]+=1lis = sorted(used.items(),key = (lambda x:x[1]),reverse=True)if len(lis)!=1 and lis[0][1]==lis[1][1]: print(&quot;?&quot;)else: print(lis[0][0]) counter ëª¨ë“ˆì„ ì‚¬ìš©í•˜ë©´ ë” ì‰½ê² ì§€ë§Œ êµ¬í˜„ì„ ì—°ìŠµí•˜ê¸° ìœ„í•´ ê·¸ëƒ¥ dictë¥¼ ì¨ì„œ êµ¬í˜„í•´ì£¼ì—ˆë‹¤. easyí–ˆë‹¤.","link":"/2021/01/02/2020-01-02-boj1157/"},{"title":"[ë°±ì¤€ 1316] ê·¸ë£¹ ë‹¨ì–´ ì²´ì»¤","text":"1. ì ‘ê·¼ &amp; êµ¬í˜„ê°„ë‹¨í•œ ë¬¸ì œì´ë‹¤. count í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì˜€ë‹¤. ì•ŒíŒŒë²³ì˜ ê°¯ìˆ˜ë¥¼ ìƒˆì¤€ë‹¤ìŒ ì´ë“¤ì´ ì—°ì†ëœ ê°¯ìˆ˜ì¸ì§€ë¥¼ checkí•´ì£¼ê¸° ìœ„í•´ count()ì˜ start,endë¥¼ ì •í•´ì£¼ì–´ ì´ë“¤ì´ ê°™ìœ¼ë©´ ì—°ì†ëœê²ƒì´ê³ , ë§Œì•½ ë‹¤ë¥´ë‹¤ë©´ ë’¤ì— ê°™ì€ ì•ŒíŒŒë²³ì´ ë” ë‚˜ì˜¨ë‹¤ëŠ” ëœ»ì´ë¯€ë¡œ ë°˜ë³µë¬¸ì„ breakí•´ì£¼ê³  bool ê°’ì„ falseë¡œ ë°”ê¾¸ì–´ì£¼ì–´ ê·¸ë£¹ë‹¨ì–´ê°€ ì•„ë‹˜ì„ ë‚˜íƒ€ë‚´ì¤€ë‹¤. ë§Œì•½ ì¡°ê±´ checkë¥¼ í†µê³¼í•˜ì˜€ë‹¤ë©´ indexë¥¼ ì—°ì†ëœ ì•ŒíŒŒë²³ ì´í›„ë¡œ ë°”ê¾¸ì–´ì£¼ì–´ ë°˜ë³µë¬¸ì„ ê³„ì†í•œë‹¤. 2. ì½”ë“œ(Python)12345678910111213141516N = int(input())l = [input() for _ in range(N)]cnt = 0for w in l: i = 0 b = True while i&lt;len(w): x = w.count(w[i]) if x!=w.count(w[i],i,i+x): b = False break else: i+=x if b: cnt+=1print(cnt)","link":"/2021/01/02/2020-01-02-boj1316/"},{"title":"[ë°±ì¤€ 2941] í¬ë¡œì•„í‹°ì•„ ì•ŒíŒŒë²³","text":"ê°„ë‹¨í•œ ë¬¸ìì—´ ë¬¸ì œì´ë‹¤. ì…ë ¥ìœ¼ë¡œ ì£¼ì–´ì§„ ë‹¨ì–´ì˜ í¬ë¡œì•„í‹°ì•„ ì•ŒíŒŒë²³ ê°¯ìˆ˜ë¥¼ ì¶œë ¥í•œë‹¤. ì¼ë‹¨ í¬ë¡œì•„í‹°ì•„ ì•ŒíŒŒë²³ì˜ ì •ë³´ë¥¼ ë‹´ê³ ìˆëŠ” dictionaryë¥¼ ë§Œë“¤ì–´ ì¤˜ë³´ì. ì´í›„ íŠ¹ë³„íˆ ê¸´ ì•ŒíŒŒë²³ì˜ ì‹œì‘ì„ ë³´ë©´ c,d,l,n,s,zì¼ë•Œë§Œ 1ìë¦¬ ë’¤ì—ê¹Œì§€ checkí•´ì£¼ê³  dzì¼ë•ŒëŠ” 3ë²ˆì§¸ê¹Œì§€ checkí•´ì¤€ë‹¤. 1234567891011121314151617181920x = input()d = [&quot;c=&quot;,&quot;c-&quot;,&quot;dz=&quot;,'d-','lj','nj','s=','z=']special = ['c','d','l','n','s','z']cnt = 0i = 0while i&lt;len(x): if x[i] in special: if i&lt;len(x)-2 and x[i]+x[i+1]+x[i+2] == 'dz=': i+=3 cnt+=1 elif i&lt;len(x)-1 and x[i]+x[i+1] in d: i+=2 cnt+=1 else: cnt+=1 i+=1 else: cnt+=1 i+=1print(cnt) indexì˜ ì¡°ì‘ì´ í•„ìš”í•´ forë¬¸ì´ ì•„ë‹Œ whileë¬¸ì„ ì‚¬ìš©í–ˆê³ , ë”±íˆ ì–´ë ¤ìš´ ë¬¸ì œëŠ” ì•„ë‹ˆì˜€ë‹¤.","link":"/2021/01/02/2020-01-02-boj2941/"},{"title":"[ë°±ì¤€ 5430] AC","text":"êµ¬í˜„ì„ ì™„ë£Œí–ˆì§€ë§Œ ì‹œê°„ ë³µì¡ë„ê°€ ë†’ì•„ ì‹œê°„ì´ˆê³¼ê°€ ë‚˜ì˜¨ë‹¤. reverseì™€ popì˜ ì‹œê°„ë³µì¡ë„ê°€ O(n)ì´ë¯€ë¡œ popë•Œë¬¸ì— ì´ëŸ¬í•œ ë¬¸ì œê°€ ë°œìƒí•˜ëŠ”ê²ƒ ê°™ë‹¤. ë”°ë¼ì„œ popì„ dequeë¥¼ ì‚¬ìš©í•˜ì—¬ popleftë¡œ ì‹œê°„ë³µì¡ë„ë¥¼ O(1)ë¡œ ë‚®ì¶”ì§€ë§Œ ì‹œê°„ì´ˆê³¼ëŠ” í•´ê²°ë˜ì§€ ì•Šì•˜ë‹¤. 12345678910111213141516171819202122from sys import stdinfrom collections import dequedef solution(command,l): if len(l)&lt;command.count('D'): return 'error' for c in command: if c==&quot;R&quot;: l.reverse() elif c==&quot;D&quot;: l.popleft() return '['+','.join(l)+']'T = int(input())ans = []for _ in range(T): command = stdin.readline() n = int(stdin.readline().rstrip()) l = deque(stdin.readline().rstrip()[1:-1].split(',')) if n==0: l = [] print(solution(command,l)) ë”°ë¼ì„œ Rì¼ë•Œë§ˆë‹¤ ë’¤ì§‘ì§€ ì•Šê³ , point ë³€ìˆ˜ë¥¼ ë§Œë“¤ì–´ì£¼ì–´ reverseì¼ë•Œë§ˆë‹¤ pointë³€ìˆ˜ë¥¼ ë°”ê¾¸ì–´ì£¼ê³  popì€ reverse ë§ˆë‹¤ ë°©í–¥ì„ ë°”ê¾¸ì–´ ì£¼ì–´ popì„ í•´ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„í•˜ì˜€ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ reverse í• ë•Œë§ˆë‹¤ pointê°’ì„ ë°”ê¾¸ì–´ì£¼ë©° Dì¼ë•Œ pointê°’ì„ í™•ì¸í•´ popleftí• ì§€ popí• ì§€ë¥¼ ê²°ì •í•œë‹¤. ìµœì¢…ì ìœ¼ë¡œ pointì˜ ê°’ì„ í™•ì¸í•˜ì—¬ reverseë¥¼ í• ì§€ ë§ì§€ ê²°ì •í•´ì¤€ë‹¤. 123456789101112131415161718192021222324252627282930313233from sys import stdinfrom collections import dequedef solution(command,l): point = 'left' if len(l)&lt;command.count('D'): return 'error' for c in command: if c==&quot;R&quot;: if point =='left': point ='right' else: point = 'left' elif c==&quot;D&quot;: if point =='left': l.popleft() else: l.pop() if point =='left': return '['+','.join(l)+']' else: l.reverse() return '['+','.join(l)+']'T = int(input())ans = []for _ in range(T): command = stdin.readline() n = int(stdin.readline().rstrip()) l = deque(stdin.readline().rstrip()[1:-1].split(',')) if n==0: l = [] print(solution(command,l)) í•´ê²°ì´ ë˜ì—ˆë‹¤!! ë¬´ì¡°ê±´ ë¬¸ì œì˜ ì¡°ê±´ì„ êµ¬í˜„í•˜ê¸° ë³´ë‹¤ëŠ” ë³´ë‹¤ íš¨ìœ¨ì ì¸ ë°©ë²•ì„ ì°¾ëŠ”ê²ƒì´ ë°”ëŒì§ í•˜ë‹¤ëŠ”ê²ƒì„ ëŠê¼ˆë‹¤.","link":"/2021/01/02/2020-01-02-boj5430/"},{"title":"[ë°±ì¤€ 15686] ì¹˜í‚¨ë°°ë‹¬","text":"ì ‘ê·¼ë¬¸ì œì˜ ì…ë ¥ì¡°ê±´ì´ ì‘ì•„ ëª¨ë“ ê±¸ íƒìƒ‰í•˜ëŠ” brute force ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í• ìˆ˜ìˆìŒì„ ì¶”ì¸¡í• ìˆ˜ìˆë‹¤. ì¼ë‹¨ Mê°œì˜ ì¹˜í‚¨ì§‘ì— ëŒ€í•œ ëª¨ë“  ì¡°í•©ì„ êµ¬í•´ listì— ë‹´ì€ë’¤ì— í•´ë‹¹ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° í•´ë‹¹ ì¹˜í‚¨ì§‘ë“¤ì´ ì„ íƒë˜ì—ˆì„ë•Œ ë„ì‹œì˜ ì¹˜í‚¨ê±°ë¦¬ë¥¼ êµ¬í•˜ì—¬ answerë°°ì—´ì— ë‹´ì•„ì¤€ë‹¤ ì´í›„ answerë°°ì—´ì¤‘ minê°’ì„ íƒí•˜ì—¬ ì¶œë ¥í•´ì¤€ë‹¤. from itertools import combinations for i, value in enumerate() ìœ„ë‘ê°€ì§€ë¥¼ ì˜ ì‚¬ìš© ì½”ë“œ1234567891011121314151617181920212223242526from itertools import combinationsN,M = map(int,input().split())m = [list(map(int,input().split())) for _ in range(N)]answer = []cnt = 0chicken = []for i in range(N): if 2 in m[i]: cnt += m[i].count(2) l = [i for i, value in enumerate(m[i]) if value == 2] for o in l: chicken.append([i,o])chicken = list(combinations(chicken,M))for x in chicken: d = [] for i in range(N): for j in range(N): if m[i][j] == 1: dist = [] for y in x: a, b = y dist.append(abs(i - a) + abs(j - b)) d.append(min(dist)) answer.append(sum(d))print(min(answer))","link":"/2021/01/04/2020-01-04-boj15686/"},{"title":"Blog ì‹œì‘","text":"ì‹ ë…„ì„ ë§ì´í•˜ì—¬ ìƒˆë¡­ê²Œ ë¬´ì–¸ê°€ë¥¼ ì‹œì‘í•´ë³¸ë‹¤ì •ë¦¬í•˜ê³  ì¦ëª…í•˜ê¸° ìœ„í•´ ê³µë¶€ì˜ í”ì ì„ ë‚¨ê²¨ë³´ì ë„¤ì´ë²„ Connectì—ì„œ ì§„í–‰í•˜ëŠ” boostcamp AI techì˜ BATì™€ 1ì°¨ ì½”ë”©í…ŒìŠ¤íŠ¸ë¥¼ í†µê³¼í•˜ì˜€ë‹¤. 1ì°¨ëŠ” ê·¸ë¦¬ ì–´ë ¤ìš´ ì•Œê³ ë¦¬ì¦˜ ì—†ì´ ê·¸ì € êµ¬í˜„í•˜ëŠ” ë¬¸ì œë“¤ì´ ì¶œì œ ë˜ì—ˆë‹¤.2ì°¨ê¹Œì§€ ì˜ë³´ì•„ì„œ ê¼­ ë¶™ì—ˆìœ¼ë©´ ì¢‹ê² ë‹¤. 6ê°œì›”ë™ì•ˆ ì—´ì •ì ìœ¼ë¡œ í•  ì¤€ë¹„ëŠ” ë˜ì–´ìˆë‹¤. ë¶„ëª… ëŒ€í•™ì› ì…ì‹œì— ë„ì›€ì´ ë˜ê² ì§€?","link":"/2020/12/28/2020-12-28-first-post/"},{"title":"Introduction and Motivation","text":"Introduction and Motivation Machine learningì˜ ëª©ì ì€ dataë¡œ ë¶€í„° valuable patternsì„ ë½‘ì•„ë‚´ëŠ” ê²ƒì´ë‹¤Datasetì— ë§ê²Œ Modelì„ ì„¤ê³„, Modelì€ inputê³¼ outputì˜ functionì„ describe í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤ A model is said to learn from data if its performance on a given task improves after the data is taken into account. Learningì´ë€ dataì˜ patternì´ë‚˜ structë¥¼ modelì˜ parameterì„ ìë™ìœ¼ë¡œ optimizeí•˜ë©° ì°¾ì•„ë‚¸ë‹¤. ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œì˜ ê·¼ë³¸ì ì¸ ì´í•´ë¥¼ ìœ„í•´ ìˆ˜í•™ì ì¸ ê¸°ë°˜ì€ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤ Finding Words for Intuition Predictor : Input dataì— ë§ëŠ” predictionì„ í•´ë‚´ëŠ” Algorithm Data as vectors vector as array (computer science view) vector as arrow with a direction and magnitude (physics view) vector as an object that obeys addition and scaling (mathmatical view) Model : input data-setê³¼ ë¹„ìŠ·í•œ dataë¥¼ ë§Œë“¤ì–´ ë‚´ê¸°ìœ„í•´ ì‚¬ìš©í•œë‹¤. ì¢‹ì€ ëª¨ë¸ì€ dataì˜ hidden patternì„ ë½‘ì•„ë‚¼ ìˆ˜ ìˆê³ , ì–´ë– í•œ ì¼ì´ ë‹¤ìŒì— ì¼ì–´ë‚ ì§€ ì˜ˆì¸¡ë˜í•œ í•  ìˆ˜ ìˆë‹¤. Learning : ìš°ë¦¬ê°€ datasetê³¼ modelì´ ì£¼ì–´ì¡Œë‹¤ê³  ê°€ì •í•˜ì. Modelì„ trainingí•˜ëŠ” ì‘ì—…ì€ training dataì— ë§ê²Œ modelì˜ parameterë¥¼ optimizeí•˜ëŠ” ê³¼ì •ì´ë‹¤ëŒ€ë¶€ë¶„ì˜ training ë°©ë²•ì€ ì‚°ì„ ì˜¤ë¥´ëŠ” ê³¼ì •ê³¼ ë¹„ìŠ·í•˜ë‹¤. ì´ ì‚°ì˜ ì •ìƒì€ maximum scoreë¥¼ ì˜ë¯¸í•œë‹¤. ìš°ë¦¬ëŠ” modelì´ UNSEEN DATA ì—ì„œ ì˜ ë™ì‘í•˜ê¸°ë¥¼ ì›í•œë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” modelì´ ì´ì „ì— ë§ˆì£¼ì¹œ ìƒí™©ê³¼ ë‹¤ë¥¸ ìƒí™©ì— ìì£¼ ë…¸ì¶œì‹œì¼œ ì£¼ì–´ì•¼ í•œë‹¤. Represent data as vector í™•ë¥ ê³¼ optimization viewë¥¼ ì‚¬ìš©í•´ ì ì ˆí•œ ëª¨ë¸ì„ ì„ ì • ë‹¤ì–‘í•œ numerical optimization ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ modelì´ trainingì— ì‚¬ìš©ëœ dataê°€ ì•„ë‹Œ ë‹¤ë¥¸ dataì—ì„œ ì˜ ë™ì‘í•˜ë„ë¡ í•¨ì— ì´ˆì ì„ ë§ì¶”ì–´ í•™ìŠµ {: .align-center} ì¶œì²˜ : Marc Peter Deisenroth, Mathmatics for Machine Learning,(Cambridge University Press)","link":"/2020/12/30/2020-12-30-ML&Mathmatics/"},{"title":"Linear Algebra","text":"Vector Geometric vectorsìš°ë¦¬ê°€ í”íˆ ì¤‘ê³ ë“±í•™êµë•Œ ë°°ìš´ vectorì˜ ê°œë… Polynomialsë‹¤í•­ì‹ ë˜í•œ vectorsì´ë‹¤. ë‘ ë‹¤í•­ì‹ì€ ë”í• ìˆ˜ìˆê³  scalarë°°ë˜í•œ í•  ìˆ˜ ìˆë‹¤.ë”°ë¼ì„œ Polynomials are Instance of Vecors Elements of $\\Bbb{R}^n$$a,b\\in\\Bbb{R}^3$ ì¼ë•Œ $a+b = c \\in\\Bbb{R}^3$ìŠ¤ì¹¼ë¼ë¡œ ê³±í•´ë„ ì†í•œë‹¤ Linear algebraëŠ” ì´ëŸ¬í•œ ë¹„ìŠ·í•¨ì— ì´ˆì ì„ ë‘”ë‹¤. {: .align-center} Vector spaceì˜ ê°œë…ê³¼ ì„±ì§ˆì´ ë§¤ìš° MLì—ì„œ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤.Vector spaceë€ ì‘ì€ Vectorì˜ ì§‘í•©ë“¤ë¼ë¦¬ ì„œë¡œ ë”í•˜ê³  Scailingí•˜ì—¬ì„œ ë‚˜ì˜¨ ì§‘í•©ì´ë‹¤. 2.1 Systems of linear equationsExample 2.1 {: .align-center}Product Nnì„ ìœ„í•´ Resource Rm ì´ í•„ìš”í•œê²½ìš° ì œí•œëŸ‰ : bj Product i ìƒì‚°ì„ ìœ„í•´ í•„ìš”í•œ Resource Rjì˜ ì–‘: aij Product i ì˜ ìƒì‚°ëŸ‰ : xi ìœ„ì™€ ê°™ì€ ì„ í˜• ë°©ì •ì‹ì„ ê°„ë‹¨í•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ matrixë¥¼ ì‚¬ìš© {: .align-center}{: width=â€50%â€ height=â€50%â€} {: .align-center}{: width=â€50%â€ height=â€50%â€} 2.2 Matrixí–‰ë ¬ì˜ ê¸°ë³¸ë²•ì¹™ì€ ì´ë¯¸ ì„ í˜•ëŒ€ìˆ˜í•™ ì‹œê°„ì— í•™ìŠµí•˜ì˜€ìœ¼ë‹ˆ ë„˜ì–´ê°€ê² ë‹¤. 2.3 Solving Systems of Linear Equation2.3.1 Particular and General Solution í•´ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì„ ì•Œê¸°ì— ì•ì„œ Linear Equationì˜ í•´ì—ëŒ€í•˜ì—¬ ì•Œì•„ë³´ì. ìœ„ì˜ í–‰ë ¬ì‹ì„ ë³´ë©´ 2ê°œì˜ ë°©ì •ì‹ê³¼ 4ê°œì˜ ë¯¸ì§€ìˆ˜ê°€ ìˆë‹¤. ë”°ë¼ì„œ ë¬´ìˆ˜íˆ ë§ì€ ê·¼ì´ ì¡´ì¬í•  ê²ƒì´ë‹¤. Ax = b ë¼ëŠ” compactí•œ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ë’¤ í•´ë¥¼ ì²«ë²ˆì§¸ column C1ì˜ 42 ë‘ë²ˆì§¸ column C2ì˜ 8ë°°ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤ ë”°ë¼ì„œ ì´ì˜ í•´ëŠ” $[42,8,0,0]^T$ ë¡œ ë‚˜íƒ€ë‚¼ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ ì‹ì€ ë¬´ìˆ˜íˆ ë§ì€ í•´ì¤‘ ë‹¨ í•˜ë‚˜ì˜ í•´ì´ê¸° ë•Œë¬¸ì— ì´ë¥¼==Particular solution==ì´ë¼ê³  ë¶€ë¥¸ë‹¤. ì´ì œ ëª¨ë“  í•´ë¥¼ ì¼ë°˜ì ìœ¼ë¡œ í‘œí˜„í•˜ê¸° ìœ„í•˜ì—¬ ìˆ˜í•™ì  techinqueì„ ì‚¬ìš©í•œë‹¤. 0ì„ ë”í•¨ìœ¼ë¡œì„œ ì–‘ë³€ì˜ equalityëŠ” ë³´ì¡´ë¨ìœ¼ë¡œ, 0ì„ ë§Œë“¤ì–´ ë‚´ë³´ì. ì„¸ë²ˆì§¸ columnì„ 1,2ë²ˆì§¸ columnì˜ í•©ìœ¼ë¡œ í‘œí˜„í•´ ë³´ë©´ ë”°ë¼ì„œ 0 = 8C1+ 2C2- 1C3 +0C4 ë¡œ ë‚˜íƒ€ë‚¼ìˆ˜ ìˆê³  ì´ë“¤ì˜ ê³„ìˆ˜ì¸ [8,2,-1,0]^T^ ê°€ ë°”ë¡œ 0ì„ ë§Œë“¤ì–´ ë‚´ëŠ” Vectorì´ë‹¤. ë˜í•œ ì´ í•´(Vector)ì˜ scalarë°°ìˆ˜ë“¤ ë˜í•œ ê°™ì€ ê²°ê³¼ë¥¼ ê°€ì§„ë‹¤. ë”°ë¼ì„œ ì´í›„ ì‘ì„±â€¦.. ì¶œì²˜ : Marc Peter Deisenroth, Mathmatics for Machine Learning,(Cambridge University Press)","link":"/2020/12/30/2020-12-30-ML&Mathmatics2/"},{"title":"Pythonì˜ ìë£Œí˜• &amp; Pythonic code","text":"ì˜¤ëŠ˜ë°°ìš´ ë‚´ìš©ë“¤ì€ êµ‰ì¥íˆ ë°©ëŒ€í•˜ë‹¤ 1. ìë£Œêµ¬ì¡°íŠ¹ì§•ì´ ìˆëŠ” ì •ë³´ë¥¼ ì–´ë–»ê²Œ ì €ì¥í•˜ë©´ ì¢‹ì„ê¹Œ? ê¸°ë³¸ë°ì´í„° êµ¬ì¡° ìŠ¤íƒê³¼ í íŠœí”Œê³¼ ì§‘í•© ì‚¬ì „ (dictionary) Collection ëª¨ë“ˆ 1-1) StackìŠ¤íƒì´ë€?ë‚˜ì¤‘ì— ë“¤ì–´ì˜¨ ë°ì´í„°ë¥¼ ë¨¼ì € ë°˜í™˜í•˜ë„ë¡ ì„¤ê³„ëœ ë©”ëª¨ë¦¬ êµ¬ì¡° LIFO (Last In First Out) Dataì˜ ì…ë ¥ : Push Dataì˜ ì¶œë ¥ : Pop Stack with list object append()ì™€ pop()ì„ ì‚¬ìš©í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ìŠ¤íƒì„ êµ¬í˜„ê°€ëŠ¥ a.pop() ì´ë¼ëŠ” í•¨ìˆ˜ëŠ” returnì´ ì¡´ì¬í•˜ê³  a ë„ ë³€í™”ì‹œì¼œì¤Œ 1-2) QueueQueueë€?ë¨¼ì €ë“¤ì–´ì˜¨ ë°ì´í„°ë¥¼ ë¨¼ì € ë°˜í™˜í•˜ë„ë¡ ì„¤ê³„ëœ ë©”ëª¨ë¦¬ êµ¬ì¡° FIFO(First in First out) Queue with list object append()ì™€ pop(0)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ íë¥¼ êµ¬í˜„ê°€ëŠ¥ 1-3) TupleTupleì´ë€? ê°’ì˜ ë³€ê²½ì´ ë¶ˆê°€ëŠ¥í•œ ë¦¬ìŠ¤íŠ¸ ì„ ì–¸ì‹œ [] ê°€ ì•„ë‹Œ ()ë¥¼ ì‚¬ìš© tupleì˜ ìš”ì†Œì— ì¸ë±ìŠ¤ë¡œ ì ‘ê·¼í•˜ì—¬ ë³€ê²½ì‹œë„ì‹œ ì˜¤ë¥˜ ë°œìƒ(í• ë‹¹ì´ ì•ˆëœë‹¤) ì™œì“¸ê¹Œ? í”„ë¡œê·¸ë¨ ì‘ë™ì¤‘ ë³€ê²½ë˜ë©´ ì•ˆë˜ëŠ” ë°ì´í„°ë¥¼ ì €ì¥í• ë•Œ ì‚¬ìš© ex) í•™ë²ˆ ìš°í¸ë²ˆí˜¸ ì´ë¦„ â€¦. ì‚¬ìš©ìì˜ ì‹¤ìˆ˜ì— ì˜í•œ ì—ëŸ¬ë¥¼ ë°©ì§€í•¨ (1)ì˜ íƒ€ì…ì€ int tupleë¡œ ë§Œë“¤ê³  ì‹¶ë‹¤ë©´ (1,)ë¼ê³  ì„ ì–¸í•´ì£¼ì–´ì•¼ í•¨ 1-4) SetSetì´ë€? ê°’ì„ ìˆœì„œì—†ì´ ì €ì¥, ì¤‘ë³µì´ ì•ˆë¨ set ê°ì²´ì˜ ì„ ì–¸ìœ¼ë¡œ êµ¬í˜„ê°€ëŠ¥ 123s = set([1,2,3,4,1,2,3,4])s = {1,2,3,1,2,3,4}#ì´ëŸ°ì‹ìœ¼ë¡œ ì„ ì–¸ì„ í•˜ë©´ ì¤‘ë³µì´ ì‚¬ë¼ì§ remove(), add(), update(), discard()ë¡œ ê°’ ìˆ˜ì •ê°€ëŠ¥ Setì˜ ì—°ì‚°1234567891011s1 = set([1,2,3,4,5])s2 = set([3,4,5,6,7])s1.union(s2) s1 | s2 # s1ê³¼ s2ì˜ í•©ì§‘í•©s1.intersection(s2)s1 &amp; s2 # s1ê³¼ s2ì˜ êµì§‘í•©s1.difference(s2)s1 - s2 #s1ê³¼ s2ì˜ ì°¨ì§‘í•© 1-5) Dictì‚¬ì „ (dictionary) ë€? ë°ì´í„°ë¥¼ ì €ì¥í•  ë•Œ ë°ì´í„°ë¥¼ êµ¬ë¶„ì§€ì„ ìˆ˜ ìˆëŠ” ê°’ì„ í•¨ê»˜ ì €ì¥ êµ¬ë¶„ì„ ìœ„í•œ ë°ì´í„° ê°’ : key keyê°’ì„ í™œìš©í•˜ì—¬, valueë¥¼ ê´€ë¦¬í•¨ 123456789101112131415c = {&quot;a&quot; : 1, &quot;b&quot; :2}c.items() # tupleì˜ í˜•íƒœë¡œ keyì™€ valueê°’ë“¤ì´ ë‚˜ì˜´c.values() # dictì˜ valueê°’ë§Œì´ listë¡œ ë‚˜ì˜´c.keys() #dictì˜ keyê°’ë§Œì´ listë¡œ ë‚˜ì˜´c[&quot;c&quot;] = 3 # dictì— ì¶”ê°€&quot;k&quot; in c#false&quot;a&quot; in c#truefor k,v in c.items(): print(k) print(v) 1-6) Collections List, Tuple, Dictë¥¼ íŒŒì´ì¬ì—ì„œ ëª¨ë“ˆë¡œ ì§€ì›í•´ì¤Œ 1-6-1) Deque queueì™€ stackì„ ë™ì‹œì— ì§€ì›í•¨ Listì— ë¹„í•´ ì‹œê°„ë³µì¡ë„ê°€ ë‚®ì•„ ë¹ ë¦„ (popì´ ì‹œê°„ë³µì¡ë„ê°€ ë‚®ìŒ) append() appendleft() Linked listì˜ íŠ¹ì„±ì„ ì§€ì›í•¨ ê¸°ì¡´ì˜ listí•¨ìˆ˜ë“¤ë„ ëª¨ë‘ ì§€ì›í•¨ 123456789101112131415from collections import dequed = deque()for i in range(5): d.append(i)#d = deque([0,1,2,3,4])d.appendleft(10)# d = deque([10,0,1,2,3,4])d.rotate(1)# d = deque([4,10,0,1,2,3])d.extend([1,2,3])# d = deque([4,10,0,1,2,3,1,2,3]) 1-6-2) Ordered dictDictì™€ ë‹¬ë¦¬ ë°ì´í„°ë¥¼ ì…ë ¥í•œ ìˆœì„œëŒ€ë¡œ dictë¥¼ ë°˜í™˜í•¨ ê·¸ëŸ¬ë‚˜ ì§€ê¸ˆì€ ë³„ë¡œ ì˜ë¯¸ê°€ ì—†ìŒ 1-6-3) Default Dict12345678910d = dict()print(d[&quot;first&quot;])#error ë°œìƒ#ì´ëŸ´ë•Œ defaultë¥¼ ì£¼ë©´ ë¨from collections import defaultdictd = defaultdict(lambda : 0)d[&quot;first&quot;]# 0 ex) í•˜ë‚˜ì˜ ì§€ë¬¸ì— ëª‡ê°œì˜ ë‹¨ì–´ê°€ ë‚˜ì˜¤ëŠ”ì§€ ì„¸ê³  ì‹¶ì€ ê²½ìš° 1234from collections import defaultdictd = defaultdict(lambda : 0)for word in text: d[word]+=1 1-6-4) Counter Sequence typeì˜ data elementë“¤ì˜ ê°¯ìˆ˜ë¥¼ dictì˜ í˜•íƒœë¡œ ë°˜í™˜ 1234567891011121314151617181920212223242526from collections import Counterc = Counter()c = Counter('gallahad')print(c)# Counter('a':3, 'l':2, 'g' : 1, 'd' : 1, 'h' : 1)c = Counter({'red':4,'blue' : 2})print(c)# Counter('red': 4, 'blue' : 2)print(list(c.elements()))# ['blue','blue','red','red','red','red']c = Counter(dogs = 10, cats = 8)print(c)# Counter('dogs':10,'cats':8)print(list(c.elements()))'# ['dogs','dogs','dogs','dogs','dogs','dogs','dogs','dogs','dogs','dogs',...,'cats']#Setì˜ ì—°ì‚°ë“¤ì„ ì§€ì›í•¨c1 = Counter('allis')c2 = Counter('paul')print(c1-c2)print(c1.subtract(c2))# Counter('a' : 0, 'l' : 1, 'i':1,'s' :1, 'p' : -1, 'u' : -1)print((c1-c2)['a'])#0 1-6-5) Namedtuple tupleì˜ í˜•íƒœë¡œ Data êµ¬ì¡°ì²´ë¥¼ ì €ì¥í•˜ëŠ” ë°©ë²• ì €ì¥ë˜ëŠ” dataì˜ variableì„ ì‚¬ì „ì— ì§€ì •í•´ì„œ ì €ì¥í•¨ 12345from collections import namedtuplePoint = namedtuple('Point', ['x','y'])p = Point(x = 11, y = 22)p[0], p[1] 2. Pythonic code íŒŒì´ì¬ ìŠ¤íƒ€ì¼ì˜ ì½”ë”© ê¸°ë²• íŒŒì´ì¬ íŠ¹ìœ ì˜ ë¬¸ë²•ì„ ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ ì½”ë“œë¥¼ í‘œí˜„í•¨ ë§ì€ ì½”ë“œë“¤ì˜ ì¥ì ì„ ì±„ìš©í•¨ ê³ ê¸‰ì½”ë“œ ì‘ì„±ì‹œ ë§ì´ í•„ìš”í•¨ ex) 12colors = ['red','blue','green']result = ''.join(colors) split, join list comprehension enumerate, zip lambda, map, reduce generateor asterisk ì™œ ì“¸ê¹Œ? ë‚¨ë“¤ì˜ ì½”ë“œë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ íš¨ìœ¨ : ë‹¨ìˆœ for loopë³´ë‹¤ listê°€ ë” ë¹ ë¥´ë‹¤ ì“°ë©´ ê°„ì§€ë‚œë‹¤ 2-1) Split &amp; Join**string typeê°’ì„ ê¸°ì¤€ê°’ìœ¼ë¡œ ë‚˜ëˆ ì„œ List ë¡œ return ** ex) split 12ex = &quot;python, java, javascript&quot;p, j, js = ex.split(,) #unpacking ex) join 123ex = ['python', 'java', 'javascript']s = '-'.join(ex)#s = 'python-java-javascript' 2-2) List Comprehension ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“ ë‹¤ íŒŒì´ì¬ì—ì„œ ê°€ì¥ë§ì´ ì‚¬ìš©ëœë‹¤ for + append ë³´ë‹¤ ë¹ ë¥´ë‹¤ ex) 12result = [i for i in range(5) if i%2==0]#result = [0,2,4] ex) 12345word1 = &quot;hello&quot;word2 = &quot;world&quot;result = [i+j for i in word1 for j in word2]# result = ['hw','ho','hr'....., 'od'] 1234567891011121314151617case_1 = [&quot;A&quot;,&quot;B&quot;,&quot;C&quot;]case_2 = [&quot;D&quot;,&quot;E&quot;,&quot;A&quot;]result = [i+j for i in case_1 for j in case_2 if not(i==j)]# Filter: ië‘ jê³¼ ê°™ë‹¤ë©´ Listì— ì¶”ê°€í•˜ì§€ ì•ŠìŒ# [i+j if not(i==j) else i for i in case_1 for j in case_2]words = 'The quick brown fox jumps over the lazy dog'.split()# ë¬¸ì¥ì„ ë¹ˆì¹¸ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ  listë¡œ ë³€í™˜#print (words)'''['The', 'quick', 'brown', 'fox', 'jumps','over', 'the', 'lazy', 'dog']'''&gt;&gt;&gt; stuff = [[w.upper(), w.lower(), len(w)]for w in words]# listì˜ ê° elementeë“¤ì„ ëŒ€ë¬¸ì, ì†Œë¬¸ì Two dimensional arrayê°€ í•„ìš” 123456789101112131415case_1 = [&quot;A&quot;,&quot;B&quot;,&quot;C&quot;]case_2 = [&quot;D&quot;,&quot;E&quot;,&quot;A&quot;]result = [[i+j for i in case1] for j in case2]#ë’¤ì—ê»˜ ë¨¼ì € ì‘ë™í•˜ê³  '''for j in case2: line = [] for i in case1: line.append(i+j) result = [['AD','BD','CD'], ['AE','BE','CE'],['AA','BA','CA']]''' 2-3) Enumerate &amp; ZipEnumerate1234567891011my_str = 'ABCD'd = {v:i for i,v in enumerate(my_str)}mylist = ['a', 'b', 'c', 'd']l = list(enumerate(mylist))#[(0, 'a'), (1, 'b'), (2, 'c'), (3, 'd')]{i:j for i,j in enumerate('Artificial intelligence (AI), is intelligence demonstrated by machines,unlike the natural intelligence displayed by humans and animals.'.split())}#ë¬¸ì¥ì„ ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë‹¨ì–´ë“¤ì˜ listë¡œ ë§Œë“¤ê³  listì˜ indexì™€ ê°’ì„ unpackingí•˜ì—¬ dictë¡œ ì €ì¥ Zipë‘ê°œì˜ listì˜ ê°’ì„ ë³‘ë ¬ì ìœ¼ë¡œ ==tuple== íƒ€ì…ìœ¼ë¡œ ì¶”ì¶œí•˜ì—¬ ì €ì¥ 123456math = (100,80,90)kor = (90,90,80)eng = (90, 80, 70)result = [sum(value)/3 for value in zip(math,kor,eng)]# [ì„¸í•™ìƒì˜ í‰ê· ì ìˆ˜ê°€ ì €ì¥ë¨] ê°™ì€ ìœ„ì¹˜ì— ìˆëŠ” ê°’ë“¤!! 2-4) Lambda &amp; Map &amp; ReduceLambda í•¨ìˆ˜ ì´ë¦„ì—†ì´ ì“¸ìˆ˜ìˆëŠ” ìµëª…í•¨ìˆ˜ ìˆ˜í•™ì˜ ëŒë‹¤ ëŒ€ìˆ˜ì—ì„œ ìœ ë˜ë¨ ex) 1234567f = (lambda x,y : x+y)f(10,50)#60 ê°’ì´ return ì´ë¨up_low = lambda x: x.upper()+ x.lower()# s = up_low(&quot;my happy&quot;)# s = &quot;MY HAPPYmy happy&quot; íŒŒì´ì¬ 3ë¶€í„° ê¶Œì¥ë˜ì§€ ì•ŠìŒ defë¥¼ ê±ì¨ë¼ ë¬¸ë²•ì´ ì–´ë ¤ì›€ í•¨ìˆ˜ ì‘ë™ì˜ test ê°€ì–´ë ¤ì›€ docstringì´ ì—†ìŒ ë‹¤ë¥¸ì‚¬ëŒë“¤ì˜ ì½”ë“œ í•´ì„ì´ ì–´ë ¤ì›€ ì´ë¦„ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í•¨ìˆ˜ê°€ ìƒê¹€ Map sequnceí˜• dataê°€ìˆì„ë•Œ í•¨ìˆ˜ì— ê°ê° ì ìš©ì„ í•´ì£¼ê³  ê·¸ê²°ê³¼ë¥¼ ë°›ëŠ” ê¸°ëŠ¥ ë‘ê°œì´ìƒì˜ listì— ì ìš©ê°€ëŠ¥, if filterë„ ì‚¬ìš©ê°€ëŠ¥ ex) 12345678ex = [1,2,3,4]f = lambda x : x**2l = list(map(f,ex))l = [f(value) for value in ex]# l = [1,4,9,16]f = lambda x,y : x*yl = list(map(f,ex,ex))# l = [1,4,9,16] Reduce map functionê³¼ ë‹¬ë¦¬ listì— ë˜‘ê°™ì€ í•¨ìˆ˜ë¥¼ ì ìš©í•´ì„œ í†µí•© 1234567891011from functools import reducex = reduce(lambda x,y : x+y, [1,2,3,4,5])'''1+2 = 33+3 = 66+4 = 1010+5 = 15ë”°ë¼ì„œ x = 15ì˜ ê°’ì´ ë‚˜ì˜¨ë‹¤''' ë¨¸ì‹ ëŸ¬ë‹ì—ì„œëŠ” ì—¬ì „íˆ Lambda, Map, Reduceë¥¼ ë§ì´ ì‚¬ìš©í•¨ 2-5) Iterable Object Sequnceí˜• ìë£Œí˜•(list,tuple,ë¬¸ìì—´ ë“±ë“±)ì—ì„œ ë°ì´í„°ë¥¼ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œí•˜ëŠ” object 12for city in ['s','t','n']: print(city) ìœ„ì™€ ê°™ì´ dataë¥¼ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œí•˜ëŠ” ë‚´ë¶€ì ìœ¼ë¡œëŠ” iter ê³¼ __next___ê°€ì‚¬ìš©ë¨ iter(), next() í•¨ìˆ˜ë¡œ í•œê°œì”© ë„˜ì–´ê° 123456789101112cities = ['Seoul',&quot;Busan&quot;,&quot;Jeju&quot;]# ì´ìì²´ê°€ ëª¨ë‘ memoryì— ì˜¬ë¼ê°€ê²Œ ë¨ ê·¸ëŸ°ë°memory_iter_object = iter(cities)# ì´ë ‡ê²Œ í•´ë²„ë¦¬ë©´ memory ë‹¤ìŒìœ„ì¹˜ë“¤ì— ëŒ€í•œ ì£¼ì†Œê°’ì„ ê°€ì ¸ì˜¤ê²Œë¨print(next(memory_iter_object))#Seoulprint(next(memory_iter_object))#Busanprint(next(memory_iter_object))#'Jeju' {: .align-center} ì´ëŸ°ì‹ìœ¼ë¡œ iterable objectê°€ ì €ì¥ì´ ë¨ listëŠ” ì‚¬ì‹¤ citiesë¼ëŠ”ê²Œ í¬ì¸í„°ê³  ë°°ì—´ì•ˆì— ì£¼ì†Œê°’ì´ ê°ê°ì €ì¥ë˜ì–´ìˆì–´ ê° indexë§ˆë‹¤ ì£¼ì†Œê°’ì´ ì €ì¥ë˜ì–´ìˆìŒ ì´ê±¸ iterable í•œ objectë¡œ í•´ì£¼ë©´ nodeê°€ ë‹¤ìŒê°ì²´ì˜ ì£¼ì†Œê°’ì„ ê°€ì§€ê³ ìˆìŒ Generator(ëª°ëë˜ê±°ë¼ ë§¤ìš° ì¤‘ìš”)Generator iterable objectë¥¼ íŠ¹ìˆ˜í•œ í˜•íƒœë¡œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜ elementê°€ ì‚¬ìš©ë˜ëŠ” ì‹œì ì— ê°’ì„ ë©”ëª¨ë¦¬ì— ë°˜í™˜ : yeildë¥¼ ì‚¬ìš©í•˜ì—¬ í•œë²ˆì— í•˜ë‚˜ì˜ elementë§Œì„ ë°˜í™˜ 12345678910111213141516171819202122232425import sysdef general_list1(value): result= [] for i in range(value): l.append(i) return resultprint(general_list1(50))#[1,2,3,...,49]result = general_list1(50)sys.getsizeof(result)#520 def general_list2(value): result = [] for i in range(value): yeild i print(general_list2(50))# &lt;generator object&gt;for a in general_list2(50): print(a)#í‰ì†Œì—ëŠ” memoryì— ì‹¤ì œë¡œ ê°’ì´ ì•ˆë“¤ì–´ê°€ ìˆê³  ì£¼ì†Œê°’ë§Œì„ ê°€ì§€ê³  ìˆìŒ#print í˜¸ì¶œì‹œ ì£¼ì†Œê°’ì„ í™œìš©í•˜ì—¬ yeildê°€ ê°’ì„ ì¤Œ# ë”°ë¼ì„œ generatorë¥¼ ì‚¬ìš©í•˜ë©´ memory ì‚¬ì´ì¦ˆë¥¼ ì¤„ì¼ìˆ˜ìˆìŒ, ëŒ€ìš©ëŸ‰ data Generator Comprehension1234g = (n*n for n in range(500))print(type(g))#&lt;class generator&gt; 2-6) Function Passing Argument Keyword argument Default argument Variable-length arguments Keyword argument í•¨ìˆ˜ì— ì…ë ¥ë˜ëŠ” parameterì˜ ë³€ìˆ˜ëª…ì„ ì‚¬ìš©, argumentsë¥¼ ë„˜ê¹€ ì´ë¦„ëŒ€ë¡œ í•¨ìˆ˜ì— parameterê°€ ë“¤ì–´ê° (ìˆœì„œê°€ X) 1234def print_somthing(my_name, your_name):print(&quot;Hello {0}, My name is {1}&quot;.format(your_name, my_name))print_somthing(&quot;Sungchul&quot;, &quot;TEAMLAB&quot;)print_somthing(your_name=&quot;TEAMLAB&quot;, my_name=&quot;Sungchul&quot;) Default argumentêµ³ì´ ë„£ì–´ì£¼ì§€ ì•Šì•„ë„ defaultë¡œ ê°’ì„ ì„¤ì •í•´ì¤„ì‹œ ê·¸ê°’ìœ¼ë¡œ ìƒì„±ì´ë¨ 1234def print_somthing_2(my_name, your_name=&quot;TEAMLAB&quot;):print(&quot;Hello {0}, My name is {1}&quot;.format(your_name, my_name))print_somthing_2(&quot;Sungchul&quot;, &quot;TEAMLAB&quot;)print_somthing_2(&quot;Sungchul&quot;) Variable Length argumentsí•¨ìˆ˜ì˜ parameterê°€ ì •í•´ì ¸ìˆì§€ ì•Šì•˜ë‹¤ë©´ ? -&gt; asterisk(ê°€ë³€ì¸ì)ë¥¼ ì‚¬ìš© *argsë¥¼ ë³€ìˆ˜ëª…ìœ¼ë¡œ ì‚¬ìš© {: .align-center} ìœ„ì™€ ê°™ì´ ì‚¬ìš©ê°€ëŠ¥ *argsì˜ typeì€ tupleí˜•íƒœë¡œ ì—¬ëŸ¬ê°œì˜ ê°’ì´ ë¬¶ì„ Keyword variable - length Parameter ì´ë¦„ì„ ë”°ë¡œ ì§€ì •í•˜ì§€ ì•Šê³  ì…ë ¥í•˜ëŠ” ë°©ë²• asterisk 2ê°œë¥¼ ì‚¬ìš©í•˜ì—¬ í•¨ìˆ˜ì˜ parameterë¥¼ í‘œì‹œ ì…ë ¥ëœ ê°’ë“¤ì€ dictë¡œ ì‚¬ìš©ê°€ëŠ¥ ê°€ë³€ì¸ìëŠ” ì˜¤ì§ í•œê°œë§Œ ê¸°ì¡´ ê°€ë³€ì¸ì ë‹¤ìŒì— ì‚¬ìš© 12345678910111213141516171819202122def kwargs_test_1(**kwargs): print(kwargs) print(type(kwargs)) kwargs_test_1(first = 3, second = 4, third = 5)'''{'first' : 3, 'second' : 4, 'third' = 5}&lt;class dict&gt;'''def kwargs_test_3(a,b,c,*args,**kwargs): print(a+b+c+sum(args)) print(kwargs) kwargs_test_3(1,2,3,5,2,1, first = 3,second = 4,third = 5)'''13{'first' : 3, 'second' : 4, 'third' : 5}''' 2-7) AsteriskUnpacking a container123456789101112131415161718192021222324252627282930313233343536373839404142434445def astersk_test(a, *args): print(a,args) print(type(args)) astersk_test(1,*(2,3,4,5))#ì´ë•Œ tuple 1ê°œì˜ ë³€ìˆ˜ê°€ ë“¤ì–´ê°„ë‹¤ê³  ìƒê°í•˜ì§€ë§Œ *ê°€ ë“¤ì–´ê°€ì„œ tupleì´ í’€ë¦¬ê²Œ ë¨# ë”°ë¼ì„œ astersk_test(1,2,3,4,5)ë¼ê³  ìƒê°í•¨astersk_test(1,(2,3,4,5))# 1 ((2, 3, 4, 5),)# &lt;class 'tuple'&gt;print(*[1,2,3,4])#1 2 3 4print([1,2,3,4])#[1, 2, 3, 4]a,b,c = ([1,2],[2,3],[3,4])print(a,b,c)data = ([1,2],[2,3],[3,4])print(data)print(*data)'''[1, 2] [2, 3] [3, 4]([1, 2], [2, 3], [3, 4])[1, 2] [2, 3] [3, 4]'''# zipì„ í™œìš©data = ([1,2],[3,4],[5,6])for d in zip(*data): print(d)#(1, 3, 5)#(2, 4, 6)#ë§Œì•½ ìœ„ì—ê²Œ ì—†ìœ¼ë©´data = ([1,2],[3,4],[5,6])a,b,c = datafor d in zip(a,b,c): print(d)# keyword unpackingdef ast(a,b,c,d): print(a,b,c,d)data = {'a' : 1, 'b' :2, 'c' : 4}ast(10,**data)#10 1 2 4 ==asteriskê°€ í•¨ìˆ˜ì˜ argumentë¡œ ë“¤ì–´ê°”ì„ë•Œ unpackingì´ ì¼ì–´ë‚œë‹¤== asteriskë¥¼ 2ê°œ ì‚¬ìš©ì‹œ keyword unpackingì´ë¼ê³  ë°”ê¿”ì¤€ë‹¤ ìƒˆë¡­ê²Œ ë°°ìš´ê²ƒ ==default dict== ==named tuple== ==reduce== ==generator== ==Variable Length arguments== ==Keyword variable - length== ==Asterisk==","link":"/2021/01/20/2021-01-20-Boostcamp3/"},{"title":"Day2","text":"VariableVariable &amp; Memoryë³€ìˆ˜ë€? ë³€ìˆ˜ = ê°’ ë³€ìˆ˜ëŠ” ë©”ëª¨ë¦¬ ì£¼ì†Œë¥¼ ê°€ì§€ê³  ìˆê³  ë³€ìˆ˜ì— ë“¤ì–´ê°€ëŠ” ê°’ì€ ì£¼ì†Œì— í•´ë‹¹ë¨ ex) A = 8 : Aë¼ëŠ” ì´ë¦„ì„ ê°€ì§„ ë©”ëª¨ë¦¬ì£¼ì†Œì— 8ì„ ì €ì¥í•´ë¼ â€» í°ë…¸ì´ë§Œ êµ¬ì¡° {: .align-center} Dynamic Typingì½”ë“œì˜ ì‹¤í–‰ì‹œì ì— íƒ€ì…ì„ ê²°ì • - ì»´íŒŒì¼ëŸ¬ ì–¸ì–´ê°€ ì•„ë‹Œ ì¸í„°í”„ë¦¬í„° ì–¸ì–´ ì—°ì‚°ìì™€ í”¼ì—°ì‚°ìë¬¸ìê°„ì—ë„ ì—°ì‚°ì´ ê°€ëŠ¥í•¨ (concatnate) ë°ì´í„° í˜•ë³€í™˜ float(), int()ì™€ ê°™ì€ í•¨ìˆ˜ë¡œ ë°ì´í„°ì˜ í˜•ë³€í™˜ ê°€ëŠ¥ Listì¸ë±ì‹± list ì— ìˆëŠ” ê°’ë“¤ì€ ì£¼ì†Œê°’ì„ ê°€ì§ -&gt; ì£¼ì†Œë¥¼ ì‚¬ìš©í•˜ì—¬ ê°’ì„ í˜¸ì¶œ ìŠ¬ë¼ì´ì‹± [ì‹œì‘ index : ë index : ê°„ê²©] ë¦¬ìŠ¤íŠ¸ì˜ ì—°ì‚°ë‘ê°œì˜ ë¦¬ìŠ¤íŠ¸ ì‚¬ì´ì—ë„ concatnationì´ ê°€ëŠ¥ append : ë¦¬ìŠ¤íŠ¸ì— ìš”ì†Œ ì¶”ê°€ extend : ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€ insert : indexì— ìš”ì†Œ ì¶”ê°€ remove : ë¦¬ìŠ¤íŠ¸ì— íŠ¹ì • ìš”ì†Œ ì‚­ì œ del : indexë¥¼ ì£¼ê³  í•´ë‹¹ indexìš”ì†Œ ì‚­ì œ 12345a = [1,2,3,4,5]b = [5,4,3,2,1]b = a ì´ë•Œ a,bëŠ” ì•ì€ ë³µì‚¬ë¥¼ í•˜ê²Œë˜ì–´ ì£¼ì†Œë¥¼ ê³µìœ í•˜ê²Œë¨ ë”°ë¼ì„œ ê°’ë§Œ ë³µì‚¬í•˜ê³  ì‹¶ì„ì‹œ b = a[:] 123t = [1,2,3]a,b,c = t ì´ì°¨ì› ë¦¬ìŠ¤íŠ¸ ì´ì°¨ì› ë¦¬ìŠ¤íŠ¸ë¥¼ ë³µì‚¬í•˜ëŠ” ë²•? 123import copycopy_score = copy.deepcopy(midterm_score) Function &amp; ConsoleFunctioní•¨ìˆ˜ë€? ì–´ë–¤ ì¼ì„ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œì˜ ë©ì–´ë¦¬ ë°˜ë³µì ì¸ ìˆ˜í–‰ì„ 1íšŒ ì‘ì„±í›„ ì§€ì†ì ì¸ í˜¸ì¶œë¡œ ëŒ€ì²´ ê°€ëŠ¥ ì½”ë“œë¥¼ ë…¼ë¦¬ì ì¸ ë‹¨ìœ„ë¡œ êµ¬ë¶„í•˜ì—¬ ì •ë¦¬í•´ë†“ì„ìˆ˜ ìˆìŒ ìº¡ìŠí™” : ì½”ë“œì˜ ì„¸ë¶€ì ì¸ ë‚´ìš©ì„ ëª¨ë¥´ê³  ì¸í„°í˜ì´ìŠ¤ë§Œ ì•Œì•„ë„ íƒ€ì¸ì˜ ì½”ë“œë¥¼ ì‚¬ìš©ê°€ëŠ¥ 12345678def calculate_rectangle_area(x,y): result = x * y return resultrectangle_x = 10rectangle_y = 20 print(calculate_rectangle_area(rectangle_x,rectangle_y)) ë©”ì¸ í”„ë¡œê·¸ë¨ ìˆ˜í–‰ í•¨ìˆ˜í˜¸ì¶œ í•¨ìˆ˜ìˆ˜í–‰ &amp; return ë‹¤ì‹œ ë©”ì¸ í”„ë¡œê·¸ë¨ ìˆ˜í–‰ return ê°’ê³¼ parameterì— ëŒ€í•œ ê°œë…ì„ ë‹¤ì‹œ ìƒê¸° Console in/outì–´ë–»ê²Œ í”„ë¡œê·¸ë¨ê³¼ ë°ì´í„°ë¥¼ ì£¼ê³  ë°›ì„ ê²ƒì¸ê°€?? Print Formatting %string format í•¨ìˆ˜ fstring 1234print('%s %s' %('one','two'))print('{} {}'.format('one','two'))print('%d %d' %(1,2))print('{} {}'.format(1,2)) 1. %-format 1print(&quot;product : %s, price : %f&quot; %('apple',5.243)) %10s : 10ì¹¸ %10.1f : 10ì¹¸ì— ì†Œìˆ«ì  1ìë¦¬ê¹Œì§€ ì¶œë ¥ Conditional &amp; Loopì¡°ê±´ë¬¸,ë°˜ë³µë¬¸ - ëŒ€ë¶€ë¶„ ìƒëµis ì—°ì‚°ì€ memoryì˜ ì£¼ì†Œë¥¼ ë¹„êµ!!! ê°’ì˜ ë¹„êµì¸ ==ì™€ ì£¼ì†Œì˜ ë¹„êµì¸ isëŠ” ë‹¤ë¥¸ ì—°ì‚°ì„ -5ë¶€í„° 256ê¹Œì§€ëŠ” íŒŒì´ì¬ì•ˆì— ì •ì  memoryë¡œ ì €ì¥ë˜ì–´ìˆìŒ 12345a = -1b = -1print(a is b) ê²°ê³¼ëŠ” True but when 12345a = -6b = -6print(a is b) ê²°ê³¼ëŠ” False all() - ëª¨ë‘ trueë©´ true ì•„ë‹ˆë©´ false any() - í•˜ë‚˜ë¼ë„ trueë©´ true ì•„ë‹ˆë©´ false ì‚¼í•­ ì—°ì‚°ì 12value = 12is_even = True if value%2 == 0 else False 1if __name__ == &quot;__main__&quot;: String and advanced function conceptStringstring íŠ¹ì§• stringì€ 1byteì˜ í¬ê¸°ë¡œ í•œê¸€ìì”© memoryì— í• ë‹¹ë¨ ë¬¸ìì—´ ë˜í•œ 2ì§„ìˆ˜ë¡œ ì»´í“¨í„°ê°€ ê·œì¹™ì— ì˜í•´ ë³€í™˜í•´ì„œ ì €ì¥ ë¬¸ìì—´ì€ listì™€ ê°™ì€ í˜•íƒœë¡œ dataë¥¼ ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸ì— slicing, indexingê³¼ ê°™ì€ íŠ¹ì§•ë“¤ì„ ë˜‘ê°™ì´ ê°€ì§ ë¬¸ìì—´ í•¨ìˆ˜ len(a) : ë¬¸ìì—´ ê¸¸ì´ë¥¼ return a.upper() : ëŒ€ë¬¸ìë¡œ ë³€í™˜ a.lower() : ì†Œë¬¸ìë¡œ ë³€í™˜ a.capitalize() : ì²«ë¬¸ìë¥¼ ëŒ€ë¬¸ìë¡œ ë³€í™˜ a.titile() : ì œëª©í˜•íƒœë¡œ (ì²«ë²ˆì§¸ ë„ì–´ì“°ê¸°, ì²«ë¬¸ì ëŒ€ë¬¸ì)ë¡œ ë³€í™˜ a.count(â€˜abcâ€™) : â€˜abcâ€™ê°€ ë“¤ì–´ê°„ íšŸìˆ˜ return a.find(â€˜abcâ€™) : â€˜abcâ€™ê°€ ë“¤ì–´ê°„ offset return a.rfind(â€˜abcâ€™) : same as find a.startswith(â€˜abcâ€™) : â€˜abcâ€™ë¡œ ì‹œì‘í•˜ëŠ”ì§€ ì—¬ë¶€ return a.endswith(â€˜abcâ€™) : â€˜abcâ€™ë¡œ ëë‚˜ëŠ”ì§€ ì—¬ë¶€ return 1234print(&quot;It\\'s OK&quot;)a = &quot;&quot;&quot;Happy new year&quot;&quot;&quot; {: .align-center} Function 2Call by object reference Call by value í•¨ìˆ˜ì˜ ì¸ìë¥¼ ë„˜ê¸¸ë•Œ ê°’ì„ ë„˜ê¹€ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì¸ìê°’ ë³€ê²½ì‹œ í˜¸ì¶œìì— ì˜í–¥ì„ ì•ˆì¤Œ Call by reference like Cì—ì„œì˜ pointer ì‚¬ìš© swapêµ¬í˜„í• ë•Œ call by valueê°€ ì•„ë‹Œ call by refernceë¡œ Call by object reference íŒŒì´ì¬ì€ ê°ì²´ì˜ ì£¼ì†Œê°€ í•¨ìˆ˜ë¡œ ì „ë‹¬ë˜ëŠ” ë°©ì‹ ì´ê±´ ì¢€ íŠ¹ì´í•œ ê²½ìš° í•¨ìˆ˜ì— ê°ì²´ê°€ ë„˜ì–´ê°”ì„ë•Œ ì£¼ì†Œê°€ ë„˜ì–´ê°€ê³  ê·¸ í•¨ìˆ˜ ì•ˆì—ì„œ ìƒˆë¡œìš´ ê°ì²´ ìƒì„±ë˜í•œ ê°€ëŠ¥ 123456def spam(eggs): eggs.append(1)# ê¸°ì¡´ê°ì²´ì˜ì£¼ì†Œê°’ì—[1] ì¶”ê°€ eggs =[2,3]# ìƒˆë¡œìš´ê°ì²´ìƒì„± ham =[0] spam(ham) print(ham)# [0, 1] Scoping Rule ë³€ìˆ˜ê°€ ì‚¬ìš©ë˜ëŠ” ë²”ìœ„ ì§€ì—­ë³€ìˆ˜ (local variable) ì „ì—­ë³€ìˆ˜ (global variable) 123456789def test(t): print(x) t =20 print(&quot;In Function :&quot;,t)x =10test(x)print(t)# tëŠ” í•¨ìˆ˜ì•ˆì—ì„œ ì‚¬ìš©í•˜ëŠ” ì§€ì—­ë³€ìˆ˜ì´ê¸° ë•Œë¬¸ì— í•¨ìˆ˜ ë°–ì—ì„œëŠ” ì‚¬ìš©ë¶ˆê°€ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì „ì—­ë³€ìˆ˜ ì‚¬ìš©ì‹œ global í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ë©´ í•¨ìˆ˜ë‚´ë¶€ì—ì„œë„ global variable ìˆ˜ì •, ì ‘ê·¼ ê°€ëŠ¥ Recursive Function ìê¸°ìì‹ ì„ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ê°™ì€ ì í™”ì‹ì„ í‘œí˜„í• ë•Œ ì‚¬ìš©í•œë‹¤. ì¬ê·€í•¨ìˆ˜ëŠ” ì¢…ë£Œì¡°ê±´ì´ ìˆì–´ ì¢…ë£Œì¡°ê±´ ì „ê¹Œì§€ ë°˜ë³µ ìˆ˜í–‰ Function Type Hint íŒŒì´ì¬ì˜ ê°€ì¥ í° íŠ¹ì§• : dynamic typing -&gt; ì²˜ìŒ ì‚¬ìš©í•˜ëŠ” ì‚¬ìš©ìê°€ interfaceë¥¼ ì•Œê¸°í˜ë“  ë‹¨ì ì´ ì¡´ì¬í•œë‹¤ 12def type_hint_example(name:str)-&gt;str: return f&quot;Hello, {name}&quot; DocstringíŒŒì´ì¬ í•¨ìˆ˜ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ë¥¼ ê¸°ì… 1234def add(a,b): ''' ì—¬ê¸°ì— ìƒì„¸ ì •ë³´ë¥¼ ê¸°ì… ''' vscodeì—ì„œ python docstring generatorë¥¼ ê¹”ë©´ ì‰½ê³  ëª…í™•í•˜ê²Œ docstring ì‘ì„± ê°€ëŠ¥ Coding Conventionëª…í™•í•œ ê·œì¹™ì€ ì—†ì§€ë§Œ íŒ€ë§ˆë‹¤ ì½”ë“œë¥¼ ì˜ ì´í•´í•˜ê¸° ìœ„í•´ì„œ ì„œë¡œì„œë¡œ ê°€ë…ì„±ì´ ì¢‹ê²Œ ì‘ì„±í•´ì•¼í•œë‹¤ ë“¤ì—¬ì“°ê¸°ëŠ” 4space ë‚˜ tapì¤‘ 1ê°œë¡œ í†µí•©í•˜ì—¬ ì‚¬ìš©í•˜ì ë˜í•œ 1ì¤„ì€ 79ìë¥¼ ë„˜ì–´ê°€ë©´ ì•ˆëœë‹¤ ë¶ˆí•„ìš”í•œ ê³µë°±ì€ í”¼í•˜ì ì—°ì‚°ìëŠ” 1ì¹¸ì”©ë§Œ ë„ìš°ì ì†Œë¬¸ì l, ëŒ€ë¬¸ì O, ëŒ€ë¬¸ì I ê¸ˆì§€ flack8ì´ë¼ëŠ” ëª¨ë“ˆë¡œ ì²´í¬ ê°€ëŠ¥","link":"/2021/01/19/2021-01-19-Boostcamp2/"},{"title":"Day1","text":"ë¶€ìŠ¤íŠ¸ ìº í”„ì˜ ì²«ë‚ ì´ ì‹œì‘ë˜ì—ˆë‹¤. ì²«ë‚ ì´ë¼ ì•ìœ¼ë¡œì˜ ê³„íšì´ë‚˜ ê°œë°œí™˜ê²½ settingì´ ì£¼ë¥¼ ì´ë£¬ ìˆ˜ì—…ì´ì˜€ë‹¤. ì˜¤ëŠ˜ì´ ì•„ë§ˆ ì¼ì •ì´ ë„ëŸ´í•œ ë§ˆì§€ë§‰ë‚ ì´ ë ê²ƒê°™ë‹¤ã…ã…ğŸ˜µğŸ˜µğŸ˜µ Hidden classíŒŒì¼ì‹œìŠ¤í…œOSì—ì„œ íŒŒì¼ì„ ì €ì¥í•˜ëŠ” íŠ¸ë¦¬êµ¬ì¡°ì˜ ì €ì¥ ì²´ê³„ Directory í´ë” ë˜ëŠ” ë””ë ‰í† ë¦¬ë¡œ ë¶ˆë¦¼ íŒŒì¼ê³¼ ë‹¤ë¥¸ë””ë ‰í† ë¦¬ í¬í•¨ ê°€ëŠ¥ íŒŒì¼ ì»´í“¨í„°ê°€ ë…¼ë¦¬êµ¬ì¡°ë¥¼ ì €ì¥í•˜ëŠ” ë‹¨ìœ„ ì½ê¸°, ì“°ê¸°, ì‹¤í–‰ë“±ì˜ ì‘ì—… ê°€ëŠ¥ ì ˆëŒ€ê²½ë¡œ vs ìƒëŒ€ê²½ë¡œê²½ë¡œ - ì»´í“¨í„° íŒŒì¼ì˜ ê³ ìœ í•œ ìœ„ì¹˜, íŠ¸ë¦¬êµ¬ì¡°ìƒ ë…¸ë“œ ì ˆëŒ€ê²½ë¡œ : ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¶€í„° íŒŒì¼ìœ„ì¹˜ê¹Œì§€ì˜ ê²½ë¡œ ìƒëŒ€ê²½ë¡œ : í˜„ì¬ìˆëŠ” ë””ë ‰í† ë¦¬ë¶€í„° íƒ€ê²ŸíŒŒì¼ê¹Œì§€ì˜ ê²½ë¡œ í„°ë¯¸ë„ë§ˆìš°ìŠ¤ê°€ ì•„ë‹Œ í‚¤ë³´ë“œë¡œ ëª…ë ¹ì„ ì…ë ¥ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ë§ˆìš°ìŠ¤ë¡œ ì…ë ¥ : GUI í™˜ê²½ í‚¤ë³´ë“œë¡œ ì…ë ¥ : CUI í™˜ê²½ Windows : CMD window, Windows terminal Mac, Linux : Terminal ê¸°ë³¸ ëª…ë ¹ì–´ cls : í™”ë©´ì„ clear cd : change directory mkdir : make directory dir : í•˜ìœ„ ë””ë ‰í† ë¦¬ ëª©ë¡ ì¶œë ¥ del : í•˜ë‚˜ì´ìƒì˜ íŒŒì¼ì„ ì§€ì›€ copy : copy a b : aë¥¼ bì— ë³µì‚¬í•´ë¼ íŒŒì´ì¬ ê°œìš”í”Œë«í¼ ë…ë¦½ì ì¸ ì¸í„°í”„ë¦¬í„° ì–¸ì–´ì´ë‹¤ ìš´ì˜ì²´ì œ ìƒê´€ì—†ì´ ì‘ë™ë˜ëŠ” ì–¸ì–´ì´ë‹¤ ëª¨ë“  í”„ë¡œê·¸ë¨ì€ OSì— ì˜ì¡´ì ì¸ë° ì†ŒìŠ¤ì½”ë“œë¥¼ ë°”ë¡œ ì‹¤í–‰í• ìˆ˜ìˆê²Œ ì§€ì›í•˜ëŠ” í”„ë¡œê·¸ë¨ ì‹¤í–‰ë°©ë²• ì¸í„°í”„ë¦¬í„° vs ì»´íŒŒì¼ëŸ¬ì»´íŒŒì¼ëŸ¬ ì†ŒìŠ¤ì½”ë“œ ì‹¤í–‰ì „ì— ì»´íŒŒì¼ëŸ¬ê°€ ê¸°ê³„ì–´ë¡œ ë¨¼ì €ë²ˆì—­ (C,ìë°”,C++) ì¸í„°í”„ë¦¬í„° ë³„ë„ì˜ ë²ˆì—­ê³¼ì •ì—†ì´ ì†ŒìŠ¤ì½”ë“œë¥¼ ì‹¤í–‰ì‹œì ì— í•´ì„ (íŒŒì´ì¬,ìŠ¤ì¹¼ë¼) ê°ì²´ì§€í–¥ì˜ ë™ì  íƒ€ì´í•‘ ì–¸ì–´ê°ì²´ì§€í–¥ ì‹¤í–‰ìˆœì„œê°€ ì•„ë‹Œ ëª¨ë“ˆì¤‘ì‹¬ìœ¼ë¡œ í”„ë¡œê·¸ë¨ì„ ì‘ì„± ë™ì íƒ€ì´í•‘ì–¸ì–´ ì‹¤í–‰ì‹œì ì— ë°ì´í„°ì—ëŒ€í•œ íƒ€ì…ì„ ê²°ì •í•¨ ì´ê²Œ ë§¤ìš° í¸í•œê²ƒ ê°™ë‹¤. ìë£Œí˜•ì´ ì‹¤ìˆ˜ì¸ì§€ ë¬¸ìì—´ì¸ì§€ ë”°ë¡œ ì§€ì •ì„ í•˜ì§€ ì•Šì•„ì¤˜ë„ ë˜ëŠ” íŒŒì´ì¬ì˜ íŠ¹ì§•ì´ë‹¤ ì´ê²Œ ë””ë²„ê¹… ê³¼ì •ì—ì„œ ë‹¤ë¥¸ ì •ì íƒ€ì´í•‘ì–¸ì–´ì—ì„œë³´ë‹¤ íš¨ìœ¨ì ì´ë‹¤. ì²«ë‚ ì´ë¼ ì •ë¦¬í• ê²Œ ë§ì§€ ì•Šì•˜ë‹¤. ë‚´ì¼ë¶€í„° ë‹¤ìŒì£¼ì—ìˆëŠ” ì •ê¸°ì„¸ì…˜ ë°œí‘œì¤€ë¹„ì™€ ë³‘í–‰í•´ì•¼ í•´ì„œ ì¢€ ë°”ë¹ ì§ˆê²ƒ ê°™ë‹¤ :fire:","link":"/2021/01/18/2021-01-18-Boostcamp1/"},{"title":"Object-Oriented Programming","text":"Python Object-Oriented Programmingë§Œë“¤ì–´ ë†“ì€ ì½”ë“œë¥¼ ì¬ì‚¬ìš© í•˜ê³ ì‹¶ë‹¤ ìƒê°í•´ë³´ê¸° ì£¼ì²´ë“¤ì„ ë§Œë“¤ê³  ì£¼ì²´ë“¤ì˜ í–‰ë™ë“¤ ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ë‚˜ëˆ„ì–´ì„œ ì½”ë”© ê°œìš”ê°ì²´ : ì†ì„±ê³¼ í–‰ë™ì„ ê°€ì§€ëŠ” ì¼ì¢…ì˜ ë¬¼ê±´ ì†ì„±ì€ ë³€ìˆ˜ë¡œ, í–‰ë™ì€ í•¨ìˆ˜ë¡œ í‘œí˜„ oopëŠ” ì„¤ê³„ë„ (í‹€)ì— í•´ë‹¹ë˜ëŠ” Classì™€ ì‹¤ì œ êµ¬í˜„ì²´ì¸ Instanceë¡œ ë‚˜ë‰¨ Class êµ¬í˜„ì•Œì•„ë‘ë©´ ì¢‹ì€ Python naming rule íŒŒì´ì¬ì˜ í•¨ìˆ˜ì™€ ë³€ìˆ˜ëª…ì—ì„œëŠ” _ë¡œ ë„ì–´ì“°ê¸°ë¥¼ êµ¬ë¶„í•˜ëŠ” snake case ë¥¼ ì‚¬ìš© íŒŒì´ì¬ì˜ Class ëª…ì—ëŠ” ë„ì–´ì“°ê¸° ë¶€ë¶„ì— ëŒ€ë¬¸ìë¥¼ ì‚¬ìš©í•˜ëŠ” camel case ë¥¼ ì‚¬ìš© Attribute ì¶”ê°€í•˜ê¸° Attribute ì¶”ê°€ëŠ” __ init __ê³¼ í•¨ê»˜ initì€ ê°ì²´ ì´ˆê¸°í™” í•¨ìˆ˜ íŒŒì´ì¬ì—ì„œ __ëŠ” íŠ¹ìˆ˜í•œ ì˜ˆì•½í•¨ìˆ˜ë‚˜ ë³€ìˆ˜, í•¨ìˆ˜ëª… ë³€ê²½(ë§¹ê¸€ë§)ìœ¼ë¡œ ì‚¬ìš© ex) __ main __ , __ str __ , __ init __ Method êµ¬í˜„í•˜ê¸° ë°˜ë“œì‹œ selfë¥¼ ì¶”ê°€í•´ì•¼ì§€ë§Œ class í•¨ìˆ˜ë¡œ ì¸ì •ì´ ë¨ self : Instance ìì‹ ì„ ì˜ë¯¸í•¨ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ì‹œ ê·¸ ì¸ìŠ¤í„´ìŠ¤ ë‚´ë¶€ì—ì„œëŠ” selfë¡œ ë¶ˆë¦¼ ex)123456789101112131415161718192021222324class SoccerPlayer(object): def __init__(self, name, position, back_number): self.name = name self.position = position self.back_number = back_number def __str__(self): return &quot;Hello, My name is %s. I play in %s in center &quot; % \\ (self.name, self.position) def __add__(self, other): return self.name + other.name def change_backnumber(self, new_number): print(&quot;ì„ ìˆ˜ì˜ ë“±ë²ˆí˜¸ë¥¼ ë³€ê²½í•©ë‹ˆë‹¤ From : %d To : %d&quot; %(self.back_number,new_number)) a = SoccerPlayer('son','FW',7)b = SoccerPlayer('kain','FW',14)print(a)print(a+b)a.change_backnumber(9)# Hello, My name is son. I play in FW in center # sonkain# ì„ ìˆ˜ì˜ ë“±ë²ˆí˜¸ë¥¼ ë³€ê²½í•©ë‹ˆë‹¤ From : 7 To : 9 ìƒì†(Inheritance) ë¶€ëª¨ í´ë˜ìŠ¤ë¡œ ë¶€í„° ì†ì„±ê³¼ Methodë¥¼ ë¬¼ë ¤ë°›ì€ ìì‹ í´ë˜ìŠ¤ë¥¼ ìƒì„± í•˜ëŠ” ê²ƒ ë‹¤ë¥¸ í´ë˜ìŠ¤ì˜ objectì— ë¶€ëª¨í´ë˜ìŠ¤ì˜ ì´ë¦„ì„ ë„£ì–´ì£¼ë©´ ìƒì†ì´ë¨ super() : ê·¸ê²ƒì˜ ë¶€ëª¨í´ë˜ìŠ¤ë¥¼ ê·¸ëŒ€ë¡œ ë°›ì•„ì™€ ë¶€ëª¨ê°ì²´ë¥¼ ì‚¬ìš©í• ë•Œ ì‚¬ìš© super().about_me() : ë¶€ëª¨í´ë˜ìŠ¤ì˜ method ì‚¬ìš© Polymorphism ê°™ì€ ì´ë¦„ì˜ methodì˜ ë‚´ë¶€ ë¡œì§ì„ ë‹¤ë¥´ê²Œ ì‘ì„± Dynamic 123456789101112131415161718192021class Animal: def __init__(self, name): # Constructor of the class self.name = name def talk(self): # Abstract method, defined by convention only raise NotImplementedError(&quot;Subclass must implement abstract method&quot;) class Cat(Animal): def talk(self): return 'Meow!' class Dog(Animal): def talk(self): return 'Woof! Woof!' animals = [Cat('Missy'),Cat('Mr. Mistoffelees'),Dog('Lassie')]for animal in animals:print(animal.name + ': ' + animal.talk())#êµ³ì´ catê³¼ dogì˜ í•¨ìˆ˜ì´ë¦„ì„ ë‹¤ë¥´ê²Œ í•  í•„ìš”ì—†ì´ ë‚´ìš©ë§Œ ë‹¤ë¥´ê²Œ í•¨ ê°€ì‹œì„± ê°ì²´ì˜ ì •ë³´ë¥¼ ë³¼ ìˆ˜ ìˆëŠ” ë ˆë²¨ì„ ì¡°ì •í•˜ëŠ” ê²ƒ ëˆ„êµ¬ë‚˜ ê°ì²´ ì•ˆì˜ ëª¨ë“  ë³€ìˆ˜ë¥¼ ë³¼ í•„ìš”ê°€ ì—†ìŒ @ ìº¡ìŠí™” í´ë˜ìŠ¤ë¥¼ ì„¤ê³„ì‹œ ì •ë³´ì€ë‹‰, í´ë˜ìŠ¤ê°„ ê°„ì„­/ê³µìœ  ìµœì†Œí™” Product ê°ì²´ë¥¼ Inventory ê°ì²´ì— ì¶”ê°€ Inventoryì—ëŠ” ì˜¤ì§ Product ê°ì²´ë§Œ ë“¤ì–´ê° Inventoryì— Productê°€ ëª‡ ê°œì¸ì§€ í™•ì¸ì´ í•„ìš” Inventoryì— Product items ì ‘ê·¼ í—ˆìš© private ë³€ìˆ˜ë¡œ ì„ ì–¸í•˜ëŠ”ë²• : self.__items = [] private ë³€ìˆ˜ë¡œ ì„ ì–¸ì‹œ ì™¸ë¶€ì—ì„œ ì ‘ê·¼ì´ ë¶ˆê°€ëŠ¥í•¨ 1234567891011121314151617181920class Product(object): passclass Inventory(object): def __init__(self): self.__items = [] def add_new_item(self, product): if type(product) == Product: self.__items.append(product) print(&quot;new item added&quot;) else: raise ValueError(&quot;Invalid Item&quot;) def get_number_of_items(self): return len(self.__items)my_inventory = Inventory()my_inventory.add_new_item(Product())print(my_inventory.__items)#ì ‘ê·¼ì´ ì•ˆë˜ê³  ì—ëŸ¬ê°€ ëœ¬ë‹¤#ë§Œì•½ ì¨ì•¼ëœë‹¤ë©´? ë§Œì•½ ì™¸ë¶€ì—ì„œ Private ë³€ìˆ˜ë¥¼ ì¨ì•¼í•˜ëŠ” ìƒí™©ì´ ì¡´ì¬í•œë‹¤ë©´? -&gt; property decoratorë¥¼ ì‚¬ìš© ë‚´ë¶€ì—ì„œ ì ‘ê·¼í•´ì„œ ë°˜í™˜í•´ì£¼ëŠ” ê¸°ëŠ¥ì„ í•´ì¤Œ 123456789101112131415161718192021222324class Product(object): passclass Inventory(object): def __init__(self): self.__items = [] def add_new_item(self, product): if type(product) == Product: self.__items.append(product) print(&quot;new item added&quot;) else: raise ValueError(&quot;Invalid Item&quot;) def get_number_of_items(self): return len(self.__items) @property def items(self): # ë³´í†µ ì•„ë˜ì²˜ëŸ¼ ì™¸ë¶€ì—ì„œ ìˆ˜ì •ì´ ê°€ëŠ¥í•˜ê²Œ ë°˜í™˜í•˜ì§€ ì•Šê³  copyë³¸ì„ returní•´ì¤Œ return self.__items my_inventory = Inventory()my_inventory.add_new_item(Product())print(my_inventory.items)#ì´ëŸ¼ ì ‘ê·¼ì´ ë¨#decoratorì€ í•¨ìˆ˜ëª…ì„ ë³€ìˆ˜ëª…ì²˜ëŸ¼ ì“¸ìˆ˜ìˆê²Œ í•´ì£¼ëŠ”!! Decorateì´í•´í•˜ê¸° ìœ„í•œ ê°œë…ë“¤ First-class objects Inner function decorator First-Class objectsì¼ë“± í•¨ìˆ˜, ì¼ê¸‰ ê°ì²´ ë³€ìˆ˜ë‚˜ ë°ì´í„° êµ¬ì¡°ì— í• ë‹¹ì´ ê°€ëŠ¥í•œ ê°ì²´ íŒŒì´ì¬ì˜ ëª¨ë“  í•¨ìˆ˜ëŠ” 1ê¸‰í•¨ìˆ˜ì´ë‹¤ íŒŒì´ì¬ì˜ ëª¨ë“ í•¨ìˆ˜ëŠ” íŒŒë¼ë©”í„°ë¡œ ì „ë‹¬ê°€ëŠ¥ 12345678def square(x): return x*xf = squaref(5)#ì´ê±´ ë§¤ìš° íŠ¹ì´í•˜ê³  í‰ì†Œì— ìƒê°í•˜ì§€ ì•Šì•˜ë˜ ê²ƒì´ë‹¤def formula(method, arg_list): return([method(value) for value in arg_list]) ì´ë ‡ê²Œ í•¨ìˆ˜ë„ ë³€ìˆ˜ì²˜ëŸ¼ ì§€ì •í•´ì„œ ì‚¬ìš©í• ìˆ˜ë„ ìˆê³  í•¨ìˆ˜ì˜ íŒŒë¼ë©”í„°ë¡œ ë„£ì„ìˆ˜ë„ ìˆêµ¬ë§Œ :scream: Inner Function í•¨ìˆ˜ë‚´ì— ë˜ë‹¤ë¥¸ í•¨ìˆ˜ì˜ ì¡´ì¬ 123456789101112def print_msg(msg): def printer(): print(msg) printer() #closuredef print_msg(msg): def printer(): print(msg) return printer ë§¤ìš° í”í•œ êµ¬ì¡°ì„ Closure : inner functionì„ return ê°’ìœ¼ë¡œ ë°˜í™˜ 1234567891011def star(func): def inner(*args, **kwargs): print(&quot;*&quot; * 30) func(*args, **kwargs) print(&quot;*&quot; * 30) return inner@stardef printer(msg): print(msg)printer(&quot;Hello&quot;) DecoratorModule and Project íŒŒì´ì¬ì€ ëŒ€ë¶€ë¶„ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì´ë¯¸ ë‹¤ë¥¸ì‚¬ìš©ìê°€ ë‹¤ êµ¬í˜„í•´ ë†“ìŒ ì´ê±¸ ë¶ˆëŸ¬ì™€ì„œ ì“°ëŠ”ê²Œ êµ‰ì¥íˆ í° ì¥ì  ë‚¨ì´ ë§Œë“  í”„ë¡œê·¸ë¨ ì“°ëŠ” ë²• ê°ì²´ &lt; ëª¨ë“ˆ ModuleOverview ì–´ë–¤ ëŒ€ìƒì˜ ë¶€ë¶„ í˜¹ì€ ì¡°ê° ex)ë ˆê³  ë¸”ë¡ ê°™ì€ê²ƒë“¤ í”„ë¡œê·¸ë¨ì„ ë‚˜ëˆˆ ì‘ì€ í”„ë¡œê·¸ë¨ ì¡°ê°ë“¤ Package : moduleì˜ ëª¨ìŒ íŒŒì´ì¬ì—ì„œì˜ module == pyíŒŒì¼ import ~~~ í•´ì£¼ë©´ ë¨ ê·¸ëŸ¼ ê·¸ì•ˆì— ìˆëŠ” ëª¨ë“  ì½”ë“œê°€ ë©”ëª¨ë¦¬ ë¡œë”©ì´ ì¼ì–´ë‚¨ ê°™ì€ directoryì•ˆì—ìˆì–´ì•¼ë¨ ì½”ë“œë¥¼ ì‰½ê²Œ ë¡œë”©í• ìˆ˜ ìˆê²Œ pycacheë¼ëŠ” í´ë”ê°€ ìƒê¹€ name space ëª¨ë“ˆí˜¸ì¶œì‹œ ë²”ìœ„ë¥¼ ì§€ì •í•˜ëŠ” ë°©ë²• ëª¨ë“ˆì•ˆì—ëŠ” í•¨ìˆ˜ì™€ í´ë˜ìŠ¤ë“±ì´ ì¡´ì¬ê°€ëŠ¥ ì¼ë¶€ë§Œ í˜¸ì¶œí•´ì£¼ê¸° ìœ„í•´ From ê³¼ importë¥¼ ì”€ ëª¨ë“ˆì—ì„œ íŠ¹ì •í•¨ìˆ˜ë‚˜ í´ë˜ìŠ¤ë¥¼ í˜¸ì¶œí•˜ê¸° ë˜ë„ë¡ì´ë©´ íŠ¹ì • í•¨ìˆ˜ê°€ ì–´ë””ì„œ ì™”ëŠ”ì§€ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ëª¨ë“ˆëª…ì„ ë³„ì¹­ìœ¼ë¡œ ì“°ëŠ” ë°©ë²•ì„ ì“°ì ë¼ê³  ìƒê° Package ë‹¤ì–‘í•œ ëª¨ë“ˆë“¤ì˜ í•© ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ë“¤ì´ íŒ¨í‚¤ì§€ë¡œ ê´€ë¦¬ë¨ í´ë”ë³„ë¡œ __init.py êµ¬ì„±í•˜ê¸° í˜„ì¬ í´ë”ê°€ íŒ¨í‚¤ì§€ì„ì„ ì•Œë ¤ì£¼ëŠ” ì´ˆê¸°í™” íŒŒì¼ ì—†ìœ¼ë©´ packageê°€ ì•„ë‹˜ í•˜ìœ„ì´ë”ì™€ pyíŒŒì¼ì„ ëª¨ë‘í¬í•¨í•¨ import ì™€ all keywordì‚¬ìš© ì°¸ê³  : package namespace packageë‚´ì—ì„œ ë‹¤ë¥¸í´ë”ì—ìˆëŠ” ëª¨ë“ˆì„ ë¶€ë¥¼ë–„ ìƒëŒ€ ì°¸ì¡°ë¡œ í˜¸ì¶œ from ..sound.echo [.. ì ì´ 2ê°œ : ë¶€ëª¨ë””ë ‰í† ë¦¬ê¸°ì¤€] from game.graphic.render -&gt; ì´ê±´ ì ˆëŒ€ì°¸ì¡° from . render -&gt; í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€ ì˜¤ëŠ˜ ë°°ì› ë˜ê²Œ ì •ë§ ì¤‘ìš”í–ˆë˜ê²ƒ ê°™ë‹¤. ë‹¤ë¥¸ ë¬¸ë²•ë“¤ë³´ë‹¤ ì´ packageì˜ êµ¬ì„±ê°™ì€ê²Œ í‰ì†Œì— êµ‰ì¥íˆ ëª¨í˜¸í–ˆëŠ”ë° ì´ë²ˆì— í™•ì‹¤í•˜ê²Œ ì¡ê³  ë„˜ì–´ ê°€ì•¼ì§€.","link":"/2021/01/21/2021-01-21-Boostcamp4/"},{"title":"Python handling","text":"Exception &amp; File &amp;Log HandlingException Handling í”„ë¡œê·¸ë¨ ì‚¬ìš©ì‹œì—ëŠ” ì˜ˆìƒì¹˜ëª»í•œ ì˜ˆì™¸ê°€ ìƒê¹€ ì˜ˆìƒì´ ê°€ëŠ¥í•œ ì˜ˆì™¸ ë°œìƒì—¬ë¶€ë¥¼ ì‚¬ì „ì— ì¸ì§€í•˜ì—¬ ê°œë°œìê°€ ëª…ì‹œì ìœ¼ë¡œ ì²˜ë¦¬í•´ì£¼ì–´ì•¼ í•¨ ì˜ˆìƒì´ ë¶ˆê°€ëŠ¥í•œ ì˜ˆì™¸ ì¸í„°í”„ë¦¬í„° ê³¼ì •ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì˜ˆì™¸ ìˆ˜í–‰ë¶ˆê°€ì‹œ ì¸í„°í”„ë¦¬í„°ê°€ ìë™í˜¸ì¶œ ì˜ˆì™¸ì²˜ë¦¬â€”&gt; Exception Handling Try except ë¬¸ë²•ex) 0ìœ¼ë¡œ ë‚˜ëˆŒë–„ exception ë°œìƒ 123456789101112131415161718192021222324252627282930try: ì˜ˆì™¸ë°œìƒ ì½”ë“œexcept &lt;type&gt;: ì˜ˆì™¸ë°œìƒì‹œ ëŒ€ì²˜í•˜ëŠ” ì½”ë“œ a = [1,2,3,4,5]for i in range(10): try: print(10 / i) print(a[i]) except ZeroDivisionError: print(&quot;Not divided by 0&quot;) except IndexError as e: print(e) else: print('hey') finally: print('wow')''' Not divided by 010.05.03.33333333333333352.52.01.66666666666666671.42857142857142861.251.1111111111111112''' ë˜ë„ë¡ì´ë©´ exceptionì„ ëª…í™•í•˜ê²Œ ì¡ì•„ ê°€ë…ì„±ì„ ë†’í˜€ë¼ else êµ¬ë¬¸ì„ ì‚¬ìš©í• ì‹œ ì˜ˆì™¸ê°€ ë°œìƒí•˜ì§€ ì•Šì„ì‹œ ì‹¤í–‰ finally ì˜ˆì™¸ ë°œìƒì—¬ë¶€ì™€ ìƒê´€ì—†ê²Œ í•­ìƒ ì‹¤í–‰ë˜ëŠ” êµ¬ë¬¸ì„ ì¶”ê°€ Raise êµ¬ë¬¸ í•„ìš”ì— ë”°ë¼ ê°•ì œë¡œ exceptionì„ ë°œìƒ 123456while True: value = input(&quot;ë³€í™˜í•  ì •ìˆ˜ ê°’ì„ ì…ë ¥í•´ì£¼ì„¸ìš”&quot;) for digit in value: if digit not in &quot;0123456789&quot;: raise ValueError(&quot;ìˆ«ìê°’ì„ ì…ë ¥í•˜ì§€ ì•Šìœ¼ì…¨ìŠµë‹ˆë‹¤&quot;) print(&quot;ì •ìˆ˜ê°’ìœ¼ë¡œ ë³€í™˜ëœ ìˆ«ì -&quot;, int(value)) Assert êµ¬ë¬¸ì‚¬ì „ì— ì‚¬ìš©ìì—ê²Œ íŠ¹ì •ì¡°ê±´ì´ ë§ëŠ”ì§€ ì¡°ê±´ì„ ë„£ì–´ì£¼ê³  Falseì¼ë•Œ errorë¥¼ ë°œìƒì‹œì¼œì¤Œ assert isinstance(decimal_number, int) File HandlingíŒŒì¼ì˜ ì¢…ë¥˜ ê¸°ë³¸ì ì¸ íŒŒì¼ì¢…ë¥˜ë¡œ text file, binary fileë¡œ ë‚˜ë‰¨ ì»´í“¨í„°ëŠ” text fileì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ binaryë¡œ ë³€í™˜ì‹œí‚´ Binary file ì´ì§„ë²•ì„ë¡œ ë‚˜íƒ€ë‚´ì ¸ ìˆìŒ ë©”ëª¨ì¥ìœ¼ë¡œ ì—´ë©´ ê¹¨ì§ ì—‘ì…€íŒŒì¼ ì›Œë“œíŒŒì¼ ë“±ë“± Text file ì¸ê°„ë„ ì´í•´ê°€ëŠ¥í•œ ë¬¸ìì—´ë¡œ ì´ë£¨ì–´ì§ ë©”ëª¨ì¥ìœ¼ë¡œ ì—´ë©´ í™•ì¸ ê°€ëŠ¥ Python File I/O123f = open(&quot;ê°™ì€ í´ë”ì˜ íŒŒì¼&quot;, &quot;r&quot;)contents = f.read() r : ì½ê¸° ëª¨ë“œ w : ì“°ê¸° ëª¨ë“œ a : ì¶”ê°€ ëª¨ë“œ with êµ¬ë¬¸ê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ê¸° ì¸ë´í…Œì´ì…˜ì´ ì¼ì–´ë‚˜ëŠ” ë™ì•ˆì—ëŠ” withì¤„ì˜ ì½”ë“œê°€ ë‹¤ ì ìš©ì´ë¨ ì¸ë´í…Œì´ì…˜ì´ ì¢…ë£Œê°€ ë˜ë©´ closeê°€ ë¨ ë¡œê·¸ë¥¼ ë‚¨ê²¨ì•¼í•  í•„ìš”ê°€ ìˆìŒ 1234f = open(&quot;i_have_a_dream.txt&quot;,&quot;r&quot;)contents = f.read()print(contents)f.close() Logging ëª¨ë“ˆprintë¬¸ì´ë‘ ë¹„ìŠ· Python data handling CSV- ì›¹(html) xml json CSV(Comma Separate Value) ì—‘ì…€ ì–‘ì‹ì˜ ë°ì´í„°ë¥¼ í”„ë¡œê·¸ë¨ì— ìƒê´€ì—†ì´ ì“°ê¸° ìœ„í•œ ë°ì´í„° í˜•ì‹ì´ë¼ê³  ìƒê°í•˜ë©´ ì‰¬ì›€ TSV,SSVë“±ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ ë§Œë“¤ê¸°ë„ í•¨ notepadë¡œë„ ì—´ ìˆ˜ ìˆê³ , ì‰¼í‘œë¡œ êµ¬ë¶„ì´ ë˜ì–´ìˆë‹¤. ìœ„ì—ì„œ data_headerë¼ëŠ”ê²Œ ìˆë‹¤. ì—¬ê¸°ì—ëŠ” ë°ì´í„°ì˜ í•„ë“œê°€ ë‹´ê²¨ìˆã…‡ë©° ë°ì´í„° ì €ì¥ì‹œ ,ë¡œ ë¶„ë¦¬ë¥¼ í•˜ëŠ” ì½”ë“œì´ë‹¤ ì²«ë²ˆì§¸ dataëŠ” ë¬´ì¡°ê±´ dataì˜ í•„ë“œì´ë‹¤ ex) data,indexì´ëŸ° ëŠë‚Œìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ì´ë‹¤ ë”°ë¼ì„œ ì²«ì¤„ì´ë¼ë©´ ,ë¡œ ë‚˜ëˆ„ì–´ì„œ data_headerë¼ëŠ” ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•´ì¤€ë‹¤ ì´í›„ ì¤„ë¶€í„°ëŠ” ,ë¡œ ë‚˜ëˆ„ì–´ì„œ í•œì¤„ì”© ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•´ì¤€ë‹¤ textíŒŒì¼ í˜•íƒœë¡œ ë°ì´í„° ì²˜ë¦¬ì‹œ ë¬¸ì¥ë‚´ì— ë“¤ì–´ê°€ìˆëŠ” â€œ,â€ ë“±ì— ëŒ€í•´ ì „ì²˜ë¦¬ ê³¼ì •ì´ í•„ìš”í•˜ë‹¤ íŒŒì´ì¬ì—ì„œëŠ” ê°„ë‹¨íˆ CSVíŒŒì¼ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ csvê°ì²´ë¥¼ ì œê³µí•¨ 12import csvreader = csv.reader(f, delimiter = ',', quotechar = '&quot;',quoting = csv.QUOTE_ALL) delimiter : ê¸€ìë¥¼ ë‚˜ëˆ„ëŠ” ê¸°ì¤€ (default : â€˜,â€™ ) lineterminator : ì¤‘ë°”ê¿ˆê¸°ì¤€ (default : \\r\\n) quotechar : ë¬¸ìì—´ì„ ë‘˜ëŸ¬ì‹¸ëŠ” ì‹ í˜¸ ë¬¸ì (default : â€œ ) quoting : ë°ì´í„°ë¥¼ ë‚˜ëˆ„ëŠ” ê¸°ì¤€ì´ quotecharì— ì˜í•´ ë‘˜ëŸ¬ì‹¸ì¸ ë¼ë²¨ 123456789101112131415161718192021222324import csvseoung_nam_data = []header = []rownum = 0with open(&quot;korea_floating_population_data.csv&quot;,&quot;r&quot;, encoding=&quot;cp949&quot;) as p_file: csv_data = csv.reader(p_file) #csv ê°ì²´ë¥¼ ì´ìš©í•´ì„œ csv_data ì½ê¸° for row in csv_data: #ì½ì–´ì˜¨ ë°ì´í„°ë¥¼ í•œ ì¤„ì”© ì²˜ë¦¬ if rownum == 0: header = row #ì²« ë²ˆì§¸ ì¤„ì€ ë°ì´í„° í•„ë“œë¡œ ë”°ë¡œ ì €ì¥ location = row[7] #â€œí–‰ì •êµ¬ì—­â€í•„ë“œ ë°ì´í„° ì¶”ì¶œ, í•œê¸€ ì²˜ë¦¬ë¡œ ìœ ë‹ˆì½”ë“œ ë°ì´í„°ë¥¼ cp949ë¡œ ë³€í™˜ if location.find(u&quot;ì„±ë‚¨ì‹œ&quot;) != -1: seoung_nam_data.append(row) #â€í–‰ì •êµ¬ì—­â€ ë°ì´í„°ì— ì„±ë‚¨ì‹œê°€ ë“¤ì–´ê°€ ìˆìœ¼ë©´ seoung_nam_data Listì— ì¶”ê°€ rownum +=1with open(&quot;seoung_nam_floating_population_data.csv&quot;,&quot;w&quot;, encoding=&quot;utf8&quot;) as s_p_file: writer = csv.writer(s_p_file, delimiter='\\t', quotechar=&quot;'&quot;, quoting=csv.QUOTE_ALL) # csv.writerë¥¼ ì‚¬ìš©í•´ì„œ csv íŒŒì¼ ë§Œë“¤ê¸° delimiter í•„ë“œ êµ¬ë¶„ì # quotecharëŠ” í•„ë“œ ê° ë°ì´í„°ëŠ” ë¬¶ëŠ” ë¬¸ì, quotingëŠ” ë¬¶ëŠ” ë²”ìœ„ writer.writerow(header) #ì œëª© í•„ë“œ íŒŒì¼ì— ì“°ê¸° for row in seoung_nam_data: writer.writerow(row) #seoung_nam_dataì— ìˆëŠ” ì •ë³´ listì— ì“°ê¸° ìœ„ëŠ” ìœ ë™ì¸êµ¬ ë°ì´í„°ì¤‘ ì„±ë‚¨ì˜ ë°ì´í„°ë§Œì„ ìˆ˜ì§‘í•˜ëŠ” ì½”ë“œì´ë‹¤ windowì—ì„œ ê´€ë¦¬ë˜ëŠ” ì½”ë“œëŠ” cp949ì´ë‹¤ vscodeëŠ” utf8ì´ê¸° ë–„ë¬¸ì— encodingì„ ë°”ê¾¸ì–´ ì£¼ì–´í– í•œë‹¤ ë”°ë¼ì„œ ì½ì„ ë•Œ cp949ë¡œ ì½ëŠ”ë‹¤ê³  ë³„ë„ë¡œ ì§€ì •í•œë‹¤ encodingì€ cp949, utf8ë¡œ ì™ ë§Œí•˜ë©´ ì €ì¥í•˜ê¸° ì™ ë§Œí•˜ë©´ ì‘ì€ â€˜ ì´ê±¸ë¡œ ë‚˜ëˆ„ê³  ë³´í†µ csvëŠ” ë‹¤ë¥¸ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì§€ê¸ˆì€ ì´ê±°ì— ì§‘ì°©í•  í•„ìš”ê°€ ì—†ë‹¤ ì™ ë§Œí•˜ë©´ pandasë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— Web ë°ì´í„° ì†¡ìˆ˜ì‹ ì„ ìœ„í•œ HTTP í”„ë¡œí† ì½œ ì‚¬ìš© ë°ì´í„° í‘œì‹œë¥¼ ìœ„í•œ HTML í˜•ì‹ ì‚¬ìš© HTML HTMLì˜ ëª¨ë“  ìš”ì†Œë“¤ì€ êº¾ì‡  ê´„í˜¸ì•ˆì— ë‘˜ëŸ¬ ìŒ“ì—¬ ìˆìŒ ëª¨ë“  HTMLì€ íŠ¸ë¦¬ëª¨ì–‘ì˜ í¬í•¨ê´€ê³„ë¥¼ ê°€ì§ ì™œ HTMLì„ ì•Œì•„ì•¼ í•˜ëŠ”ê°€? HTMLë„ ì¼ì¢…ì˜ í”„ë¡œê·¸ë¨ìœ¼ë¡œ, ê·œì¹™ì„ ë¶„ì„í•˜ì—¬ ë°ì´í„°ì˜ ì¶”ì¶œì´ ê°€ëŠ¥í•˜ë‹¤ ì¶”ì¶œëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•˜ì—¬ ë‹¤ì–‘í•œ ë¶„ì„ì´ ê°€ëŠ¥ Regular Expression ì •ê·œ í‘œí˜„ì‹ ë³µì¡í•œ ë¬¸ìì—´ íŒ¨í„´ì„ ì •ì˜í•˜ëŠ” ë¬¸ì í‘œí˜„ ê³µì‹ íŠ¹ì •í•œ ê·œì¹™ì„ ê°€ì§„ ë¬¸ìì—´ì˜ ì§‘í•© HTMLì—­ì‹œ tagë¥¼ ì‚¬ìš©í•œ ì¼ì •í•œ í˜•ì‹ì´ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì— ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ê¸°ê°€ í¸í•¨ ë¬¸ë²•ì´ ë°©ëŒ€í•˜ì—¬ ê·¸ë–„ ê·¸ë•Œ ê²€ìƒ‰ì´ í•„ìš” ê¸°ë³¸ì ì¸ê²ƒë§Œ ì¼ë‹¨ ìˆ™ì§€í•˜ì XML ë°ì´í„°ì˜ êµ¬ì¡°ì™€ ì˜ë¯¸ë¥¼ ì„¤ëª…í•˜ëŠ” tagë¥¼ ì‚¬ìš©í•˜ì—¬ í‘œì‹œí•˜ëŠ” ì–¸ì–´(like html) XMLì€ ì»´í“¨í„°(PC &lt;-&gt; ìŠ¤ë§ˆíŠ¸í°)ê°„ì˜ ì •ë³´ë¥¼ ì£¼ê³ ë°›ê¸° ë§¤ìš° ìœ ìš©í•œ ì €ì¥ë°©ì‹ìœ¼ë¡œ ì“°ì´ê³  ìˆìŒ ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ Parsingì´ ê°€ëŠ¥í•¨ ê·¸ëŸ¬ë‚˜ ê°€ì¥ ë§ì´ ì“°ì´ëŠ” parserì¸ beautifulsoup BeautifulSoup ì–¸ì–´ Scrapingì„ ìœ„í•œ ë„êµ¬ ëŠë¦¬ì§€ë§Œ ê°„í¸í•˜ë‹¤ JSON javaScript object Notation ì›¹ì–¸ì–´ì¸ Java Scriptì˜ ë°ì´í„° ê°ì²´ í‘œí˜„ ë°©ì‹ ê°„ê²°í•˜ë‹¤ ë°ì´í„° ìš©ëŸ‰ì´ ì ê³  Codeë¡œ ì „í™˜ì´ ì‰½ë‹¤ ë”°ë¼ì„œ XMLì„ ëŒ€ì²´í•˜ê³  ìˆë‹¤ Pythonì˜ DIct Typeê³¼ ìœ ì‚¬, Key:Valueê°’ìœ¼ë¡œ ì ‘ê·¼ê°€ëŠ¥ Json in PythonëŒ€ë¶€ë¶„ì˜ ì‚¬ì´íŠ¸ì—ì„œ ì •ë³´êµí™˜ì‹œ JSONì„ ì‚¬ìš©í•œë‹¤ ë°ì´í„° ì €ì¥ê³¼ ì½ê¸°ëŠ” dict typeê³¼ ìƒí˜¸í˜¸í™˜ ê°€ëŠ¥","link":"/2021/01/22/2021-01-22-Boostcamp5/"},{"title":"Gradient","text":"Gradientê´€ë ¨ ê²½ì‚¬í•˜ê°•ë²•ê²½ì‚¬ ìƒìŠµ/ê²½ì‚¬í•˜ê°•ë²•ì€ ê·¹ê°’ì— ë„ë‹¬ì‹œ ì›€ì§ì„ì„ ë©ˆì¶˜ë‹¤ Algorithmvar = init grad = gradient(var) while (abs(grad)&gt;eps): â€‹ var = var - lr *grad â€‹ grad = gradiwnt(var) í•™ìŠµë¥  lr ì„ ì¡°ì ˆí•˜ì—¬ ì†ë„ë¥¼ ì¡°ì ˆ eps : ì¢…ë£Œì¡°ê±´ lrì€ ë‹¤ë£° ë•Œ ì¡°ì‹¬íˆ ë‹¤ë£¨ì gradí•¨ìˆ˜ê°€ ë¯¸ë¶„ê°’ì„ êµ¬í•´ì£¼ê³  varê°’ì—ì„œ ë¯¸ë¶„ê°’ì„ ë¹¼ì£¼ì–´ ê·¸ê°’ì—ì„œ ë¯¸ë¶„ê°’ì„ ë˜êµ¬í•œë‹¤ ì´ë ‡ê²Œ ê³„ì† ì—…ë°ì´íŠ¸ ì´ë²ˆ ê°•ì˜ ë¶€í„° ìˆ˜í•™ ê°•ì˜ì—ì„œëŠ” ìˆ˜ì‹ì´ ë§ìœ¼ë¯€ë¡œ PDF íŒŒì¼ì— ìˆ˜ì‹ì„ í•„ê¸°í•œê²ƒìœ¼ë¡œ ì •ë¦¬ë¥¼ ëŒ€ì²´í•˜ê² ë‹¤","link":"/2021/01/26/2021-01-26-Boostcamp7/"},{"title":"Pandas &amp; ë”¥ëŸ¬ë‹ í•™ìŠµë°©ë²•","text":"pandas êµ¬ì¡°í™”ëœ ë°ì´í„°ì˜ ì²˜ë¦¬ë¥¼ ì§€ì›í•˜ëŠ” íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ panel data -&gt; pandas ê°€ë¡œì¤„ì€ instance, row data tableì€ ëª¨ë“  dataê°€ ë‹´ê²¨ ìˆëŠ” sample columnì€ ê°ê°ì˜ featureì´ë¦„ë“¤, data.columnsí•˜ë©´ ì´ë¦„ë“¤ì„ ê°ê° ì§€ì •í•´ ì¤„ ìˆ˜ ìˆìŒ feature vector, ê° featureì— í•´ë‹¹í•˜ëŠ” vector dataëŠ” ê°ê°ì˜ ê°’ headëŠ” ì•ì— 5ê°œ ì¶œë ¥ data_urlì§€ì • í›„ , pd.readcsv(data_url, sep=â€™\\s+â€™, header = None) headerì„¤ì •ì‹œ ì´ˆê¸°í™”ì‹œ columnì„ ê°™ì´ ì´ˆê¸°í™” í•´ì¤„ìˆ˜ ìˆìŒ ì´ë ‡ê²Œ read_csvë¥¼ë¡œ ì½ì–´ì˜¤ë©´ typeì€ numpyì„ SeriesColumnì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ì˜ ëª¨ìŒ indexê°’ì´ ìˆë‹¤ 1234567891011121314151617181920212223import pandaslist_data = [1, 2, 3, 4, 5]list_name = ['a','b','c','d','e']#ê¸°ì¡´ ë°ì´í„°ë¥¼ ì ‘ê·¼í•˜ëŠ” ì¸ë±ì‹±ì´ ìˆ«ìë‚˜ ë¬¸ìë¡œ ì§€ì •ì„ í•  ìˆ˜ ìˆë‹¤example_obj = Series(data=list_data)example_obj'''0 11 22 33 44 5dtype: int64'''example_obj = Series(data=list_data,index =list_name )'''a 1b 2c 3d 4e 5dtype: int64''' subclass of ndarray dictíƒ€ì…ì„ seriesì— ë„£ì–´ì£¼ë©´, keyê°’ë“¤ì´ index valueëŠ” valueë¡œ matchingë˜ì–´ ìƒì„±ë¨ Seriesì— inedxë¡œ ì ‘ê·¼í• ë•Œ dictì™€ ë¹„ìŠ·í•˜ê²Œ ì ‘ê·¼í•œë‹¤ astypeìœ¼ë¡œ typeë³€í™˜ ê°€ëŠ¥ Series.name = â€˜numberâ€™ë¼ê³  í•´ì„œ seriesì˜ ì´ë¦„ì„ ì§€ì • ê°€ëŠ¥ index.nameë„ ì§€ì •ê°€ëŠ¥ 123456789101112131415dict_data_1 = {&quot;a&quot;: 1, &quot;b&quot;: 2, &quot;c&quot;: 3, &quot;d&quot;: 4, &quot;e&quot;: 5}indexes = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;, &quot;g&quot;, &quot;h&quot;]series_obj_1 = Series(dict_data_1, index=indexes)series_obj_1'''a 1.0b 2.0c 3.0d 4.0e 5.0f NaNg NaNh NaNdtype: float64''' Data FrameSeries dataê°€ ëª¨ì—¬ì„œ í•˜ë‚˜ì˜ Data Frameì„ ì´ë£¸ ì—¬ê¸°ì„œëŠ” Columnë„ ê°€ëŠ¥ ì ‘ê·¼ ìì²´ë¥¼ indexì™€ columnsë¡œ í•¨(í–‰ë ¬ì²˜ëŸ¼ ì ‘ê·¼ê°€ëŠ¥) ê° Colunmë“¤ì€ typeì´ ë‹¬ë¼ë„ ë¨ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from pandas import Series, DataFrameimport pandas as pdimport numpy as np#Example from - https://chrisalbon.com/python/pandas_map_values_to_values.htmlraw_data = { &quot;first_name&quot;: [&quot;Jason&quot;, &quot;Molly&quot;, &quot;Tina&quot;, &quot;Jake&quot;, &quot;Amy&quot;], &quot;last_name&quot;: [&quot;Miller&quot;, &quot;Jacobson&quot;, &quot;Ali&quot;, &quot;Milner&quot;, &quot;Cooze&quot;], &quot;age&quot;: [42, 52, 36, 24, 73], &quot;city&quot;: [&quot;San Francisco&quot;, &quot;Baltimore&quot;, &quot;Miami&quot;, &quot;Douglas&quot;, &quot;Boston&quot;],}df = pd.DataFrame(raw_data, columns=[&quot;first_name&quot;, &quot;last_name&quot;, &quot;age&quot;, &quot;city&quot;])DataFrame(raw_data, columns=[&quot;age&quot;, &quot;city&quot;])df.first_namedf['first_name']'''0 Jason1 Molly2 Tina3 Jake4 AmyName: first_name, dtype: object'''#ì´ë ‡ê²Œ dataframeë‚´ì˜ seriesì— ì ‘ê·¼ ê°€ëŠ¥df.loc[1]# indexì˜ ê°’ì„ ë„£ì–´ì¤˜ì„œ í–‰ì„ ë¶ˆëŸ¬ì˜´(indexì˜ ì´ë¦„)df.iloc[:3]#indexì˜ numbers = pd.Series(np.nan, index=[49, 48, 47, 46, 45, 1, 2, 3, 4, 5])s.loc[:3]'''49 NaN48 NaN47 NaN46 NaN45 NaN1 NaN2 NaN3 NaNdtype: float64'''#3ì´ë¼ëŠ” ì´ë¦„ì„ ê°€ì§„ indexê¹Œì§€ë§Œ ë½‘ìŒdf.debt = df.age&gt;40#boolean ê°’ì„ ê°€ì§„ series ì£¼ê°€ ê°€ëŠ¥df.T#Transposedf.values#array í˜•íƒœë¡œ ì¶œë ¥í•´ì¤Œdf.to_csv()# csvíŒŒì¼ë¡œ ì €ì¥ê°€ëŠ¥ columnì„ ì‚­ì œí•¨ axis = 1 -&gt; columnê¸°ì¤€ìœ¼ë¡œ debtë¥¼ ì‚­ì œ del ì„ ì‚¬ìš©í•˜ë©´ ì‚­ì œí• ìˆ˜ ìˆìŒ ê±°ì˜ csvíŒŒì¼ì´ë‚˜ json data Selection with column names1ê°œ ì´ìƒì˜ columns ì¶”ì¶œ listê¼´ë¡œ ë„£ì–´ì£¼ë©´ dataframeí˜•íƒœë¡œ ë½‘íŒë‹¤ fancy index, boolen index Map apply Lambda mapê³¼ lambdaë¥¼ ì‚¬ìš©í•˜ì—¬ series dataì— lambdaí•¨ìˆ˜ë¥¼ ì ìš©í•´ì¤€ë‹¤ ì—¬ê¸°ì„œ mapì„ dict typeìœ¼ë¡œ ë°ì´í„°ë¥¼ êµì²´í•˜ê¸°ë„ í•¨ indexëŠ” ê·¸ëŒ€ë¡œ s1ì„ ë”°ë¥´ê³  dictì— keyê°’ì— í•´ë‹¹í•˜ëŠ” indexì— value ê°’ì„ ë„£ì–´ì¤€ë‹¤ ë‚˜ë¨¸ì§€ëŠ” Nanì²˜ë¦¬ Series + dataframeì´ê²ƒë„ ì—°ì‚°ì„ í•  ìˆ˜ ìˆë‹¤ ë§ì…ˆì‹œ addí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ axisë¥¼ ì¡ì•„ì£¼ì–´ì•¼ í•œë‹¤ ê·¸ëŸ¬ë©´ broadcatingì´ ì¼ì–´ë‚œë‹¤ ì´ë ‡ê²Œ ì²˜ë¦¬í•´ì£¼ë©´ ìˆ«ìë¡œ í•™ìŠµì„ í•´ì•¼í•˜ëŠ” ë”¥ëŸ¬ë‹ ê³¼ì •ì—ì„œ ë¬¸ìì—´ ë°ì´í„°ë¥¼ ìˆ«ìê°’ìœ¼ë¡œ ë°”ê¾¸ì–´ ì£¼ì–´ ê³„ì‚°ì´ ê°€ëŠ¥í•´ì§„ë‹¤. ìœ„ì™€ê°™ì´ í•¨ìˆ˜ë¥¼ í•˜ë‚˜ ì •ì˜í•´ì„œ mapìœ¼ë¡œ ì²˜ë¦¬í•´ì£¼ë©´ ëœë‹¤ ë˜í•œ applyë¼ëŠ” í•¨ìˆ˜ë¡œ ê° columnì— ëŒ€í•´ì„œ í•¨ìˆ˜ì—°ì‚°ì„ í•´ ì¤„ ìˆ˜ë„ ìˆë‹¤ pandas2 Groupby SQL groupbyëª…ë ¹ì–´ì™€ ê°™ì€ split -&gt; apply -&gt; combine ê³¼ì •ì„ ê±°ì³ ì—°ì‚°í•¨ ê¸°ì¡´ì˜ dataì—ì„œ ë¨¼ì € ê°™ì€ ì¢…ë¥˜ì˜ indexê°€ ê°™ì€ ê²ƒ ë¼ë¦¬ ë¬¶ì–´ì„œ ê³„ì† ì—°ì‚°ì„ ì§„í–‰í•˜ëŠ”ê²ƒ df.groupby(&quot;Team&quot;)['Points'].sum() ìœ„ì—°ì‚° ì‹¤í–‰ì‹œ Teamì´ë¼ëŠ” columnê¸°ì¤€ìœ¼ë¡œ Pointsfmf ëª¨ë‘ ë”í•´ì„œ ëª¨ë‘ ê°™ì€ teamì´ ê°™ì€ê°’ì„ ê°€ì§€ëŠ”ì• ë“¤ë¼ë¦¬ pointë¥¼ ëª¨ë‘ ë”í•œë‹¤ í•œê°œ ì´ìƒì˜ columnì„ ë¬¶ì„ ìˆ˜ ìˆìŒ groupbyì˜ ê²°ê³¼(typeì€ seriesì´ë‹¤)ë¡œ ë¬¶ì¸ seriesì— unstackì„ í•˜ê²Œ ë˜ë©´ dataframeìœ¼ë¡œ ë§Œë“¤ì–´ì¤Œ swaplevel()ë¡œ indexì˜ levelì„ ë³€ê²½í•  ìˆ˜ ìˆìŒ ( ê²°ê³¼ë¬¼ë§Œ ë°”ë€Œì–´ì„œ ì¶œë ¥ë˜ëŠ”ê±°ì§€ , ì›ë³¸ì€ ë°”ë€Œì§€ X ) sort_index(level = 0) sort_values() df.groupby([&quot;Team&quot;,&quot;Year&quot;])['Points'].sum() ì´ê²°ê³¼ë¡œ ì—¬ëŸ¬ teamê³¼ yearë¡œ ë‚˜ëˆ„ì–´ì§„ indexë¥¼ ê°€ì§„ seriesê°€ ìƒì„±ë˜ëŠ”ë° ì—¬ê¸°ì„œ .sum(level) levelê°’ìœ¼ë¡œ indexë¥¼ ì •í•´ ì—°ì‚°ì„ ì·¨í•´ ì¤„ ìˆ˜ ìˆë‹¤. Grouped12345grouped = df.groupby(&quot;Team&quot;)list(grouped)#groupedëŠ” generatorì˜ í˜•íƒœë¡œ listë¥¼ ì·¨í•´ì£¼ì–´ì•¼ ê°’ì„ í™•ì¸ ê°€ëŠ¥#í•´ë‹¹í•˜ëŠ” indexë¥¼ ê¸°ì¤€ìœ¼ë¡œ key:value í˜•íƒœë¡œ ë°˜í™˜í•´ì„œ ì €ì¥í•´ì¤Œ# keyëŠ” í•´ë‹¹í•˜ëŠ” index valueëŠ” dataframeì˜ í˜•íƒœë¡œ ë§Œë“¤ì–´ ì¤€ë‹¤ ì¶”ì¶œëœ groupì •ë³´ì—ëŠ” ì„¸ê°€ì§€ ìœ í˜•ì˜ applyê°€ ê°€ëŠ¥í•¨ Aggregation : ìš”ì•½ëœ í†µê³„ì •ë³´ë¥¼ ì¶”ì¶œí•´ ì¤Œ Transformation : í•´ë‹¹ ì •ë³´ë¥¼ ë³€í™˜í•´ì¤Œ FIltration : íŠ¹ì • ì •ë³´ë¥¼ ì œê±° í•˜ì—¬ ë³´ì—¬ì£¼ëŠ” í•„í„°ë§ ê¸°ëŠ¥ ê°œë³„ë°ì´í„°ì— ì´ê±¸ ì ìš©í•  ìˆ˜ ìˆìŒ ë°ì´í„°ì˜ ë³€í™˜ê³¼ì •ì—ì„œ ë§ì´ ì“´ë‹¤ gropedëœ ìƒíƒœì—ì„œ groupë³„ë¡œ ì—°ì‚°ì„ ê°€ëŠ¥í•˜ê²Œ í•´ì¤€ë‹¤ df.groupby(â€œTeamâ€).filter(lambda x : len(x) &gt;=3) ë”¥ëŸ¬ë‹ í•™ìŠµë°©ë²•ì‹ ê²½ë§ ê° í–‰ë²¡í„°ì¸ oëŠ” ë°ì´í„° Xì™€ ê°€ì¤‘ì¹˜ W ê·¸ë¦¬ê³  biadí•­ì¸ bì˜ í•©ìœ¼ë¡œ í‘œí˜„ëœë‹¤ ì…ë ¥ë²¡í„°ì¸ Xì˜ ì°¨ì› = n x d ì¶œë ¥ë²¡í„°ì¸ Oì˜ ì°¨ì› = n x p dê°œì˜ ë³€ìˆ˜ë¡œ pê°œì˜ ì„ í˜•ëª¨ë¸ì„ ë§Œë“¤ì–´ pê°œì˜ ì ì¬ë³€ìˆ˜ë¥¼ ì„¤ëª… Xë¥¼ Oì— Mappingí•œë‹¤ê³  ìƒê°í•˜ë©´ í¸í•˜ë‹¤ í–‰ë ¬ì„ ìƒê°í•˜ëŠ” ë°©ì‹ì—ì„œ í–‰ë ¬ì„ í•˜ë‚˜ì˜ dataë¥¼ ë‹¤ë¥¸ domainìœ¼ë¡œ ë³€í™˜í•˜ëŠ” operatorë¼ê³  ìƒê°í–ˆì„ë•Œì˜ ê°œë…ì„ ì‚¬ìš©í•œ ê²ƒì´ë‹¤ Softmax ì¶œë ¥ ë²¡í„° oì— expë¥¼ ì·¨í•´ì£¼ì–´ ê° ì¶œë ¥ë²¡í„°ì— ëŒ€í•œ í™•ë¥ ìœ¼ë¡œ ë‚˜íƒ€ë‚´ ì¤€ë‹¤ ë¶„ë¥˜ë¬¸ì œë¥¼ í’€ë•Œ ì„ í˜•ëª¨ë¸ê³¼ Softmaxí•¨ìˆ˜ë¥¼ ê²°í•©í•˜ì—¬ ì˜ˆì¸¡í•œë‹¤ ì‹ ê²½ë§ì€ ì„ í˜•ëª¨ë¸ê³¼ í™œì„±í•¨ìˆ˜ë¥¼ í•©ì„±í•œ í•¨ìˆ˜ì´ë‹¤ Xë¼ëŠ” ì…ë ¥ ë²¡í„°ë“¤ì— ê°€ì¤‘ì¹˜ë¥¼ ê³±í•´ì£¼ê³  biasë¥¼ ë”í•´ì£¼ì–´ activation functiondì„ ì ìš© ìœ„ì˜ ì‹ì€ 2 layerì˜ ê°€ì¥ ê°„ë‹¨í•œ NNì´ë‹¤ í™œì„±í•¨ìˆ˜ë€? í”íˆë“¤ ìš”ì¦˜ì—ëŠ” ReLUí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤ í™œì„±í•¨ìˆ˜ë¥¼ ì“°ì§€ ì•Šìœ¼ë©´ ì„ í˜•ëª¨ë¸ê³¼ ë”¥ëŸ¬ë‹ì€ ì°¨ì´ê°€ ì—†ê¸° ë•Œë¬¸ì— ì¤‘ìš”í•œ ìš”ì†Œì´ë‹¤ ex) sigmoid, softmax, ReLU, tanh, ë‹¤ì¸µ perceptronì€ ìœ„ì˜ Hë¥¼ ë‹¤ì‹œ ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ì— ê³±í•˜ê³  â€¦. ì´í›„ì˜ ê³¼ì •ì„ ë°˜ë³µí•œë‹¤ ì´ë¡ ì ìœ¼ë¡œëŠ” 2-layer ì‹ ê²½ë§ìœ¼ë¡œë„ í•¨ìˆ˜ë¥¼ ê·¼ì‚¬í•  ìˆ˜ ìˆì§€ë§Œ ì¸µì´ ê¹Šì„ìˆ˜ë¡ ëª©ì í•¨ìˆ˜ë¥¼ ê·¼ì‚¬í•˜ëŠ”ë° í•„ìš”í•œ nodeì˜ ìˆ«ìê°€ í›¨ì”¬ ë¹¨ë¦¬ ì¤„ì–´ë“¤ì–´ íš¨ìœ¨ì ì´ë‹¤ ì¸µì´ ì–‡ìœ¼ë©´ wideí•œ ì‹ ê²½ë§ì´ ë˜ì–´ì•¼ í•œë‹¤ ì—­ì „íŒŒ ìˆ˜ì‹ì „ê°œ","link":"/2021/01/27/2021-01-27-Boostcamp8/"},{"title":"í™•ë¥ ë¡ 2","text":"Day10 : í™•ë¥ ë¡ 2 ëª¨ìˆ˜ë€? í†µê³„ì  ëª¨ë¸ë§ì€ ì ì ˆí•œ ê°€ì •ìœ„ì—ì„œ í™•ë¥ ë¶„í¬ë¥¼ ì¶”ì •í•˜ëŠ”ê²ƒì´ ëª©í‘œ! ë°ì´í„°ëŠ” ìœ í•œí•˜ê¸° ë•Œë¬¸ì— ê·¼ì‚¬ì ìœ¼ë¡œ í™•ë¥ ë¶„í¬ë¥¼ ì¶”ì •í•  ìˆ˜ ë°–ì— ì—†ë‹¤ ë°ì´í„°ê°€ íŠ¹ì •í™•ë¥ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ì„ í—˜ì ìœ¼ë¡œ ê°€ì •í•œ í›„ ê·¸ë¶„í¬ë¥¼ ê²°ì •í•˜ëŠ” ëª¨ìˆ˜(parameter)ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì„ ëª¨ìˆ˜ì  ë°©ë²•ë¡ ì´ë¼ê³  í•œë‹¤ íŠ¹ì •í™•ë¥ ë¶„í¬ë¥¼ ê°€ì •í•˜ì§€ ì•Šê³  ë°ì´í„°ì— ë”°ë¼ ëª¨ë¸ì˜ êµ¬ì¡° ë° ëª¨ìˆ˜ì˜ ê°œìˆ˜ê°€ ìœ ì—°í•˜ê²Œ ë°”ë€Œë©´ ë¹„ëª¨ìˆ˜ë°©ë²•ë¡ ì´ë‹¤ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ì›ë¦¬ë¥¼ ë¨¼ì € ê³ ë ¤í•˜ëŠ” ê²ƒì´ ì›ì¹™ !!! ë°ì´í„°ë¡œ ëª¨ìˆ˜ë¥¼ ì¶”ì •í•´ë³´ì! í‘œë³¸í‰ê·  : ì£¼ì–´ì§„ ë°ì´í„°ì˜ ì‚°ìˆ í‰ê·  â€‹ N-1ë¡œ ë‚˜ëˆ„ëŠ”ê²Œ ì¡°ê¸ˆ ì‹ ê¸°í•˜ê³  ë‹¤ë¥¸ ì  -&gt; ë¶ˆí¸ì¶”ì •ëŸ‰ ì¤‘ìš”í•œ ê°œë… í†µê³„ëŸ‰(í‘œë³¸ë¶„ì‚°, í‘œë³¸í‰ê· )ì˜ í™•ë¥ ë¶„í¬ë¥¼ í‘œì§‘ë¶„í¬ë¼ë¶€ë¥´ë©°(sampling distribution) ì´ê²Œ ì¢€ ì‹ ê¸°í•œê²Œ ë°ì´í„°ë“¤ì˜ í™•ë¥ ë¶„í¬ëŠ” í‘œë³¸ë¶„í¬(sample distribution)ì´ë‹¤ ëª¨ì§‘ë‹¨ì˜ í™•ë¥ ë¶„í¬ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•Šì•„ë„ sampleì˜ ê°¯ìˆ˜ë¥¼ ëŠ˜ë¦°ë‹¤ë©´ í‘œë³¸í‰ê· ì˜ ëª¨ì§‘ë¶„í¬ëŠ” ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ ê°€ëŠ¥ë„ì˜ ì§ê´€ì ì¸ ì •ì˜ : í™•ë¥ ë¶„í¬í•¨ìˆ˜ì˜ yê°’ ì…€ ìˆ˜ ìˆëŠ” ì‚¬ê±´: ê°€ëŠ¥ë„ = í™•ë¥  ì—°ì† ì‚¬ê±´: ê°€ëŠ¥ë„ â‰ â‰  í™•ë¥ , ê°€ëŠ¥ë„ = PDFê°’ ìˆ˜ì‹ì€ ê°™ì§€ë§Œ ë³€ìˆ˜ê°€ ë‹¤ë¦„ ìµœëŒ€ê°€ëŠ¥ë„ ì¶”ì •ë²• (MLE) ì´ë¡ ì ìœ¼ë¡œ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ëª¨ìˆ˜ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ê°€ ë°”ë¡œ MLEë‹¤ ê°€ëŠ¥ë„ í•¨ìˆ˜ : ë°ì´í„°ê°€ ì£¼ì–´ì§„ ìƒí™©ì—ì„œ $\\theta$ë¥¼ ë³€í˜• ì‹œí‚´ì— ë”°ë¼ ë³€í•˜ëŠ” í•¨ìˆ˜ë¡œ ì´í•´ ì¦‰ ì¡°ê±´ë¶€ í•¨ìˆ˜ì™€ ë¹„ìŠ· ê·¸ëŸ¬ë‚˜ $\\theta$ì— ëŒ€í•œ í™•ë¥ ì´ ì•„ë‹Œ ëŒ€ì†Œë¹„êµê°€ ê°€ëŠ¥í•œ ê·¸ëƒ¥ í•¨ìˆ˜ë¼ê³  ìƒê°ì„ í•˜ì ë°ì´í„° ì§‘í•© Xê°€ ë…ë¦½ì ìœ¼ë¡œ ì¶”ì¶œë˜ì—ˆì„ ê²½ìš° ë¡œê·¸ê°€ëŠ¥ë„ë¥¼ ìµœì í™” í•©ë‹ˆë‹¤ P(xi | $\\theta$)ì˜ ê³±ì´ í’€ì–´ì„œ ì“°ë©´ ë¡œê·¸ë“¤ì˜ ê³±ì´ë¼ ë¡œê·¸ì˜ í•©ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤ ì™œ ë¡œê·¸ê°€ëŠ¥ë„ë¥¼ ì‚¬ìš©í•˜ë‚˜ìš”?ë°ì´í„°ì˜ ìˆ«ìê°€ ì¡¸ë¼ ë§ì•„ì§€ë©´ ì»´í“¨í„°ì˜ ì •í™•ë„ë¡œëŠ” Likelyhoodë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤ ë”°ë¼ì„œ ë°ì´í„°ê°€ ë…ë¦½ì¼ ê²½ìš° ê°€ëŠ¥ë„ì˜ ê³±ì…ˆì„ ê°€ëŠ¥ë„ì˜ ë§ì…ˆìœ¼ë¡œ ë°”ë€Œë©´ ì»´í“¨í„°ë¡œ ì—°ì‚°í•´ì„œ ìµœì í™” ê°€ëŠ¥ ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ ê°€ëŠ¥ë„ë¥¼ ìµœì í™”í•  ë•Œ ë¯¸ë¶„ ì—°ì‚°ì„ ì‚¬ìš©í•˜ê²Œ ë˜ëŠ”ë°, ë¡œê·¸ ê°€ëŠ¥ë„ë¥¼ ì‚¬ìš©í•˜ë©´ ì—°ì‚°ëŸ‰ì„ O(n^2^)ì—ì„œ O(n)ìœ¼ë¡œ ì¤„ì–´ë“ ë‹¤ ëŒ€ê°œì˜ ì†ì‹¤í•¨ìˆ˜ì˜ ê²½ìš° gradient descentë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, ìŒì˜ ë¡œê·¸ê°€ëŠ¥ë„ë¥¼ ìµœì í™”í•˜ê²Œ ëœë‹¤. WHY ìŒì˜ ë¡œê·¸ê°€ëŠ¥ë„? -&gt; ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì†Œí™” í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ìŒì˜ ë¡œê·¸ê°€ëŠ¥ë„ ì‚¬ìš© Ex1 : ì •ê·œë¶„í¬ë…ë¦½ì ì¸ í‘œë³¸ì„ ì–»ì—ˆì„ ë•Œ ìµœëŒ€ê°€ëŠ¥ë„ ì¶”ì •ë²•ì„ ì´ìš©í•˜ì—¬ ëª¨ìˆ˜ë¥¼ ì¶”ì •í•˜ë©´ $\\theta$($\\mu,\\sigma$)ì— ëŒ€í•´ ì˜¤ë¥¸ìª½ ìˆ˜ì‹ì„ ë¯¸ë¶„ ìœ ë„ê³¼ì •ì„ ì†ìœ¼ë¡œ ë„ì¶œí•´ë³´ê¸° Ex2 : ì¹´í…Œê³ ë¦¬ ë¶„í¬ ì¹´í…Œê³ ë¦¬ ë¶„í¬ì´ê¸° ë•Œë¬¸ì— ì œì•½ì‹ì´ ìƒê¹€ ì—¬ê¸°ì„œì˜ ëª¨ìˆ˜ëŠ” 1-d ì°¨ì› ê¹Œì§€ ê°’ì´ 1ë˜ëŠ”0ì´ë  í™•ë¥  ëª¨ë‘ ë”í–ˆì„ë•Œ 1ì´ ë˜ì–´ì•¼ í•˜ëŠ” ì œì•½ì‹ì´ ìƒê¸´ê²ƒ ê·¼ë° ì´ë³´ë‹¤ ë¨¼ì € ê°€í…Œê³ ë¦¬ ë¶„í¬ì— ëŒ€í•œ ì´í•´ë¥¼ í•´ë³´ì ë² ë¥´ëˆ„ì´ ë…ë¦½ì‹œí–‰ ë™ì „ì„ 100ë²ˆ ë˜ì¡Œë‹¤. ê·¸ ì¤‘ 60ë²ˆ ë‚˜ì™”ë‹¤. ì•ë©´ì´ ë‚˜ì˜¬ í™•ë¥ ì´ p(ëª¨ìˆ˜)ë¼ê³  í•˜ë©´ ê°€ì¥ ì´ëŸ¬í•œ ì‚¬ê±´ì´ ì¼ì–´ë‚  ê°€ëŠ¥ë„ê°€ ë†’ì€ pë¥¼ êµ¬í•´ë³´ë©´.. ì¼ë‹¨ 100ë²ˆ ë˜ì ¸ì„œ 60ë²ˆ ì•ë©´ì´ ë‚˜ì˜¤ëŠ” í™•ë¥  P = 100C60 p^60 (1-p)^40 ì´ëŸ¬í•œ í™•ë¥ ì„ pë¡œ ë¯¸ë¶„í–ˆì„ ë•Œ 0ì´ ë˜ëŠ” pê°’ì´ Pê°€ ìµœëŒ€ê°€ ë  ë–„ì¼ ê²ƒì´ë©° ì´ê²ƒì´ MLEë¡œ ì¶”ì •ë˜ëŠ” ëª¨ìˆ˜ pì´ë‹¤. ë¡œê·¸í•¨ìˆ˜ì˜ ì„±ì§ˆì„ ìƒê°í–ˆì„ë•Œ Pì˜ ì¦ê°ì€ f(p) =log(p^60 (1-p)^40)ê³¼ ì¦ê°ì´ ê°™ë‹¤. ì¦‰ f(p) ê°€ ìµœëŒ€ê°€ ë ë•Œ Pë„ ìµœëŒ€ê°€ ëœë‹¤. df/dp = 60/p - 40/(1-p) = 0 , ì¦‰ p = 0.6 ì¼ë•Œ fâ€™(p) = 0 ì´ ë˜ë©° ì´ê²ƒì´ ìš°ë¦¬ì˜ ì§ê´€ê³¼ ì¼ì¹˜í•œë‹¤. (MLE ë) ì´ê²Œ ì´ì œ ì´í•­ë¶„í¬ ì „ì¶”ì • : 0.5ê°’ì„ ê°€ì§€ê³  ì‹¶ì–´ì„œ ë˜ì¡ŒëŠ”ë° êµ¬ë¶„ì¶”ì • : ì‹ ë¢°êµ¬ê°„ ì¹´í…Œê³ ë¦¬ í™•ë¥ ë¶„í¬ì—ì„œ xë°ì´í„°ê°€ one hot encodingëœ ë²¡í„°êµ¬ë§Œ ê·¸ë˜ì„œ ì´ë ‡ê²Œ í‘œí˜„í•˜ëŠ”ê²ƒì´ì˜€ë‹¤ ì˜¤ë¥¸ìª½ ì œì•½ì‹ì„ ë§Œì¡±í•˜ë©´ì„œ ì™¼ìª½ ëª©ì ì‹ì„ ìµœëŒ€í™” -&gt; ìµœëŒ€ê°€ëŠ¥ë„ ì¶”ì •(MLE) ì—¬ê¸°ì„œë„ ë¼ê·¸ë‘ì£¼ê°€â€¦.. ì´ê±° ìˆ˜ì‹ì „ê°œ í•´ë³´ê¸° ë”¥ëŸ¬ë‹ì—ì„œì˜ ìµœëŒ€ê°€ëŠ¥ë„ ì¶”ì •ë²•ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ $\\theta$(W1,W2,W3â€¦,WL)ì´ë¼ í‘œê¸°í–ˆì„ë•Œ ë§ˆì§€ë§‰ softmax vectorì€ ì¹´í…Œê³ ë¦¬ë¶„í¬ì˜ ëª¨ìˆ˜(p1,p2â€¦,pk)ë¥¼ ëª¨ë¸ë§í•œë‹¤ ì›í•«ë²¡í„°ë¡œ í‘œí˜„ëœ ì •ë‹µ ë ˆì´ë¸” y = (y1,y2â€¦.,yk)ë¥¼ ê´€ì°°ë°ì´í„°ë¡œ ì´ìš©í•´ í™•ë¥ ë¶„í¬ì¸ ì†Œí”„íŠ¸ë©•ìŠ¤ ë²¡í„°ì˜ ë¡œê·¸ê°€ëŠ¥ë„ë¥¼ ìµœì í™” í•  ìˆ˜ ìˆë‹¤. í™•ë¥ ë¶„í¬ì—ì„œì˜ ê±°ë¦¬ ê¸°ê³„í•™ìŠµì—ì„œ ìœ ë„ë˜ëŠ” Loss functionë“¤ì€ ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” í™•ë¥ ë¶„í¬ì™€ ë°ì´í„°ì—ì„œ ê´€ì°°ë˜ëŠ” í™•ë¥ ë¶„í¬ì˜ ê±°ë¦¬ë¥¼ í†µí•´ ìœ ë„ëœë‹¤ ë‘í™•ë¥ ë¶„í¬ì‚¬ì´ì˜ ê±°ë¦¬ ì´ë³€ë™ê±°ë¦¬ (Total Variation Distance, TV) ì¿¨ë°±-ë¼ì´ë¸”ëŸ¬ ë°œì‚° ë°”ìŠˆíƒ€ì¸ ê±°ë¦¬ ì¿¨ë°±-ë¼ì´ë¸”ëŸ¬ ë°œì‚° (Kullback - Leibler Divergence)ì •ì˜ ì¡¸ë¼ë¦¬ ì–´ë µë‹¤ ë­”ì†Œë¦¬ì¸ê°€ ë„ëŒ€ì²´ ì´ê²Œ ê²°êµ­ì€ ì •ë‹µë ˆì´ë¸”ì„ P, ëª¨ë¸ ì˜ˆì¸¡ì„ Që¼ ë‘ë©´ ? Log likelihoodë¥¼ ìµœëŒ€í™” í•˜ëŠ”ê²ƒê³¼ Pì™€ Qì‚¬ì´ì˜ ì¿¨ë°±í•˜ì´ë¸”ëŸ¬ ë°œì‚°ì˜ ìµœì†Œí™”ëŠ” ë°€ì ‘í•˜ë‹¤?? ë‘ê°œì˜ í™•ë¥ ë¶„í¬ì˜ ê±°ë¦¬ë¥¼ ìµœì†Œí™” í•œë‹¤? ê²°êµ­ ê²°ë¡ ì€ !!!!!!!!!!!!! ë”¥ëŸ¬ë‹ ê¸°ê³„í•™ìŠµì—ì„œ í†µê³„í•™ì ì¸ ì§€ì‹(MLE)ë“¤ì„ ì‚¬ìš©í•˜ì—¬ Loss functionì„ ìµœì†Œí™” ì‹œí‚¬ìˆ˜ ìˆë‹¤???? í”¼ì–´ì„¸ì…˜ì—ì„œ ì´ë²ˆ ê°•ì˜ì— ëŒ€í•œ ì‹¬ë„ê¹Šì€? í† ë¡ ì„ í•˜ì˜€ë‹¤ Further Question í™•ë¥ ê³¼ ê°€ëŠ¥ë„ì˜ ì°¨ì´ëŠ” ë¬´ì—‡ì¼ê¹Œìš”? (ê°œë…ì ì¸ ì°¨ì´, ìˆ˜ì‹ì—ì„œì˜ ì°¨ì´, í™•ë¥ ë°€ë„í•¨ìˆ˜ì—ì„œì˜ ì°¨ì´) í™•ë¥  ëŒ€ì‹  ê°€ëŠ¥ë„ë¥¼ ì‚¬ìš©í•˜ì˜€ì„ ë•Œì˜ ì´ì ì€ ì–´ë–¤ ê²ƒì´ ìˆì„ê¹Œìš”? ë‹¤ìŒì˜ code snippetì€ ì–´ë–¤ í™•ë¥ ë¶„í¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì¼ê¹Œìš”? í•´ë‹¹ í™•ë¥ ë¶„í¬ì—ì„œ ë³€ìˆ˜ thetaê°€ ì˜ë¯¸í•  ìˆ˜ ìˆëŠ” ê²ƒì€ ë¬´ì—‡ì´ ìˆì„ê¹Œìš”? 123456import numpy as npimport matplotlib.pyplot as plttheta = np.arange(0, 1, 0.001)p = theta ** 3 * (1 - theta) ** 7plt.plot(theta, p)plt.show() ìœ„ì˜ ì½”ë“œë¥¼ ë³´ì thetaëŠ” 0-1 ì‚¬ì´ì—ì„œ 0.001ì˜ ê°„ê²©ìœ¼ë¡œ ê°’ì„ ê°€ì§„ë‹¤ ëª¨ë“  thetaì˜ í•©ì€ 1ë¡œ ì´ëŠ” ê²°êµ­ í™•ë¥ ì´ë‹¤ ì´ë²ˆì£¼ë§ì— ì •ë¦¬í•´ì•¼ ë  ê²ƒë“¤ ë‹¤ì–‘í•œ í™•ë¥ ë°€ë„ í•¨ìˆ˜ì—ëŒ€í•œê³ ì°° í™•ë¥ ê³¼ ê°€ëŠ¥ë„ ê°€ëŠ¥ë„ì—ì„œì˜ ìµœëŒ€ê°€ëŠ¥ë„ ì¶”ì •ë²• MLEì˜ ì ìš© - ì •ê·œë¶„í¬, ì´í•­ë¶„í¬ gradient descent ìˆ˜ì‹ì „ê°œ ì›”ìš”ì¼ë‚  ë°œí‘œ (ê¸°ë³¸ì ì¸ kaggle dataset) back propagation ìˆ˜ì‹ì „ê°œ pandas ì •ë¦¬ ì‹œê°í™” ë„êµ¬","link":"/2021/01/29/2021-01-28-Boostcamp10/"},{"title":"í™•ë¥ ë¡ 1","text":"Day9 : í™•ë¥ ë¡  ë§›ë³´ê¸° í™•ë¥ ë¡  ë”¥ëŸ¬ë‹ì€ í™•ë¥ ë¡  ê¸°ë°˜ì˜ ê¸°ê³„í•™ìŠµ ì´ë¡ ì— ë°”íƒ•ì„ ë‘ê³  ìˆë‹¤. ì†ì‹¤í•¨ìˆ˜ì˜ ì‘ë™ì›ë¦¬ê°€ ë°ì´í„° ê³µê°„ì„ í™•ë¥ , í†µê³„ì ìœ¼ë¡œ í•´ì„ íšŒê·€ë¶„ì„ì—ì„œ Loss function : L2 norm ì˜ˆì¸¡ì˜¤ì°¨ì˜ ë¶„ì‚°ì„ ìµœì†Œí™” Classificationì—ì„œ Loss function : CrossEntropyLoss : ëª¨ë¸ì˜ˆì¸¡ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ìµœì†Œí™” í•˜ëŠ” ë°©í–¥ ì´ë“¤ì˜ Lossë¥¼ ìµœì†Œí™” í•˜ê¸°ìœ„í•´ì„œëŠ” ì¸¡ì •í•˜ëŠ”ë²•ì„ ì•Œì•„ì•¼í•¨ : ì´ê²Œ ë°”ë¡œ í™•ë¥ ë¡  í™•ë¥ ë¶„í¬ëŠ” ë°ì´í„°ì˜ ì´ˆìƒí™” ë°ì´í„°ê³µê°„ì„ x*yë¼ í‘œê¸°í•˜ê³  DëŠ” ì´ ë°ì´í„° ê³µê°„ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ë¶„í¬ì…ë‹ˆë‹¤ ì´ì‚°í™•ë¥ ë³€ìˆ˜ vs ì—°ì†í™•ë¥ ë³€ìˆ˜í™•ë¥ ë³€ìˆ˜ëŠ” í™•ë¥ ë¶„í¬ Dì— ë”°ë¼ ì´ì‚°í˜•(discrete)ì™€ ì—°ì†í˜• (continous)í™•ë¥ ë³€ìˆ˜ë¡œ êµ¬ë¶„í•˜ê²Œ ë©ë‹ˆë‹¤ ë°ì´í„° ê³µê°„ì´ ì•„ë‹ˆë¼ í™•ë¥ ë³€ìˆ˜ì˜ ì¢…ë¥˜ì— ì´ˆì  ì´ì‚°í˜• í™•ë¥ ë³€ìˆ˜ í™•ë¥ ë³€ìˆ˜ê°€ ê°€ì§ˆìˆ˜ìˆëŠ” ëª¨ë“  ê²½ìš°ì˜ìˆ˜ë¥¼ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ í™•ë¥ ì„ ëª¨ë‘ ë”í•´ì„œ ëª¨ë¸ë§í•œë‹¤ ì—°ì†í˜• í™•ë¥ ë³€ìˆ˜ ì—°ì†í˜• í™•ë¥ ë³€ìˆ˜ëŠ” ë°ì´í„° ê³µê°„ì— ì •ì˜ëœ í™•ë¥ ë³€ìˆ˜ì˜ ë°€ë„(density)ìœ„ì—ì„œì˜ ì ë¶„ì„ í†µí•´ ëª¨ë¸ë§í•œë‹¤ í™•ë¥  ë¶„í¬ì— ë”°ë¼ ëª¨ë¸ë§ ë°©ë²•ì— ì°¨ì´ê°€ìˆë‹¤ ê·¸ë˜ì„œ ì ‘ê·¼ë²•ì´ ë‹¬ë¼ì•¼ í•œë‹¤ dataë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ë§ í• ë•Œ ì „ì²´ data X,Y ë¶„í¬ì—ì„œ P(x,y) -&gt; joint distribution ê°ê°ì˜ ë¹¨ê°„ ì¹¸ë“¤ì— ëŒ€í•´ì„œ íŒŒë€ì ë“¤ì„ COUNTí•œë‹¤ ì „ì²´ë¥¼ ë³´ë©´ ì—°ì†í™•ë¥ ë³€ìˆ˜ ê°™ì§€ë§Œ ë‚˜ëˆ„ì–´ ì£¼ë©´ ì´ì‚°í™•ë¥ ë¶„í¬ë¡œ ìƒê°í• ìˆ˜ ìˆê²Œë¨ í™•ë¥  ë¶„í¬ DëŠ” ëª¨ë¥´ê¸° ë•Œë¬¸ì— ì£¼ì–´ì§„ dataì˜ ê²°í•©ë¶„í¬ë¡œ ì›ë˜ í™•ë¥ ë¶„í¬ Dë¥¼ modeling í•  ìˆ˜ ìˆë‹¤. ì›ë˜ í™•ë¥ ë¶„í¬ì˜ Dì— ë”°ë¼ P(x,y)ì˜ ë¶„í¬ê°€ ê²°ì •ë˜ëŠ”ê²Œ ì•„ë‹ˆë¼!!!! ì›ë˜ í™•ë¥ ë¶„í¬ì— ìƒê´€ì—†ì´ ê²°í•©ë¶„í¬ëŠ” ë¬´ì—‡ì´ë“  ë  ìˆ˜ ìˆë‹¤. ì´ê±´ ì¦‰ modeling ë°©ì‹ì— ë”°ë¼ ê²°ì •ì´ ë˜ëŠ” ê²ƒì´ë‹¤ ì´ë ‡ê²Œ í•˜ëŠ”ì´ìœ ëŠ” ì›ë˜ì˜ í™•ë¥ ë¶„í¬ì™€ ë‹¤ë¥´ë”ë¼ë„ ì»´í“¨í„°ëŠ” ì´ì— ê·¼ì‚¬ë¥¼ ì‹œí‚¬ ìˆ˜ ìˆê¸°ë•Œë¬¸ì— P(x)ëŠ” ì…ë ¥ xì— ëŒ€í•œ ì£¼ë³€í™•ë¥ ë¶„í¬ë¡œ yì— ëŒ€í•œ ì •ë³´ë¥¼ ì£¼ì§€ëŠ” ì•ŠëŠ”ë‹¤ xì— ëŒ€í•´ì„œ ë§ì…ˆì´ë‚˜ ì ë¶„í•˜ë©´ yì—ëŒ€í•œ ì£¼ë³€í™•ë¥ ë¶„í¬ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. ì§€ê¸ˆë‹¤ë£¨ëŠ” í™•ë¥ ë¶„í¬ì— ê´€í•œ ì´ì•¼ê¸°ëŠ” ì‘ë…„ 1í•™ê¸°ì— ë“¤ì—ˆë˜ í™•ë¥ ë³€ìˆ˜ë¡ ì—ì„œ ìì„¸í•˜ê²Œ ë‹¤ë£¨ì—ˆë‹¤ ë¬¼ë¡  focusê°€ ì „ì „ì€ ì¡°ê¸ˆ ë‹¤ë¥´ì§€ë§Œ í•™ë¬¸ì˜ ê¸°ì´ˆì ì¸ baseëŠ” ë¹„ìŠ·í•˜ê¸° ë•Œë¬¸ì— í•œë²ˆ ë´ì•¼ê² ë‹¤ ì¡°ê±´ë¶„ y = 1ì¼ë•Œë§Œ counting í•´ì£¼ì–´ ê°ê°ì˜ xê°’ì— ëŒ€í•œ í™•ë¥ ë¶„í¬ ì´ê²Œ ë°”ë¡œ ì¡°ê±´ë¶€ í™•ë¥ ë¶„í¬ ì´ê±´ ì…ë ¥xì— ëŒ€í•œ yì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. ì¡°ê±´ë¶€í™•ë¥ ê³¼ ê¸°ê³„í•™ìŠµì¡°ê±´ë¶€ í™”ë¥  P(y|x)ëŠ” ì…ë ¥ë³€ìˆ˜ xì— ëŒ€í•œ ì •ë‹µì´ yì¼ í™•ë¥  ì—°ì† í™•ë¥ ë¶„í¬ì¼ë•ŒëŠ” í™•ë¥ ì´ ì•„ë‹ˆë¼ ë°€ë„ì„ logistic regressionì—ì„œ ì‚¬ìš©í–ˆë˜ ì„ í˜•ëª¨ë¸ê³¼ softmaxì˜ ê²°í•©ì€ ë°ì´í„°ì—ì„œ ì¶”ì¶œëœ íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ í™•ë¥ ì„ í•´ì„ ë¶„ë¥˜ë¬¸ì œì—ì„œ softmax(W$\\phi$ + b)ëŠ” ë°ì´í„° xë¡œë¶€í„° ì¶”ì¶œëœ íŠ¹ì§•íŒ¨í„´ $\\phi$ (x)ì™€ ê°€ì¤‘ì¹˜ Wë¥¼ í†µí•´ ì¡°ê±´ë¶€ í™•ë¥  P(y|x)ë¥¼ ê³„ì‚°í•œë‹¤ íšŒê·€ë¬¸ì œì˜ ê²½ìš° ë°€ë„í•¨ìˆ˜ë¼ ì¡°ê±´ë¶€ê¸°ëŒ€ê°’ E[y|x]ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤ ì¡°ê±´ë¶€ í™•ë¥ ë°€ë„ í•¨ìˆ˜ì— ì—°ì†ì´ë¼ ì ë¶„ ì´ì‚°í™•ë¥ ë¶„í¬ë©´ sumì˜ í˜•íƒœë¡œ ì¡°ê±´ë¶€ ê¸°ëŒ“ê°’ì€ L2-normì„ ìµœì†Œí™” í•˜ëŠ” í•¨ìˆ˜ì™€ ì¼ì¹˜í•œë‹¤!!!! -&gt; ì´ê±´ ì´ë¯¸ ì¦ëª…ì´ ë˜ì–´ìˆëŠ”ê²ƒ (ì¦ëª…ê³¼ì •ì´ í•„ìš”í• ê¹Œ?) ì˜ˆì¸¡ì˜ ì˜¤ì°¨ì˜ ë¶„ì‚°ì„ ìµœì†Œí™”í•˜ëŠ” -&gt; ì¡°ê±´ë¶€ ê¸°ëŒ“ê°’ í•­ìƒì€ ì•„ë‹ˆì§€ë§Œ ì‚¬ìš©í•¨ Expectationë°ì´í„°ë¥¼ ëŒ€í‘œí•˜ëŠ” í†µê³„ëŸ‰ ë‹¤ë¥¸ í†µê³„ì  í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ”ë° ì´ìš©í•˜ëŠ” ê¸°ëŒ“ê°’ í•™êµì—ì„œ ë°°ì› ë˜ ìˆ˜ì‹ê³¼ ì—¬ê¸°ì„œì˜ ìˆ˜ì‹ì„ ë¹„êµí•´ë³´ì ì—°ì†í™•ë¥ ë³€ìˆ˜ : ë°€ë„í•¨ìˆ˜ ì´ì‚°í™•ë¥  : ì§ˆëŸ‰í•¨ìˆ˜ ë”¥ëŸ¬ë‹ì€ ë‹¤ì¸µì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¡œë¶€í„° íŠ¹ì§•íŒ¨í„´ì„ ì¶”ì¶œí•©ë‹ˆë‹¤ ì´íŠ¹ì§• íŒ¨í„´ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ì–´ë–¤ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì‚¬ìš©í• ì§€ëŠ” ê¸°ê³„í•™ìŠµì˜ ë¬¸ì œì™€ ëª¨ë¸ì— ì˜í•´ ê²°ì •ëœë‹¤ Montecarlo Samplingì´ê±´ ê°•í™”í•™ìŠµì—ì„œ ë§¤ìš° ë§ì´ ì¨ë³´ì•˜ë‹¤ í™•ë¥ ë¶„í¬ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¥¼ë•Œì—ëŠ” ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ê¸°ëŒ€ê°’ì„ ê³„ì‚° ëª¬í…Œì¹´ë¥¼ë¡œ ìƒ˜í”Œë§ ë°©ë²•ì„ ì‚¬ìš©í•´ì•¼í•œë‹¤ ëª¬í…Œì¹´ë¥¼ë¡œ ì˜ˆì œ êµ¬ê°„ì˜ ê¸¸ì´ê°€ 2ì´ë¯€ë¡œ ì ë¶„ê°’ì„ 2ë¡œë‚˜ëˆ„ë©´ ê²°êµ­ ê¸°ëŒ“ê°’ì„ ê³„ì‚°í•˜ëŠ”ê²ƒì´ë¯€ë¡œ ëª¬í…Œì¹´ë¥¼ë¡œ ì‚¬ìš© ë¬¸ì œëŠ” ìƒ˜í”Œì‚¬ì´ì¦ˆê°€ ë§ì•„ì•¼ ì˜¤ì°¨ë²”ìœ„ê°€ ì¤„ì–´ë“ ë‹¤","link":"/2021/01/28/2021-01-28-Boostcamp9/"},{"title":"Week2","text":"week2 ê°„ë‹¨ ì •ë¦¬ ê°€ë³´ì ê°€ë³´ì~ ì„ í˜•íšŒê·€ë¶„ì„ ì—¬ê¸°ì„œ ì¤‘ìš”í•œê±´ ë°ì´í„°ë“¤ì„ í•˜ë‚˜ì˜ ì§ì„ ìœ¼ë¡œ ê·¼ì‚¬í•˜ëŠ”ê²ƒì´ë‹¤ ì •ë‹µ ì§ì„  -&gt; y ë°ì´í„°ë“¤ì˜ - &gt; x ê²°êµ­ ì´ê±°ê±°ë“± xë¥¼ yì— ê·¼ì‚¬ì‹œí‚¤ëŠ”ê±° ë² íƒ€ë¥¼ êµ¬í•˜ëŠ”ê²Œ ì„ í˜•íšŒê·€ë¶„ì„ì´ì§€ ë§Œì•½ n&gt;=m ì´ê±´ ê²°êµ­ í–‰ì´ë” í¬ë‹¤ -&gt; ì‹ì˜ ê°¯ìˆ˜ê°€ ë³€ìˆ˜ì˜ ê°¯ìˆ˜ë³´ë‹¤ ë§ë‹¤ ì´ëŸ¼ ì—°ë°© í‘¸ëŠ”ê²Œ ë¶ˆê°€ëŠ¥ í•˜ì§€ ë”°ë¼ì„œ yì— ê·¼ì ‘í•˜ëŠ” yâ€™ì„ ì°¾ëŠ”ê±°ë‹¤ ë”°ë¼ì„œ !!!!! ì •ë‹µ yì™€ yâ€™ì˜ ì°¨ì´ê°€ìµœì†Œí™” ë˜ê²Œ -&gt; L2 normì„ ì´ìš©í•´ì„œ L2 normì„ ìµœì†Œí™”í•˜ëŠ” Bë¥¼ ì°¾ëŠ”ê²ƒ ì´ë•Œ ë¬´ì–´ íœë¡œì¦ˆ ì—­í–‰ë ¬ ì‚¬ìš© í•´ì„œ Bë¥¼ êµ¬í•œë‹¤ í•˜í•˜í•˜ ì´ì„ í˜• íšŒê·€ë¶„ì„ì´ë‘ ê²½ì‚¬í•˜ê°•ë²•ì´ë‘ ë¬´ìŠ¨ìƒê´€??????? ë¬´ì–´íœë¡œì¦ˆ ì‚¬ìš©í•˜ê¸° ì‹œëŸ¬ì„œ ê²½ì‚¬í•˜ê°•ë²• ì“°ëŠ”ê±°ì§€ìš” ì™œ ì‹œë¥¼ê¹Œ? Reason : ì§€ê¸ˆì€ ì„ í˜•ë§Œ í•˜ê³ ìˆì–´ì„œ ë¬´ì–´ë‘ ê²½ì‚¬ë‘ ìƒê´€ì—†ëŠ”ë° ì´ì œ ë§ì€ ì‹¤ì œ ëª¨ë¸ë“¤ì€ ë¹„ì„ í˜•ì´ë‹¤. ë”°ë¼ì„œ ë¹„ì„ í˜•ì—ì„œë„ ì‚¬ìš© í•  ìˆ˜ ìˆëŠ” ê²½ì‚¬í•˜ê°•ë²•ì„ ì‚¬ìš©í•˜ëŠ”ê²ƒ!!!!!!!! ê²½ì‚¬í•˜ê°•ë²• ê²°êµ­ ê°€ì¥ì¤‘ìš”í•œê±´ gradient vectorì´ë‹¤ ë³€ìˆ˜ê°€ vectorì¸ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ê²½ìš° í¸ë¯¸ë¶„ì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ gradient ì—¬ê¸°ì„œ normì€ ì ˆëŒ€ê°’ì„ ëŒ€ì²´í•˜ëŠ” ê²ƒ eps : ì¢…ë£Œì¡°ê±´ ì™œëƒ? ì™„ì „ì´ gradê°€ 0ì´ë˜ëŠ”ê±´ ì»´ì´ë¼ ë¶ˆê°€ëŠ¥í•¨ var = init grad = gradient(var) while (norm(grad)&gt;eps): â€‹ var = var-lr*grad â€‹ grad = gradient(var) ì´ì œì¢€ gradient descentì˜ í°ê·¸ë¦¼ì´ ë³´ì¸ë‹¤ ê²°êµ­ì€ ì–´ëŠë°©í–¥ìœ¼ë¡œ ì›€ì§ì—¬ì•¼ í•¨ìˆ˜ê°’ì´ ì¦ê°€í•˜ëŠ”ì§€ ê°ì†Œí•˜ëŠ”ì§€ë¥¼ ì•Œë ¤ì£¼ëŠ”ê²Œ gradientë¡œ gradientë¥¼ ê³„ì†í•´ì„œ updateí•˜ì—¬ normì´ 0ì— ê°€ê¹Œìš´ ê°’ì´ë©´ ì´ì œ ìµœì†Ÿê°’ì„ ë³´ì¥í•´ì£¼ëŠ” ì´ëŸ° ëŠë‚Œì ì¸ ëŠë‚Œ ã…‡ã…‹ ê·¼ë° ì´ì œ ë¬¸ì œëŠ” ì´ë¥¼ í–‰ë ¬ì„ ì´ìš©í•œ ìˆ˜ì‹ìœ¼ë¡œ ì¦ëª…í•˜ê³  ì „ê°œí•˜ëŠ”ê²ƒì´ë‹¤ ìš”ê²Œ ì´ì œ ì–´ë ¤ìš´ Point ì´ê±´ ì†ìœ¼ë¡œ ì‘ì„±í•˜ê² ë‹¤. Further Question ê°•ì˜ì˜ìƒ 03:47ë¶€í„° ì†Œê°œë˜ëŠ” ë‚´ìš©ì¸, d-ì°¨ì› ë²¡í„°(ë² íƒ€)ì— ëŒ€í•œ ê·¸ë ˆë””ì–¸íŠ¸ ë²¡í„°ë¥¼ êµ¬í•˜ëŠ” ê³„ì‚°ì„ ê°ì ì§ì ‘ ì†ìœ¼ë¡œ í•´ë³´ê¸° ë°”ëë‹ˆë‹¤! ê·¸ëŸ¼ ì´ê²ƒë„ ì†ìœ¼ë¡œ ê³„ì‚°í•´ì„œ update (with ipad) ë”¥ëŸ¬ë‹ í•™ìŠµë°©ë²• ì´í•´í•˜ê¸°ë”¥ëŸ¬ë‹ì€ ë¹„ì„ í˜• ëª¨ë¸ì„ í•´ì„í•˜ëŠ” ê²ƒì´ë‹¤ ê·¸ ì „ì— ë¨¼ì € ì„ í˜•ëª¨ë¸ì„ ë‹¤ë£¨ì–´ ë³´ì ì‹ ê²½ë§ì„ ìˆ˜ì‹ìœ¼ë¡œ ë¶„í•´ ì´ê²Œ ê°€ì¥ ì¤‘ìš”í•œ ìˆ˜ì‹ ë‹¤ì‹œ í–‰ë ¬ reviewë¥¼ í•´ë³´ë©´ XëŠ” ë°ì´í„°ë“¤ì˜ ì§‘í•© xiëŠ” ië²ˆì§¸ data xijëŠ” ië²ˆì¨° dataì˜ jë²ˆì§¸ ë³€ìˆ˜ê°’ ë‹¤ì‹œë§í•´ nxd ì¸ Xí–‰ë ¬ì€ nê°œì˜ dataë¥¼ ê°€ì§€ê³  ê°ë°ì´í„°ëŠ” dê°œì˜ ë³€ìˆ˜ë¥¼ê°€ì§. WëŠ” dataë¥¼ ë‹¤ë¥¸ ì°¨ì›ì— mappingí•´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤. ë°ì´í„°ì— ê°€ì¤‘ì¹˜ë¥¼ ê³±í•˜ê³  biasë¥¼ ë”í•´ì£¼ëŠ” ì´ëŸ°ëŠë‚Œ Softmax ë¶„ë¥˜ë¬¸ì œë¥¼ í’€ë•Œ ì„ í˜•ëª¨ë¸ì˜ ì¶œë ¥ê³¼ softmaxì˜ ê²°í•©ìœ¼ë¡œ íŠ¹ì •ë²¡í„°ê°€ ì–´ëŠ í´ë˜ìŠ¤ì— ì†í•˜ëŠ”ì§€ ì•Œ ìˆ˜ ìˆìŒ ë²¡í„°ë¥¼ í™•ë¥ ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤ â€‹ â€‹ â€‹ â€‹ ìœ„ì™€ ê°™ì´ ìˆœì°¨ì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ì™€ biasë¥¼ ê³„ì‚°í•œ ê°’ì„ ë‹¤ìŒë…¸ë“œì— ë˜ ì§‘ì–´ë„£ê³  ê³„ì†í•´ì„œ ìˆœë°©í–¥ìœ¼ë¡œ ì „íŒŒ -&gt; forward propagation í•™ìŠµí• ë•ŒëŠ” back propagationì„ ì‚¬ìš© â€‹ â€‹ Back Propagationâ€‹ ì„ í˜•íšŒê·€ë¶„ì„ì—ì„œ Bì— í•´ë‹¹í•˜ëŠ” gradientë²¡í„°ë¥¼ ê³„ì‚°í•´ì„œ ì—…ë°ì´íŠ¸ í–ˆë˜ê²ƒì²˜ëŸ¼ ê°ì¸µì— ì¡´ì¬í•˜ëŠ” parameterì˜ ë¯¸ë¶„ê°’ì„ ê³„ì‚°í•´ì„œ ì—…ë°ì´íŠ¸ ê°ì¸µì˜ ê°€ì¤‘ì¹˜ì— ëŒ€í•œ gradientë²¡í„°ë¥¼ ê³„ì‚°í•˜ëŠ” ë²• í–‰ë ¬ë“¤ì˜ ì›ì†Œì˜ ëª¨ë“  ê°œìˆ˜ë§Œí¼ ê²½ì‚¬í•˜ê°•ë²•ì´ ì ìš©ì´ ëœë‹¤ í•œì¸µì¼ë•ŒëŠ” ëª©ì ì‹ì— ëŒ€í•œ gradient ë²¡í„°ë¥¼ ë™ì‹œì— ê³„ì‚°í•  ìˆ˜ ìˆì§€ë§Œ, ë”¥ëŸ¬ë‹ì˜ ê²½ìš° gradient ë²¡í„°ë¥¼ ìˆœì°¨ì (back propagation)ìœ¼ë¡œ ê³„ì‚°í•˜ê²Œëœë‹¤ â€‹ â€‹ ì†ì‹¤í•¨ìˆ˜ â€“&gt; Lì¼ë–„ ìš°ì¸¡ì„ ê³„ì‚° ìœ„ì¸µì— ìˆëŠ” gradientë¥¼ ê³„ì‚°í•œ ë‹¤ìŒì— chain ruleì„ ì´ìš©í•´ ì•„ë˜ì¸µì˜ gradientë¥¼ ê³„ì‚° â€‹ ì´ëŸ¬í•˜ê²Œ ê³„ì†í•´ì„œ ê° ê°€ì¤‘ì¹˜ì— ëŒ€í•œ gradient vectorë¥¼ êµ¬í•œë’¤ ì´ë“¤ì„ SGDë¥¼ ì‚¬ìš©í•´ì„œ Loss functionì„ updateì‹œí‚¨ë‹¤ í™•ë¥ ë¡  ë§›ë³´ê¸° íšŒê·€ë¶„ì„ì—ì„œ ì†ì‹¤í•¨ìˆ˜ë¡œ ì‚¬ìš©ë˜ëŠ” L2 normì€ ì˜ˆì¸¡ì˜¤ì°¨ì˜ ë¶„ì‚°ì„ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•˜ë„ë¡ ìœ ë„ Cross-EntropyëŠ” ëª¨ë¸ì˜ˆì¸¡ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ìµœì†Œí™” í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•˜ë„ë¡ ìœ ë„ ë¶„ì‚° ë° ë¶ˆí™•ì‹¤ì„±ì„ ìµœì†Œí™” í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ì•¼ í•¨ ì´ì‚°í™•ë¥ ë³€ìˆ˜ VS ì—°ì†í™•ë¥ ë³€ìˆ˜ìœ„ì˜ ê°œë…ì´ ê²°êµ­ì€ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ ì´ì‚°ì€ ê²½ìš°ì˜ìˆ˜ë¥¼ ë‹¤ ê³ ë ¤í•´ì„œ í™•ë¥ ì„ êµ¬í•¨ ì—°ì†ì€ ë°€ë„í•¨ìˆ˜ë¥¼ ì ë¶„í•´ì„œ í™•ë¥ ì„ êµ¬í•¨ ì¡°ê±´ë¶€ í™•ë£° ë³€ìˆ˜","link":"/2021/01/29/2021-01-29-week2/"},{"title":"Numpy &amp; Basic Math1","text":"Start Numpyí° matrixë¥¼ íŒŒì´ì¬ìœ¼ë¡œ í‘œí˜„í•˜ê¸° ìœ„í•´ ì‚¬ìš© ê·¸ë¦¬ê³  memoryë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë°˜ë³µë¬¸ ì—†ì´ ë°ì´í„° ë°°ì—´ì²˜ë¦¬ë¥¼ ì§€ì›í•¨ ndarrayimport numpy as np test_array = np.array([1,2,3,4],float) ndarry ì™€ list ì˜ ì°¨ì´ì  numpyëŠ” í•˜ë‚˜ì˜ ë°ì´í„° typeë§Œ ë°°ì—´ì— ë“¤ì–´ê°ˆìˆ˜ ìˆë‹¤ dynamic typingì´ ì•ˆë¨ numpyëŠ” ì°¨ë¡€ëŒ€ë¡œ ë°ì´í„°ê°€ ë°”ë¡œ ë©”ëª¨ë¦¬ì— í• ë‹¹ë˜ì§€ë§Œ pythonì˜ listëŠ” ì£¼ì†Œê°’ì„ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ê²Œ ë˜ì–´ 2ë²ˆì„ ê±°ì³ì„œ ê°’ì„ ë¶ˆëŸ¬ì˜´ ë”°ë¼ì„œ ê°’ì´ ndarrayì˜ ê°’ì´ ê°™ë”ë¼ë„ is ë¥¼ ì¨ë³´ë©´ falseê°€ ë‚˜ì˜¨ë‹¤ listëŠ” ì•„ë‹˜ arrayì˜ rankrank 0 : scalar rank 1 : vector rank 2 : matrix rank 3 : 3- tensor rank 4 : n-tensor dtype : numpyëŠ” nbytesndarrayì˜ object ë©”ëª¨ë¦¬ í¬ê¸°ë¥¼ ë°˜í™˜í•¨ ShapereshapeArrayì˜ shape í¬ê¸°ë¥¼ ë³€ê²½í•¨, elementì˜ ê°¯ìˆ˜ëŠ” ë™ì¼ 12345import numpy as nptest_matrix = [[1,2,3,4],[1,2,3,4]]np.array(test_metrix).reshape(2,4)np.array(test_metrix).reshape(-1,2).shape# (4,2) flattenë‹¤ì°¨ì› arrayë¥¼ 1ì°¨ì› arrayë¡œ ë³€í™˜ Indexingë¦¬ìŠ¤íŠ¸ì™€ ë‹¬ë¦¬ [0,0]ê³¼ ê°™ì€ ì ‘ê·¼ë²•ì„ í—ˆìš©í•œë‹¤ Slicingdataë¥¼ ì ‘ê·¼ í•  ë•Œ slicingì„ êµ‰ì¥íˆ ë§ì´ ì“´ë‹¤ ì´ê±´ ë§ì´ ì¨ë´ì„œ ã„±ã…Š 123#ê±´ë„ˆ ë›°ëŠ”ê²ƒë„ ì”€arr[:,::2]#2ì¹¸ì”© ê±´ë„ˆ ë›°ëŠ” Creation Function123456np.arange(0, 10, 0.5)#array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. ,6.5, 7. , 7.5, 8. , 8.5, 9. , 9.5])np.zeros(shape = (10,),dtype =np.int8)np.empty(shape = (10,),dtype =np.int8)#emptyí•¨ìˆ˜ëŠ” memoryì— ë¹ˆê³µê°„ì´ ì¡í˜ ê·¸ì•ˆì—ëŠ” ì“°ë ˆê¸° ê°’ì´ ìˆìŒ identityë‹¨ìœ„í–‰ë ¬ (ií–‰ë ¬)ì„ ìƒì„±í•¨ eyeëŒ€ê°ì„ ì´ 1ì¸í–‰ë ¬, kê°’ì˜ ì‹œì‘ index ì§€ì •ê°€ëŠ¥ diagëŒ€ê°í–‰ë ¬ ê°’ì„ ì¶”ì¶œí•¨ random samplingë°ì´í„° ë¶„í¬ì— ë”°ë¥¸ samplingìœ¼ë¡œ arrayë¥¼ ìƒì„± np.random Operation functionAxisê¸°ì¤€ì´ë˜ëŠ” dimension ì¶• 12345678test_array = np.arrange(1,13).reshape(3,4)'''array([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12]])'''test_array.sum(axis=1)#(array([10,26,42])) Concatenate numpy arrayë¥¼ ë¶™ì´ëŠ” í•¨ìˆ˜ np.concatenate((a,b),axis = 0) ê°’ì„ ë¶™ì¼ë•Œ 2ê°œ í•¨ìˆ˜ concatenate,hstack,vstack ë§ì¶°ì„œ ì¶•ì„ ëŠ˜ë¦¬ê³  ì‹¶ì€ë•Œ newaxis ì‚¬ìš© ì´ê²Œ ìƒê°ë³´ë‹¤ ìµìˆ™í•˜ì§€ ì•Šì•„ì„œ ì—°ìŠµì´ ì¢€ í•„ìš”í• ë“¯ ì‹¶ë‹¤ í–‰ë ¬ì˜ dot product a.dot(test_b) Transpose test_a.T Broadcasting(element wise operation : arrayì˜ shapeì´ ê°™ì•„ì„œ ê°ê° ì—°ì‚°) Broadcastingì€ í¬ê¸°ê°€ shapeì´ ë‹¤ë¥¼ë•Œ ì•Œì•„ì„œ í¼ì ¸ì„œ ì—°ì‚°ì´ ì¼ì–´ë‚¨ ìš”ëŸ°ëŠë‚Œ 1234567891011test_matrix = np.arange(1, 13).reshape(4, 3)test_vector = np.arange(10, 40, 10)array([10, 20, 30])test_vector.reshape(-1, 3).T + test_vector'''array([[20, 30, 40], [30, 40, 50], [40, 50, 60]])'''# ì´ëŸ° ê³„ì‚°ë„ ê°€ëŠ¥í•¨ Numpy performance ì—°ì‚°ì˜ ì†ë„ëŠ” listë³´ë‹¤ í›¨ì”¬ í¬ë‹¤ (dynamic typingì„ í¬ê¸°í•œ ê²°ê³¼) í•˜ì§€ë§Œ í• ë‹¹ì—ì„œëŠ” ì—°ì‚°ì†ë„ì˜ ì´ì ì´ ì—†ìŒ Comparison123456789101112131415161718192021222324252627282930import numpy as npa = np.arrange(10)a&lt;4'''array([ True, True, True, True, False, False, False, False, False, False])'''np.any(a&gt;5)# í•˜ë‚˜ë¼ë„ trueë©´ ì „ì²´ê°€ Trueë‹¤np.all(a&gt;5)#í•˜ë‚˜ë¼ë„ falseë©´ ì „ì²´ê°€ falseì´ë‹¤#numpy ë°°ì—´ê°„ì˜ shapeì´ ë™ì¼í• ë•ŒëŠ” elementê°„ì˜ í¬ê¸°ë¥¼ ë¹„êµnp.logical_and(a&gt;0,a&lt;3)#ì¡°ê±´ì˜ andnp.logical_not(b)np.logical_or(a&gt;0,a&lt;3)# np.wherenp.where(a &gt; 5)#(array([6, 7, 8, 9], dtype=int64),np.where(a&gt;5,3,2)# array([2, 2, 2, 2, 2, 2, 3, 3, 3, 3])# argmax : ê°€ì¥ í°ê°’ì˜ index ë°˜í™˜, argmin : ê°€ì¥ ì‘ì€ê°’ì˜ indexë°˜í™˜a = np.array([1,2,3],[4,5,6],[7,8,9])a.argmax(axis = 0) Boolean Indexindexì— ì¡°ê±´ì„ ë„£ì–´ì£¼ì–´ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” í•´ë‹¹í•˜ëŠ” ê°’ë§Œ ë½‘ì•„ì˜´ 123456import numpy as npa = np.arange(10)condition = a&gt;5a[condition]# array([6, 7, 8, 9]) Fancy indexindexê°’ì„ ë„£ì–´ì¤Œ 123456789101112a = np.array([2, 4, 6, 8], float)cond1 = np.array([1, 1, 1, 2, 1, 1, 1, 3])cond2 = np.array([1, 1, 1, 2, 1, 1, 1, 2])a.take(cond1)#array([4., 4., 4., 6., 4., 4., 4., 8.])a[cond]#array([4., 4., 4., 6., 4., 4., 4., 8.])#ë‹¤ì°¨ì› ë¦¬ìŠ¤íŠ¸ì— ì ìš©a = np.array([[1,2,3,4],[3,4,2,2],[0,0,0,0],[1,1,2,1]])a[cond1,cond2]#array([4, 4, 4, 0, 4, 4, 4, 2]) VectorWhat is vector ë²¡í„°ëŠ” ìˆ«ìë¥¼ ì›ì†Œë¡œ ê°€ì§€ëŠ” ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë°°ì—´ ê³µê°„ì—ì„œ í•œì ì„ ë‚˜íƒ€ë‚¸ë‹¤ (ì›ì ìœ¼ë¡œ ë¶€í„° ìƒëŒ€ì  ìœ„ì¹˜) Hadamard product (element product)ì„±ë¶„ê³±ì´ë€ ê°™ì€ shapeì—ì„œì˜ ê° elementë¼ë¦¬ì˜ ê³± Vectorì˜ ë§ì…ˆ &amp; ëº„ì…ˆì›ì ìœ¼ë¡œ ë¶€í„°ê°€ ì•„ë‹Œ ë‹¤ë¥¸ ë²¡í„°ë¡œ ë¶€í„° ìƒëŒ€ì  ìœ„ì¹˜ì´ë™ ëº„ì…ˆì€ ë°©í–¥ì„ ë’¤ì§‘ì€ ë§ì…ˆ ë²¡í„°ì˜ norm ì–´ë– í•œ ë²¡í„°ì˜ ì›ì ì—ì„œë¶€í„°ì˜ ê±°ë¦¬ ê¸°í˜¸ ë°‘ í‘œí˜„ normì€ ì„ì˜ì˜ ì°¨ì› dì—ì„œ ì„±ë¦½ L1 normê°ì„±ë¶„ì˜ ë³€í™”ëŸ‰ì˜ ì ˆëŒ€ê°’ ì¢Œí‘œëª…ë©´ì—ì„œ ê°ì¢Œí‘œì¶•ì„ ë”°ë¼ ì´ë™í•˜ëŠ” ê±°ë¦¬ì˜ í•© : L2 norm**í”¼íƒ€ê³ ë¼ìŠ¤ ì •ë¦¬ë¥¼ ì´ìš©í•´ ìœ í´ë¦¬ë“œ ê±°ë¦¬ë¥¼ ê³„ì‚° ** : ì™œ ë‹¤ë¥¸ normì„ ì‚¬ìš©í•˜ë‚˜? normì˜ ì¢…ë¥˜ì— ë”°ë¼ ê¸°í•˜í•™ì  ì„±ì§ˆë“¤ì´ ë‹¬ë¼ì§„ë‹¤ ì›ì˜ ì •ì˜ : ì›ì ì—ì„œ ë¶€í„° ê±°ë¦¬ê°€ r ì¸ ì ë“¤ì˜ ì§‘í•© normì„ ì‚¬ìš©í•˜ì—¬ ë‘ë²¡í„° ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ êµ¬í•˜ì L1 normì„ ì‚¬ìš©í•˜ì—¬ êµ¬í•œ ë²¡í„°ì‚¬ì´ì˜ ê±°ë¦¬ L2 normì„ ì‚¬ìš©í•˜ì—¬ êµ¬í•œ ë²¡í„°ì‚¬ì´ì˜ ê°ë„ í–‰ë ¬í–‰ë ¬ì´ë€ í–‰ë²¡í„°ë¥¼ ì›ì†Œë¡œ ê°€ì§€ëŠ” ì´ì°¨ì› ë°°ì—´ ì „ì¹˜í–‰ë ¬ í–‰ë ¬ì˜ ì´í•´ë²¡í„°ê°€ ê³µê°„ì—ì„œ í•œì ì„ ì˜ë¯¸í•œë‹¤ë©´ í–‰ë ¬ì€ ì—¬ëŸ¬ì ë“¤ì„ ë‚˜íƒ€ë‚¸ë‹¤ í–‰ë ¬ì˜ í–‰ë²¡í„° xi ëŠ” ië²ˆì§¸ dataë¥¼ ì˜ë¯¸í•œë‹¤ xijëŠ” xë²ˆì§¸ dataì˜ j ë²ˆì§¸ ë³€ìˆ˜ë¥¼ ì˜ë¯¸ í–‰ë ¬ì˜ ê³±ì…ˆië²ˆì§¸ í–‰ë²¡í„°ì™€ j ë²ˆì§¸ ì—´ë²¡í„°ì˜ ë‚´ì ì„ ì„±ë¶„ìœ¼ë¡œ ê°€ì§€ëŠ” í–‰ë ¬ì„ ê³„ì‚°í•œë‹¤ numpyì—ì„œëŠ” @ ì—°ì‚°ì„ í™œìš©í•œë‹¤ í–‰ë ¬ë„ ë‚´ì ì´ ìˆì„ê¹Œë„˜íŒŒì´ì˜ np.innerì€ ië²ˆì§¸ í–‰ë²¡í„°ì™€ jì§¸ í–‰ë²¡í„° ì‚¬ì´ì˜ ë‚´ì ì„ ì„±ë¶„ìœ¼ë¡œ ê°€ì§€ëŠ” í–‰ë ¬ì„ ê³„ì‚° ìˆ˜í•™ì—ì„œ ë§í•˜ëŠ” ë‚´ì ê³¼ ëŠ” ë‹¤ë¥¸ê°œë…ì´ë‹¤ í–‰ë ¬ì˜ ë‚´ì ì€ ë³´í†µ ë‘ í–‰ë ¬ì‚¬ì´ì˜ traceë¥¼ ê³„ì‚°í•˜ê²Œ ë˜ëŠ”ë° numpyëŠ” ë‹¤ë¥´ë‹¤ ì´ë ‡ê²Œ transposeì™€ì˜ í–‰ë ¬ê³±ì…ˆì´ numpyì—ì„œì˜ innerproductì´ë‹¤ í–‰ë ¬ì˜ ì´í•´ 2í–‰ë ¬ì€ ë²¡í„°ê³µê°„ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì—°ì‚°ì(operator)ë¡œ ì´í•´í•œë‹¤ ì„œë¡œë‹¤ë¥¸ dataë¥¼ ì—°ê²°ì‹œí‚¤ëŠ” ì—°ì‚°ì mì°¨ì› ê³µê°„ì˜ ì ì¸ xì™€ , nì°¨ì› ê³µê°„ì˜ ì ì¸ zë¥¼ ì—°ê²°í•´ì¤„ë•Œ xë¼ëŠ” ë²¡í„°ì— Aë¼ëŠ” í–‰ë ¬ê³¼ ê³±í•´ì£¼ê²Œ ë˜ë©´ ìƒˆë¡œìš´ Zë¼ëŠ” ì—´ë²¡í„°ê°€ ë‚˜ì˜¨ë‹¤ mappingí•˜ëŠ” ì—°ì‚°ì!! ì£¼ì–´ì§„ dataì—ì„œ patternì„ ì¶”ì¶œí•˜ê±°ë‚˜ dataì••ì¶•ì´ ê°€ëŠ¥í•˜ë‹¤!!!!! í–‰ë ¬ì„ ì‚¬ìš©í•˜ëŠ” ì—°ì‚°ì -&gt; Linear Transform(ì„ í˜•ë³€í™˜) ì—­í–‰ë ¬í–‰ê³¼ ì—´ì˜ ìˆ«ìê°€ ê°™ê³  detê°€ 0ì´ ì•„ë‹ˆì—¬ì•¼ í•œë‹¤ np.linalg.inv(x) ë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ í–‰ë ¬ì˜ í–‰ì˜ ê°¯ìˆ˜ê°€ ì—´ì˜ ìˆ«ìë³´ë‹¤ ë§ë‹¤ë©´ -&gt; ì£¼ì–´ì§„ í–‰ë ¬ì˜ ì—´ì˜ ê°¯ìˆ˜ê°€ í–‰ì˜ ê°¯ìˆ˜ë³´ë‹¤ ë§ë‹¤ë©´ -&gt; ìœ ì‚¬ì—­í–‰ë ¬ì„ ì‚¬ìš©í•˜ì—¬ í•´ì¤‘ 1ê°œë¥¼ ê³„ì‚° ì„ í˜•íšŒê·€ ë¶„ì„ (Linear regression)ë§Œì•½ ë³€ìˆ˜ì˜ ê°¯ìˆ˜ë³´ë‹¤ ì‹ì˜ ê°¯ìˆ˜ê°€ ë§ì€ê²½ìš° -&gt; ì„ í˜•íšŒê·€ë¡œ í‘¼ë‹¤ ìœ ì‚¬ì—­í–‰ë ¬ì„ êµ¬í•´ì„œ ì„ í˜•íšŒê·€ ë°©ì •ì‹ì„ êµ¬í•œë‹¤ ì—¬ëŸ¬ê°œì˜ ì ì„ í–‰ë ¬ë¡œ í‘œí˜„í›„ ê³„ìˆ˜ë²¡í„°ì¸ Î²ë¥¼ ê³±í•´ì£¼ê²Œ ë˜ë©´ í•˜ë‚˜ì˜ ì„ ìœ¼ë¡œ í‘œí˜„ëœë‹¤ ì–´ë– í•œ Î²ë¥¼ ì“¸ê¹Œë¥¼ êµ¬í•˜ëŠ” -&gt; ì„ í˜•íšŒê·€ ë¶„ì„ ë¯¸ì§€ìˆ˜ê°€ ë” ë§ê¸° ë•Œë¬¸ì— í•´ë¥¼ ì°¾ëŠ”ê²ƒì€ ë¶ˆê°€ëŠ¥ ë”°ë¼ì„œ ìµœëŒ€í•œ ê°€ê¹Œìš´ ì„ ì„ ì°¾ëŠ”ë‹¤ -&gt; L2 normì„ ì‚¬ìš©í•˜ì—¬ ì´ ê³„ìˆ˜ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ì„œëŠ” 2ê°€ì§€ì˜ ë°©ë²•ì´ ìˆë‹¤. ì´ë•Œ Moore-Penroseë¡œ êµ¬í•˜ë ¤ë©´ yì ˆí¸ê°’ì„ ì¶”ê°€í•´ì£¼ì–´ì•¼ í•œë‹¤. ê³µë¶€ë¥¼ ë§ˆì¹˜ê³  ê¶ê¸ˆí•œê²Œ ë„ˆë¬´ ë§ì•˜ë‹¤ íŠ¹íˆ ì„ í˜•ëŒ€ìˆ˜í•™ íŒŒíŠ¸ì—ì„œ ì´ë‹¤ ë¬´ì–´íœë¡œì¦ˆ ì—­í–‰ë ¬ì„ ì´ìš©í•˜ì—¬ í•´ë¥¼ í•˜ë‚˜ êµ¬í•˜ëŠ” ê³¼ì •ì—ì„œ ì´ë ‡ê²Œ ë‚˜ì™€ìˆëŠ”ë° Aê°€ R^n*m^ ì¸ ì°¨ì›ì—ì„œ nì€ ì‹ì˜ ê°¯ìˆ˜ mì€ ë¯¸ì§€ìˆ˜ì˜ ê°¯ìˆ˜ mì´ në³´ë‹¤ í¬ê¸° ë•Œë¬¸ì— ì´ ì¡°ê±´ì„ ë³´ë©´ ì™¼ìª½ì— ê³±í•´ì£¼ì–´ì•¼ í•­ë“±í–‰ë ¬ì´ ëœë‹¤ ê·¼ë° AxA+ê°€ xê°€ ë ìˆ˜ìˆë‚˜? xê°€ ë²¡í„°ë¼ ìë¦¬ë¥¼ ë°”ê¿” ê³±í•´ë„ ë˜ëŠ”ê±´ê°€ ë¬´ì–´ íœë¡œì¦ˆë¥¼ ë‹¤ì‹œí•œë²ˆ ìì„¸íˆ ë³´ë©´ ì¡°ê±´ì´ ìƒì„±ë˜ëŠ” ì´ìœ ëŠ” nê³¼ mì´ ë‹¤ë¥¼ë•Œ AA^T^ ì™€ A^T^A ì¤‘ í•˜ë‚˜ëŠ” ì—­í–‰ë ¬ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ ì´ ì¡°ê±´ì´ ë°”ë¡œ m&lt;nì´ë©´ AA^T^ ì˜ ì—­í–‰ë ¬ì´ ì¡´ì¬í•˜ì§€ ì•Šê³ , m&gt;nì´ë©´ A^T^Aì˜ ì—­í–‰ë ¬ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ ë”°ë¼ì„œ ìœ ì‚¬ì—­í–‰ë ¬ì´ ì¡°ê±´ì— ë”°ë¼ ì €ë ‡ê²Œ í‘œí˜„ë˜ëŠ”ê±´ ì•Œê² ëŠ”ë° A+Aë‚˜ AA+ ë¥¼ í•´ì£¼ì—ˆì„ë•Œ ì°¨ì›ì´ ë‹¤ë¥´ê²Œ ë‚˜ì˜¬ë¿ì´ì§€ ì´ê²Œ ì§„ì§œ I ê°€ í•˜ë‚˜ëŠ” ì•ˆë‚˜ì˜¤ë‚˜? ì»´í“¨í„°ë¡œ ëŒë ¤ë´ì•¼ê² ë‹¤ ê·¸ë¦¬ê³  ë’¤ì— linear regression ë¶€ë¶„ë„ ë„ˆë¬´ ê°„ë‹¨í•˜ê²Œ ì•Œë ¤ì£¼ì…”ì„œ ì¤‘ìš”í•œ íŒŒíŠ¸ì¸ë° ë” ë”°ë¡œ ì•Œì•„ë´ì•¼ê² ë‹¤.","link":"/2021/01/25/2021-01-25-Boostcamp6/"},{"title":"ë”¥ëŸ¬ë‹ ê¸°ì´ˆ","text":"ë² ì´ì¦ˆ í†µê³„í•™ ë°ì´í„°ê°€ ì¶”ê°€ë˜ì—ˆì„ë•Œ ì“°ëŠ” ì¸ê³¼ê´€ê³„ì— ëŒ€í•œ ì¶”ë¡ ë²• ì¡°ê±´ë¶€ í™•ë¥  ë² ì´ì¦ˆ ì •ë¦¬ëŠ” ê³§ ì¡°ê±´ë¶€ í™•ë¥ ì„ ì´ìš©í•˜ì—¬ ì •ë³´ë¥¼ ê°±ì‹ í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì¤ë‹ˆë‹¤ ê²°êµ­ ìš°ë¦¬ê°€ ì•Œê³  ì‹¶ì€ ê²ƒì€ Aë¼ëŠ” ìƒˆë¡œìš´ ì¶”ê°€ì ì¸ ì •ë³´ê°€ ë“¤ì–´ì™”ì„ë•Œ P(B)ë¡œ ë¶€í„° P(B l A)ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì„ ì œê³µí•œë‹¤ ì‹¤ì œë¡œ ê·¸ëŸ¼ ì–´ë””ì— ì“¸ê¹Œ D : data , $\\theta$ : ëª¨ìˆ˜ ì‚¬í›„í™•ë¥  : ë°ì´í„°ê°€ ì£¼ì–´ì ¸ ìˆì„ë•Œ $\\theta$ì— ëŒ€í•œ í™•ë¥  ì‚¬ì „í™•ë¥  : ë°ì´í„°ê°€ ì—†ëŠ” ìƒí™©ì—ì„œ ì‚¬ì „ì— ì£¼ì–´ì§„ $\\theta$ ì— ëŒ€í•œ í™•ë¥  Likelihood : í˜„ì¬ ì£¼ì–´ì§„ ëª¨ìˆ˜ì—ì„œ ì–´ë– í•œ Dataê°€ ê´€ì°°ë  í™•ë¥  Evidence : Data ìì²´ì˜ ë¶„í¬ ex) COVIDì˜ ë°œë³‘ë¥ ì´ 10%ë¡œ ì•Œë ¤ì ¸ ìˆì„ ë•Œ, ì´ ë°”ì´ëŸ¬ìŠ¤ì— ì‹¤ì œë¡œ ê±¸ë ¸ì„ë•Œ(ì¡°ê±´ë¶€) ê²€ì§„ë  í™•ë¥  : 99%, ì˜¤ê²€ì§„ í™•ë¥  : 1% ì´ ë•Œ, ì–´ë–¤ ì‚¬ëŒì´ ì–‘ì„±íŒì •ì¼ë•Œ ì •ë§ë¡œ ì´ì‚¬ëŒì´ ë°”ì´ëŸ¬ìŠ¤ì— ê°ì˜€ë˜ì—ˆì„ í™•ë¥  ë°œë³‘ë¥  : ì‚¬ì „í™•ë¥  ì‹¤ì œë¡œ ê±¸ë ¸ì„ í™•ë¥  : $\\theta$ ê²€ì§„ëœ ê²½ìš° : D ì‹¤ì œë¡œ ê±¸ë ¸ì„ ë•Œ ê²€ì§„ë  í™•ë¥  : P(D l $\\theta$) : Likelihood ê°€ëŠ¥ë„ì™€ ì‚¬ì „í™•ë¥ ì´ ì£¼ì–´ì ¸ ìˆìœ¼ë¯€ë¡œ, ì‚¬í›„í™•ë¥  ê³„ì‚°ê°€ëŠ¥ ê·¸ë ‡ë‹¤ë©´ Evidenceì˜ ê³„ì‚°ë²•ì€???????? ë”°ë¼ì„œ $\\theta$ë¥¼ ë¶€ì •í–ˆì„ë•Œì˜ likelihoodë„ ì•Œì•„ì•¼ ê³„ì‚°ì´ ê°€ëŠ¥í•˜ë‹¤ ì¡°ê±´ë¶€ í™•ë¥ ì˜ ì‹œê°í™” ë°ì´í„°ì˜ ì„±ê²©ì— ë”°ë¼ 1ì¢…ì˜¤ë¥˜ì„ ì¤„ì´ëƒ 2ì¢…ì˜¤ë¥˜ë¥¼ ì¤„ì´ëƒê°€ ë‹¬ë¼ì§„ë‹¤ Ex) ì˜ë£Œë¬¸ì œì˜ ê²½ìš° False Negative : ì§ˆë³‘ì´ ì•„ë‹ˆë‹¤ë¼ê³  í–ˆëŠ”ë° ì‹¤ì œë¡œ ì§ˆë³‘ì¼ ê²½ìš° ë”°ë¼ì„œ False Negativeì— ì‹ ê²½ì„ ì“´ë‹¤ ì‚¬ì „ í™•ë¥  : ì§ˆë³‘ì— ê±¸ë¦´ í™•ë¥ , ì•ˆê±¸ë¦´ í™•ë¥  ë¯¼ê°ë„ : ê±¸ë¦°ê±¸ ê±¸ë ¸ë‹¤ê³  íƒì§€ ì‹¤ì œë¡œ ê±¸ë ¸ì„ ë•Œ ê±¸ë ¸ë‹¤ê³  í•  í™•ë¥  : ë¯¼ê°ë„ ì‹¤ì œë¡œ ê±¸ë¦¬ì§€ ì•Šì•˜ì„ë•Œ ê±¸ë ¸ë‹¤ê³  í•  í™•ë¥  : ì˜¤íƒ ì–‘ì„±ì´ ë‚˜ì™”ì„ë•Œ ì§„ì§œ ì–‘ì„±ì¸ ê²½ìš° : True Positive ìŒì„±ì´ ë‚˜ì™”ì„ë•Œ ì§„ì§œ ìŒì„±ì¸ ê²½ìš° : True Negative ì–‘ì„±ì´ ë‚˜ì™”ì„ë•Œ ìŒì„±ì¸ ê²½ìš° : 1ì¢… ì˜¤ë¥˜ ìŒì„±ì´ ë‚˜ì™”ëŠ”ë° ì§ˆë³‘ì— ê±¸ë¦° ê²½ìš° : 2ì¢… ì˜¤ë¥˜ ì •ë°€ë„ : TP/(TP+FP) ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ í†µí•œ ì •ë³´ì˜ ê°±ì‹  ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ í†µí•´ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë“¤ì–´ì™”ì„ ê²½ìš° ì•ì„œ ê³„ì‚°í•œ ì‚¬í›„í™•ë¥ ì„ ì‚¬ì „í™•ë¥ ë¡œ ì‚¬ìš©í•˜ì—¬ ê°±ì‹ ëœ ì‚¬í›„í™•ë¥ ì„ ê³„ì‚°ê°€ëŠ¥ ì•ì„œ ì–‘ì„± íŒì •ì„ ë°›ì€ ì‚¬ëŒì´ 2ë²ˆì§¸ ê²€ì§„ì„ ë°›ì•˜ì„ ë•Œë„ ì–‘ì„±ì´ ë‚˜ì™”ì„ë•Œ ì§„ì§œ COVIDì— ê±¸ë ¸ì„ í™•ë¥ ì€?&gt;?????? ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ì‚¬í›„í™•ë¥ ì„ ì—°ì†ìœ¼ë¡œ ê³„ì‚°í•´ë³´ë©´ ì •ë°€ë„ê°€ í™”ì•„ì•„ì•… ì˜¬ë¼ê°„ë‹¤ ì´ê²Œ ë°”ë¡œ ë² ì´ì¦ˆ ì •ë¦¬ì˜ ê°•ì !! BUT!!! ì¡°ê±´ë¶€í™•ë¥ ë¡œ ì¸ê³¼ê´€ê³„ë¥¼ ì¶”ë¡ í•´ì„œëŠ” ì•ˆëœë‹¤!!!!!!!!!!! ì¸ê³¼ ê´€ê³„ëŠ” ë°ì´í„° ë¶„í¬ì˜ ë³€í™”ì— ê°•ê±´í•œ ì˜ˆì¸¡ëª¨í˜•ì„ ë§Œë“¤ ë•Œ ê³ ë ¤í•´ ì£¼ì–´ì•¼ í•œë‹¤ ì¸ê³¼ ê´€ê³„ë¥¼ ê³ ë ¤í–ˆì„ ì‹œì— ì˜ˆì¸¡ë„ëŠ” ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ ì¸ê³¼ ê´€ê³„ë¥¼ ì•Œì•„ë‚´ê¸° ìœ„í•´ì„œëŠ” ì¤‘ì²©ìš”ì¸(confounding factor)ì˜ íš¨ê³¼ë¥¼ ì œê±°í•˜ê³  ì›ì¸ì— í•´ë‹¹í•˜ëŠ” ë³€ìˆ˜ë§Œì˜ ì¸ê³¼ê´€ê³„ë¥¼ ê³„ì‚°í•´ì•¼ í•œë‹¤ ex) í‚¤ì™€ ì§€ëŠ¥ -&gt; ì—¬ê¸°ì— ë‚˜ì´ì— ë”°ë¥¸ íš¨ê³¼ë¥¼ ì œê±°í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ í‚¤ê°€ í´ìˆ˜ë¡ ì§€ëŠ¥ì´ ë†’ë‹¤ë¼ëŠ” ê²°ê³¼ê°€ ë‚˜ì˜¤ê²Œ ë¨ ì´ëŸ¬í•œ ë‚˜ì´ì™€ ê°™ì€ ì¤‘ì²©ìš”ì¸ì„ ì œê±°í•˜ëŠ” ê²ƒì´ Main Point Ex) ìœ„ì˜ ë¬¸ì œê°€ ë°”ë¡œ ì•„ì£¼ ìœ ëª…í•œ simpsons ì—­ì„¤ aì™€bì˜ ì¤‘ì²©íš¨ê³¼ë¥¼ ì œê±°í•´ì•¼ ë¨ Zì˜ ê°œì…ì„ ì œê±°í•˜ê¸° ìœ„í•´ ì¡°ì •(intervention)íš¨ê³¼ë¥¼ ì‚¬ìš©í•œë‹¤??? ì¡°ì •íš¨ê³¼ì— ëŒ€í•œ ê°œë…ì´ ì¡°ê¸ˆ í•„ìš”í• ë“¯ ì‹¶ë‹¤ Deep Learning ; Historical Reviewì‚¬ëŒì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ëŠ” ì¸ê³µì§€ëŠ¥ AI &gt; data based Machine Learning &gt; Deep Learning (NNì„ ì‚¬ìš©í•˜ëŠ”) Key components of Deep learning Data that model can learn Model how to transform data Loss Function that quantifies the badness of model Algorithm to adjust the parameters to minimize the Loss Function (ìµœì í™” ì•Œê³ ë¦¬ì¦˜) Historical Review 1. AlexNet 224*224ì˜ image data ë¶„ë¥˜ëŒ€íšŒì—ì„œ ì²˜ìŒìœ¼ë¡œ Deep Learningì„ ì‚¬ìš©í•˜ì—¬ 1ë“±í•¨ ì‹¤ì œì ìœ¼ë¡œ ë”¥ëŸ¬ë‹ì˜ ì„±ëŠ¥ì´ ì…ì¦ì´ ë˜ì—ˆë˜ ì‚¬ì‹¤ 2. DQN Q -Learningì˜ function estimationì— NNì„ ì¶”ê°€í•˜ì—¬ ë†’ì€ ì„±ëŠ¥ì„ ì´ëŒì–´ëƒ„ 3. Encoder/Decoder NLPì˜ trendê°€ ë§ì´ ë°”ë€Œì—ˆë‹¤ 4. Adam Optimizer ìš°ë¦¬ê°€ optimizerë¥¼ ì„ ì •í• ë•Œ Adamì„ ê·¸ëƒ¥ ì“°ëŠ” ì´ìœ ê°€ ìˆì„ê¹Œ? ê²°ê³¼ê°€ ì˜ë‚˜ì˜¤ëŠ” ì´ìœ ê°€ ìˆì„ê¹Œ? 5. GAN Generate Model : Networkê°€ dataë¥¼ ìƒì„±! 6. RESNET ì™œ ë”¥ëŸ¬ë‹ì´ëƒë¥¼ ì„¤ëª…í•´ì£¼ì—ˆë‹¤ Layerë¥¼ ë”ìš±ë” ê¹Šê²Œ ìŒ“ì„ìˆ˜ ìˆê²Œ ë§Œë“¤ì–´ì¤Œ 7. Transformer Attention is all you need!! ëª¨ë“  ê¸°ì¡´ì˜ RNNì„ ëŒ€ì²´í•˜ì˜€ê³ , ì´ì   CNNë„ ë„˜ë³´ê³  ìˆë‹¤ 8. BERT (fine tuned NLP model) ì¼ë°˜ì ì¸ ë‹¨ì–´ë“¤ë¡œ modelì„ trainí•œí›„ì— ë‚´ê°€ ì›í•˜ëŠ” ì†Œìˆ˜ì˜ dataì— fine tunning OPENAIì˜ GPT-3 9. Self Supervised Learning ì´ë¯¸ì§€ ë¶„ë¥˜ì™€ ê°™ì€ ë¶„ë¥˜ë¬¸ì œë¥¼ í’€ë•Œ í•œì •ëœ í•™ìŠµë°ì´í„°ë¡œ modelê³¼ loss funì„ ë°”ê¿”ê°€ëŠ”ê²Œ ì•„ë‹Œ í•™ìŠµë°ì´í„° ì™¸ì— ë¼ë²¨ì„ ëª¨ë¥´ëŠ” Unsupervised Learning í™œìš© SimCLR Neural Networks ë‹¨ìˆœíˆ Function approximateì´ë‹¤ gradient ascent ë¼ë©´ rewardë¥¼ í‚¤ìš°ë„ë¡ ! ì´ëŸ°ê²Œ ë˜ê² êµ¬ë§Œ step size $\\eta$ í–‰ë ¬ì„ ì°¾ê² ë‹¤ -&gt; ì„œë¡œë‹¤ë¥¸ ì°¨ì›ì˜ ì„ í˜•ë³€í™˜ì„ ì°¾ê² ë‹¤ ì´ë ‡ê²Œ ê·¸ì € ì—°ì†ëœ ì„ í˜•ë³€í™˜ì€ í•˜ë‚˜ì˜ ì„ í˜•ë³€í™˜ìœ¼ë¡œ í•©ì³ì§ˆìˆ˜ ìˆê¸° ë–„ë¬¸ì— ì¸µì„ ì—¬ëŸ¬ê°œ ìŒ“ëŠ” ì´ìœ ê°€ ì‚¬ë¼ì§„ë‹¤ ë”°ë¼ì„œ ì¸µ ì¤‘ê°„ì— activation functionì„ ë„£ì–´ì¤€ë‹¤ -&gt; nonlinear transform ì„ í˜•ê²°í•©ì˜ ë°˜ë³µì´ ì•„ë‹Œ nonlinear transformì˜ ê²°í•© ë” ë§ì€ í‘œí˜„ë ¥ì„ ê°€ì§€ê²Œ ë¨ ì¡´ì¬ì„±ì´ ì¤‘ìš”í•œê²Œ ì•„ë‹ˆë¼ í‘œí˜„ë ¥ì´ ì¤‘ìš”í•¨ MSEëŠ” ì œê³±ì´ë¼ data ì‚¬ì´ì— errorê°€ ê»´ìˆì„ë•Œ -&gt; ì „ì²´ì ì¸ NNì´ ë§ê°€ì§ˆìˆ˜ë„ ìˆìŒ ì´ë ‡ê²Œ í•­ìƒ ê°™ì€ Loss Functionì„ ì“°ëŠ”ê²Œ ì•„ë‹ˆë¼ ìƒí™©ì— ë§ê²Œ ë‹¤ë¥¸ ê°’ì— ë¹„í•´ì„œ ê·¸ê°’ì´ ë†’ê¸°ë§Œ í•˜ë©´ ê·¸ indexë¥¼ ë½‘ëŠ”ê²ƒ -&gt; ê·¸ë˜ì„œ ì´ê±¸ ìˆ˜í•™ì ìœ¼ë¡œ CEë¥¼ ì‚¬ìš©í•¨","link":"/2021/02/01/2021-02-01-Boostcamp11/"},{"title":"Optimization","text":"1. Optimization ìš©ì–´ë“¤ì˜ ëª…í™•í•œ ì •ë¦¬ê°€ í•„ìš” Concept of Optimization Generalization Under fitting vs Over fitting Cross Validation Bias-varience tradeoff Bootstraping Bagging and Boosting Generalization ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’íŒë‹¤? ì¼ë°˜í™”ë€ : Training errorê° 0 ì´ë¼ê³  í•´ì„œ Test errorê°€ 0ì¸ê²ƒì€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ì¢‹ì€ generalization : networkì˜ Test data ì„±ëŠ¥ì´ í•™ìŠµë°ì´í„°ì™€ ë¹„ìŠ·í•˜ê²Œ ë‚˜ì˜¨ë‹¤ Cross validation Training dataì—ì„œ validation dataë¥¼ ë‚˜ëˆ„ì–´ì„œ trainingëœ ëª¨ë¸ì´ validation data ê¸°ì¤€ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì˜ ë™ì‘í•˜ëŠ”ì§€ë¥¼ íŒë‹¨ ë‚˜ëˆ„ëŠ” ê¸°ì¤€?????? í•™ìŠµë°ì´í„°ê°€ ì ìœ¼ë©´ ì•ˆëœë‹¤ ë”°ë¼ì„œ Cross validationì„ ì”€ í•™ìŠµë°ì´í„°ë¥¼ Kê°œì”©ìœ¼ë¡œ ë‚˜ëˆ„ì–´ í•˜ë‚˜ì”© ë°”ê¾¸ì–´ê°€ë©° validation dataë¡œ ì„¤ì •í•˜ê³  trainingê³¼ validationì„ ë°˜ë³µì§„í–‰ Test dataëŠ” ì €ì–¼ëŒ€ model í•™ìŠµì— ì‚¬ìš©ë˜ì–´ì„œëŠ” ì•ˆëœë‹¤!! Bias-varience tradeoff í•™ìŠµë°ì´í„°ì— noiseê°€ ê»´ìˆì„ë•Œ Costë¥¼ minimizingí•˜ëŠ”ê²ƒì€ 3ê°œë¡œ decomposed ë ìˆ˜ìˆë‹¤ bias varience noise biasì™€ varienceëŠ” trade offê´€ê³„ì— ìˆë‹¤ Bootstrapping Any test or matric that uses random sampling with replacement í•™ìŠµ dataê°€ 100ê°œê°€ ìˆìœ¼ë©´ 80ê°œì”© randomìœ¼ë¡œ ë½‘ì•„ì„œ ëª¨ë¸ì„ ì—¬ëŸ¬ê°œë¥¼ ë§Œë“¤ì–´ì„œ í•˜ë‚˜ì˜ ì…ë ¥ì— ëŒ€í•œ consensusë¥¼ ë³´ê³  ëª¨ë¸ì„ ìˆ˜ì •í•˜ëŠ”ë²• Bagging &amp; Boosting Bagging (Bootstrapping aggregating) Muitiple models are being trained with bootstrapping í•™ìŠµ dataê°€ 100ê°œê°€ ìˆìœ¼ë©´ 80ê°œì”© randomìœ¼ë¡œ ë½‘ì•„ì„œ(bootstrapping) ëª¨ë¸ì„ ì—¬ëŸ¬ê°œë¥¼ ë§Œë“¤ì–´ì„œ í•˜ë‚˜ì˜ ì…ë ¥ì— ëŒ€í•œ consensusë¥¼ ë³´ê³  ëª¨ë¸ì„ ìˆ˜ì •í•˜ëŠ”ë²• Boosting ê°„ë‹¨í•˜ê²Œ ëª¨ë¸ì„ ë§Œë“¤ì–´ testingí›„ ì•ˆì¢‹ì€ ë¶€ë¶„ì„ ê³ ì³ë‚˜ê°€ë©° ì—¬ëŸ¬ê°œì˜ modelì„ ë§Œë“ ë‹¤ ì´ë“¤ì„ ë…ë¦½ì ì¸ ëª¨ë¸ì´ ì•„ë‹Œ ì´ ëª¨ë¸ë“¤ì„ Sequentialí•˜ê²Œ í•©ì³ì„œ í•˜ë‚˜ì˜ strong learnerë¥¼ ë§Œë“ ë‹¤ Gradient Descent Method Stocastic (í•œë²ˆì— 1ê°œì˜ sampleì„ ì‚¬ìš©í•˜ì—¬ gradient update) Mini batch (í•œë²ˆì— ì ë‹¹íˆ ì‘ì€ batch sizeê°œìˆ˜ì˜ samplesë¥¼ ì‚¬ìš©í•˜ì—¬ update) S Batch (ëª¨ë“  dataë¥¼ ë‹¤ ì¨ì„œ gradientë¥¼ update) Batch gradient descentë¥¼ ì‚¬ìš©í•  ê²½ìš° step í•œë²ˆì— ëª¨ë“  dataì— ëŒ€í•œ loss functionì„ ê³„ì‚°í•´ì•¼ í•˜ë¯€ë¡œ ê³„ì‚°ëŸ‰ì´ í„°ì§„ë‹¤ ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì“°ëŠ” ê²ƒì´ SGD (stocastic gradient descent), mini batch Batchë³´ë‹¤ ë‹¤ì†Œ ë¶€ì •í™• í• ìˆ˜ëŠ” ìˆì§€ë§Œ ë¹ ë¥¸ ê³„ì‚°ì†ë„ë¡œ ì¸í•œ ë¹ ë¥¸ ìˆ˜ë ´ì†ë„ BATCH SIZE MATTERS!!! Large batch size converge to sharp minimizers Small batch size converge to flat minimizers â€”&gt; High Generalize performance ìœ„ì˜ ê·¸ë˜í”„ëŠ” modelì˜ training dataì™€ testing dataì— ëŒ€í•œ loss functionì´ë‹¤. í° batch sizeë¥¼ ì¨ì„œ sharp minimum ê°’ì„ ê°€ì§€ê²Œ ëœë‹¤ë©´, ìš°ë¦¬ê°€ ì›í–ˆë˜ training fuctionì˜ minimumì—ì„œì˜ testing functionì—ì„œì˜ testing functionì˜ ê°’ì„ ë³´ë©´ ìµœì†Œì ì´ ì•„ë‹Œ ê½¤ë‚˜ í°ê°’ì„ ê°€ì§„ë‹¤. ì´ëŠ” ëª¨ë¸ì´ Generalizeì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤ëŠ” ì´ì•¼ê¸°ë¡œ ê·€ê²°ëœë‹¤. í•˜ì§€ë§Œ ì‘ì€ batch sizeë¥¼ ì¨ì„œ functionë“¤ì´ Flat Minumumê°’ì„ ê°€ì§€ê²Œ ëœë‹¤ë©´, ê½¤ë‚˜ Generalize ì„±ëŠ¥ì´ ì¢‹ë‹¤. ìœ„ì˜ ë…¼ë¬¸ ì½ì–´ë³´ë©´ ì¢‹ë‹¤ê³  ì¶”ì²œí•´ ì£¼ì‹¬ Automatic Differentiation SGD Momentum Adagrad RMSprop Adam â€¦ Momentum1ë²ˆ gradientê°€ í•œìª½ìœ¼ë¡œ íë¥´ê²Œ ë˜ë©´ ì´ì „ì˜ gradientì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ì–´ê°€ëŠ” ë°©í–¥ ? ì´ëŸ°ëŠë‚Œ SGDì™€ ë‹¬ë¦¬ parameterì—…ë°ì´íŠ¸ì‹œ gradientë¥¼ ë°”ë¡œ ì‚¬ìš©í•˜ì—¬ ì—…ë°ì´íŠ¸ í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ aë¼ëŠ” termì„ ë§Œë“¤ì–´ ì´ì „ì˜ gradient ê°’ì„ ë°˜ì˜í•´ì£¼ëŠ” termì„ ì¶”ê°€í•´ ì£¼ì—ˆë‹¤. at+1 &lt;= Bat + gt Wt+1 &lt;= Wt - $\\eta$at+1 Bê°€ momentum, at+1ê°€ accumulation, aëŠ” momentumì„ í¬í•¨í•˜ê³  ìˆì–´ì„œ í•œë²ˆ í˜ëŸ¬ê°€ê¸° ì‹œì‘í•œ gradientë¥¼ ìœ ì§€ì‹œì¼œì¤Œ momentumì„ ì‚¬ìš©í•˜ë©´ SGDì—ì„œ local minimumì— ë¹ ì¡Œë˜ ë¬¸ì œë¥¼ í•´ê²°í• ìˆ˜ë„ ìˆë‹¤. Momentumìœ¼ë¡œ ê¸°ì¡´ì˜ local minimumì„ ë¹ ì ¸ë‚˜ì™€ ë” ì¢‹ì€ minimumìœ¼ë¡œ ê°ˆìˆ˜ë„ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. NAG (Nesterov Accelerated Gradient) at+1 &lt;= Bat + $\\nabla$ L(Wt - $\\eta$Bat+1) Wt+1 &lt;= Wt - $\\eta$at+1 $\\nabla$ L(Wt - $\\eta$Bat+1) : aë¼ê³  ë¶ˆë¦¬ìš°ëŠ” í˜„ì¬ì •ë³´ì—ì„œ ê·¸ë°©í–¥ìœ¼ë¡œ í•œë²ˆê°€ë³´ê³  (lookahead) ì´ë¥¼ í¬í•¨í•´ì„œ update momentumì€ ê´€ì„± : ë”°ë¼ì„œ ê°’ì´ local minimumì— ìˆ˜ë ´í•˜ì§€ ëª»í•˜ëŠ” í˜„ìƒì´ ì¼ì–´ë‚  ìˆ˜ë„ ìˆìŒ (ê´€ì„±ì„ ê°€ì ¸ì„œ) NAGë¥¼ ì“°ë©´ convergance ratioê°€ ì¢‹ë‹¤ Adagrad adapts the learning rate parameterê°€ ë³€í•´ì™”ëŠ”ì§€ ì•ˆë³€í•´ì™”ëŠ”ì§€ë¥¼ ë³´ê³  parameterë¥¼ ì—…ë°ì´íŠ¸ Sum of gradient squares -&gt; G Gê°€ ê²°êµ­ ê³„ì†ì»¤ì§€ê¸° ë•Œë¬¸ì— Wê°€ ì—…ë°ì´íŠ¸ê°€ ì•ˆë˜ê³  í•™ìŠµì´ ë©ˆì¶”ëŠ” í˜„ìƒì´ ë°œìƒ ë”°ë¼ì„œ Gì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ”ê²Œ ë’¤ì˜ optimizerì¸ Adadelta Adadelta ì—„ì²­ë‚œ ì–‘ì˜ memoryê°€ í•„ìš” ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê°ë§ˆ, 1-ê°ë§ˆ : exponential moving average(EMA) There is no learning rate in Adadelta!! RMSprop Adam adaptive moment estimation 2. Regulization For good generalization í•™ìŠµì„ ë°©í•´í•˜ëŠ”ê²Œ ìš”ì  Overfitting ë°©ì§€ ì´ëŸ°ê±° ì¢…ë¥˜ Early Stopping Parameter norm penalty Data augmentation Noise robustness Dropout Batch Normalization Early stopping ì¤‘ê°„ì— í•™ìŠµì„ ë©ˆì¶”ì–´ validation dataë¥¼ ë§Œë“œëŠ” Parameter norm penalty weightê°€ ì‘ì„ìˆ˜ë¡ ì¢‹ë‹¤? -&gt; function spaceë‚´ì—ì„œ ë¶€ë“œëŸ¬ìš´ í•¨ìˆ˜ì¼ìˆ˜ë¡ generalization performanceê°€ ë†’ì„ê²ƒì´ë‹¤ Data augmentation ë°ì´í„°ì˜ ê°œìˆ˜ë¥¼ ëŠ˜ë¦¬ê¸° ìœ„í•´ label preserving data augmentationê°™ì€ê±¸ ì‚¬ìš© labelì´ ë³€í™˜ë˜ì§€ ì•ŠëŠ” ì„ ì—ì„œ dataë¥¼ ë³€í™˜ Label smoothing ì´ëŸ¬í•œ ë°©ë²•ìœ¼ë¡œ datasetì„ í™•ì¥ì‹œì¼œ modelì„ training í•´ë³´ë©´ ì„±ëŠ¥í–¥ìƒì´ ëšœë ¸í•˜ë‹¤ Batch Normalization ë‚´ê°€ ì ìš©í•˜ê³ ì í•˜ëŠ” statisticsë¥¼ ì •ê·œí™” ê°ê°ì˜ layerê°€ 1000ê°œì˜ parameterë¼ë©´ ê°ê°ì˜ parameterê°€ ì •ê·œí™”ë˜ê²Œ í•˜ëŠ”ê²ƒ Internal feature shiftë¥¼ ì¤„ì¸ë‹¤???? -&gt; ë…¼ë€ì´ ë§ë‹¤ ê·¸ëŸ¼ì—ë„ í™œìš©í•˜ë©´ ì¼ë°˜ì ìœ¼ë¡œ ì„±ëŠ¥ì´ ë§ì´ í–¥ìƒëœë‹¤ í•˜ë‚˜í•˜ë‚˜ë¥¼ í™œìš©í•˜ì—¬ Normalizeë¥¼ í•˜ë©´ì„œ ì¢‹ì€ ì„±ëŠ¥ì´ ë‚˜ëŠ”ê±¸ ì„ íƒ ã…‹ã…‹ Further Question Regression Task, Classification Task, Probabilistic Taskì˜ Loss í•¨ìˆ˜(or í´ë˜ìŠ¤)ëŠ” Pytorchì—ì„œ ì–´ë–»ê²Œ êµ¬í˜„ì´ ë˜ì–´ìˆì„ê¹Œìš”? ì˜¬ë°”ë¥´ê²Œ(?) cross-validationì„ í•˜ê¸° ìœ„í•´ì„œëŠ” ì–´ë–» ë°©ë²•ë“¤ì´ ì¡´ì¬í• ê¹Œìš”? Time seriesì˜ ê²½ìš° ì¼ë°˜ì ì¸ k-fold cvë¥¼ ì‚¬ìš©í•´ë„ ë ê¹Œìš”? TimeseriesCV Further Question 1 Pytorch ë‚´ë¶€ì—ì„œì˜ Loss function êµ¬í˜„ Regression Taskì˜ Loss function torch.nn.L1Loss(size_average=None, reduce=None, reduction: str = â€˜meanâ€™) Measures the mean absolute error (MAE) between each element in the input x and target y . ë‚´ë¶€ì ìœ¼ë¡œ L1Lossê°€ ì–´ì°Œ êµ¬í˜„ë˜ì–´ ìˆë‚˜ í™•ì¸í•´ ë³´ì ë‚´ë¶€ì ìœ¼ë¡œ reductionì„ meanìœ¼ë¡œ ì„¤ì •ì‹œ ìš°ë¦¬ê°€ ì•Œê³ ìˆëŠ” sumì„ nìœ¼ë¡œ ë‚˜ëˆˆ ê°’ì„ lossë¡œ ì‚¬ìš©(ê¸°ë³¸ê°’ : mean) reductionì„ sumìœ¼ë¡œ ì„¤ì •ì‹œ nìœ¼ë¡œ ë‚˜ëˆ„ëŠ”ê²Œ ì‚¬ë¼ì§„ ê·¸ì € ì°¨ì´ì˜ normì˜ sumê°’ torch.nn.MSELoss(size_average=None, reduce=None, reduction: str = â€˜meanâ€™) Measures the mean squared error (squared L2 norm) between each element in the input xx and target yy . ìœ„ì˜ L1 lossì™€ ë‹¤ë¥¸ì ì€ ì°¨ì´ì˜ ì œê³± Classification Taskì˜ Loss function torch.nn.BCELoss(weight: Optional[torch.Tensor] = None, size_average=None, reduce=None, reduction: str = â€˜meanâ€™) ë‚´ê°€ ì´ loss functionì„ ì‚¬ìš©í–ˆì„ë•ŒëŠ” ë¶„ë¥˜ë¬¸ì œì¤‘, 2ê°œì˜ label ì‚¬ì´ì—ì„œ classificationì„ í• ë•ŒëŠ” BCELossë¥¼ ì‚¬ìš©í•˜ê³  NNì˜ ì¶œë ¥ë‹¨ì— sigmoidí•¨ìˆ˜ë¥¼ ì ìš©í•´ì£¼ì—ˆë‹¤. nn.BCEWithLogitsLossí•˜ì§€ë§Œ ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©ì‹œ sigmoidê°€ ë‚´ë¶€ì ìœ¼ë¡œ í¬í•¨ë˜ì–´ìˆë‹¤ loss functionì„ ë³´ë©´ torch.nn.CrossEntropyLoss(weight: Optional[torch.Tensor] = None, size_average=None, ignore_index: int = -100, reduce=None, reduction: str = â€˜meanâ€™) Probabilistic Taskì˜ Loss function torch.nn.NLLLoss(weight: Optional[torch.Tensor] = None, size_average=None, ignore_index: int = -100, reduce=None, reduction: str = â€˜meanâ€™) The negative log likelihood loss. It is useful to train a classification problem with C classes. Further Question 2 ì˜¬ë°”ë¥´ê²Œ(?) cross-validationì„ í•˜ê¸° ìœ„í•´ì„œëŠ” ì–´ë–¤ ë°©ë²•ë“¤ì´ ì¡´ì¬í• ê¹Œìš”? ì¼ì •í•œ kê°œë¡œ dataë¥¼ ë‚˜ëˆ„ì–´ ê·¸ì¤‘ì— í•˜ë‚˜ë¥¼ validation dataë¡œ ì‚¬ìš©í•˜ëŠ” -&gt; k-fold cv validation hyper paramter : ìš°ë¦¬ê°€ ì •í•˜ëŠ” ê°’ ex) lr, network ê¹Šì´, loss function ì¢…ë¥˜, ë“±ë“± cross validationìœ¼ë¡œ ìµœì ì˜ hyper parameterë¥¼ ì°¾ê³  ì´ê±¸ ê³ ì •í•œ ìƒíƒœì—ì„œ ì „ì²´ training dataë¥¼ ì‚¬ìš©í•´ì„œ í•™ìŠµì„ ì‹œí‚¨ë‹¤ ëª¨í˜•ì˜ íŒŒë¼ë¯¸í„° ì¶”ì •ì—ëŠ” íŠ¸ë ˆì´ë‹ì…‹ì„ ì‚¬ìš©í•˜ê³ , í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ì—ëŠ” ë°¸ë¦¬ë°ì´ì…˜ ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ Further Question 3 Time seriesì˜ ê²½ìš° ì¼ë°˜ì ì¸ k-fold cvë¥¼ ì‚¬ìš©í•´ë„ ë ê¹Œìš”? ì‹œê°„ì˜ ì •ë³´ë¥¼ ê°€ì§„ dataë¥¼ ê¸°ì¡´ì˜ k-fold cvë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ì–´ ë²„ë¦°ë‹¤ë©´ , ê³¼ê±°ì™€ ë¯¸ë˜ê°€ ë’¤ì„ì—¬ ì•ˆëœë‹¤ for time series data we utilize hold-out cross-validation where a subset of the data (split temporally) is reserved for validating the model performance. ì´ëŠ” ê²°êµ­ training data set -&gt; validation data set -&gt; test data set ì´ ì‹œê°„ìˆœìœ¼ë¡œ ë°°ì—´ë˜ì•¼ í•œë‹¤ëŠ” ëœ»ì´ë‹¤ Time dependency í˜„ì¬ì˜ ì‹œì ì—ì„œ ë¯¸ë˜ì˜ dataë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§ì¶”ëŠ” ë° ì‚¬ìš© ëœ ì´ë²¤íŠ¸ ì´í›„ì— ì‹œê°„ìˆœìœ¼ë¡œ ë°œìƒí•˜ëŠ” ì´ë²¤íŠ¸ì— ëŒ€í•œ ëª¨ë“  ë°ì´í„°ë¥¼ ë³´ë¥˜í•´ì•¼í•©ë‹ˆë‹¤. ë”°ë¼ì„œ êµì°¨ì ìœ¼ë¡œ dataë¥¼ ë°”ê¾¸ì–´ì£¼ëŠ” K-fold ëŒ€ì‹  hold-out cross-validationì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤ ì´ëŠ” ê²°êµ­ training data set -&gt; validation data set -&gt; test data set ì´ ì‹œê°„ìˆœìœ¼ë¡œ ë°°ì—´ë˜ì•¼ í•œë‹¤ëŠ” ëœ»ì´ë‹¤ Arbitrary Choice of Test Set ë§Œì•½ ìš°ë¦¬ê°€ ì„ì˜ì ìœ¼ë¡œ ì •í•œ test setì—ì„œ poorí•œ ê²°ê³¼ë¥¼ ë‚´ì—ˆë‹¤ë©´, ì´ëŠ” ì „ì²´ì ì¸ dataì— ëŒ€í•œ poorí•œê²°ê³¼ê°€ ì•„ë‹Œ ê·¸ íŠ¹ì •í•œ ë…ë¦½ì ì¸ test setì—ì˜ poorí•œ ê²°ê³¼ì´ë‹¤ ë”°ë¼ì„œ ìš°ë¦¬ëŠ” Nested Cross-Validationì„ ì‚¬ìš©í•œë‹¤ ê·¸ë¦¼ì„ ë³´ë©´ì„œ Nested cvë¥¼ ì•Œì•„ë³´ì Nested CVì—ëŠ” ì˜¤ë¥˜ ì¶”ì •ì„ ìœ„í•œ ì™¸ë¶€ loopì™€ hyperparameterì¶”ì •ì„ ìœ„í•œ Inner loopê°€ ìˆë‹¤ ë‚´ë¶€ë£¨í”„ëŠ” train data setì„ ë‚˜ëˆ„ëŠ” ê±¸ë¡œ ì•ì„œ ì„¤ëª…í•œì¼ë°˜ì ì¸ CVêµ¬ì¡°ì´ë‹¤ ì´ì œ data setë¥¼ ì—¬ëŸ¬ trainê³¼ test setìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì™¸ë¶€ë£¨í”„ê°€ ì¶”ê°€ë˜ì—ˆê³  ê° ë¶„í• ëœ ì˜¤ë¥˜ì˜ í‰ê· ì„ êµ¬í•œë‹¤ Nested CV for Time series data Predict second half ë°ì´í„°ì˜ ì „ë°˜ë¶€ (ì¼ì‹œì ìœ¼ë¡œ ë¶„í• )ëŠ” í›ˆë ¨ ì„¸íŠ¸ì— í• ë‹¹ë˜ê³  í›„ë°˜ë¶€ëŠ” í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ê°€ ëœë‹¤ validation dataì˜ í¬ê¸°ëŠ” ë‹¬ë¼ì§ˆìˆ˜ ìˆì§€ë§Œ, ìˆœì„œëŠ” dataì˜ ì‹œê°„ ìˆœì„œëŠ” í•­ìƒ test data setì´ trainë³´ë‹¤ ë’¤ì—ìˆì–´ì•¼ í•œë‹¤ ì´ê²Œ ì•ì„  1ì˜ time dependencyë¥¼ í•´ì†Œì‹œí‚¨ ê²ƒì´ë‹¤ Day Forward-Chaining predict second halfì˜ ë‹¨ì ì€ hold-out dataset(ì•ì„œ ë§í•œ ì—°ëŒ€ìˆœ)ì„ ì„ì˜ë¡œ ì„ íƒí•˜ê²Œ ë˜ë©´ time dependencyëŠ” í•´ê²°ë˜ì—ˆì§€ë§Œ Arbitrary Choice of Test Setì„ í•´ê²°í•˜ì§€ ëª»í•˜ê²Œ ëœë‹¤ ë”°ë¼ì„œ ì•ì„œ ë§í•œ Nested CVì™€ ê°™ì´ ë§ì€ trainê³¼ test data setì„ ë§Œë“¤ì–´ ì´ë“¤ì˜ ì˜¤ë¥˜ê°’ì„ êµ¬í•´ í‰ê· ì„ ë‚´ì–´ì¤€ë‹¤ ì˜ˆë¥¼ ë“¤ë©´ 1ì¼ì„ test setìœ¼ë¡œ ê°„ì£¼í•˜ê³  ë‚˜ë¨¸ì§€ë¥¼ train setìœ¼ë¡œ í•´ì£¼ëŠ” ê²ƒì´ë‹¤","link":"/2021/02/02/2021-02-02-Boostcamp12.1/"},{"title":"CNN1","text":"CNN Convolution ì—°ì‚° ì´í•´í•˜ê¸° ì§€ê¸ˆê¹Œì§€ ë°°ìš´ MLPëŠ” fully connected. ê°€ì¤‘ì¹˜ í–‰ë“¤ì´ ië²ˆì§¸ ìœ„ì¹˜ë§ˆë‹¤ í•„ìš”í•´ì„œ iê°€ ì»¤ì§€ë©´ ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ í¬ê¸°ê°€ ì»¤ì§€ê²Œ ë¨ ìš°ë¦¬ê°€ ì´ì œë¶€í„° ë³¼ Convolution ì—°ì‚°ì€ ì»¤ë„ì´ë¼ëŠ” ê³ ì •ëœ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ì‚¬ìš©í•˜ì—¬ ê³ ì •ëœ ì»¤ë„ì„ ì…ë ¥ë²¡í„°ì—ì„œ ì˜®ê²¨ê°€ë©° ì ìš© xë¼ëŠ” ì…ë ¥ë²¡í„° ìƒì—ì„œ ì»¤ë„ì‚¬ì´ì¦ˆ ë§Œí¼ ì›€ì§ì—¬ ê°€ë©° ì—°ì‚° ë‹¤ì–‘í•œ ì°¨ì›ì—ì„œì˜ Convolution â€‹ 2ì°¨ì› Convolution ì—°ì‚° ì…ë ¥ì„ kernal sizeì— ë§ì¶°ì„œ ì…ë ¥ìœ„ì¹˜ì— í•´ë‹¹í•˜ëŠ” indexë§Œí¼ ì˜®ê²¨ë‹¤ë‹ˆë©´ì„œ, ì„±ë¶„ê³±ì„ ì—°ì‚°í•˜ëŠ” 2D ì´ë¯¸ì§€ ë‹¤ë¥¸ kernelì„ ì ìš©í•˜ì—¬ Convolution filterë¥¼ ì ìš©í•˜ë©´ kernelì— ë§ëŠ” íŠ¹ì„±ì„ ê°€ì§€ëŠ” 2D ì´ë¯¸ì§€ê°€ ë‚˜ì˜¨ë‹¤ ì…ë ¥í¬ê¸°ë¥¼ (H,W), ì»¤ë„í¬ê¸°ë¥¼ (KH, KW), ì¶œë ¥í¬ê¸°ë¥¼ (OH, OW)ë¼ í• ë•Œ channelì´ ì—¬ëŸ¬ê°œì¸ 2ì°¨ì› ì…ë ¥ì˜ ê²½ìš° 2ì°¨ì› Convolutionì„ ì±„ë„ ê°œìˆ˜ë§Œí¼ ì ìš©í•œë‹¤ (2ì°¨ì› ì´ë¯¸ì§€ë”ë¼ë„, RGBê°€ìˆì–´ì„œ 3 channel) ì±„ë„ì´ ì—¬ëŸ¬ê°œì¸ ì…ë ¥ì¸ ê²½ìš° ì»¤ë„ë„ ì±„ë„ì˜ ê°œìˆ˜ë§Œí¼ ìˆì–´ì•¼ í•œë‹¤ ì±„ë„ì´ ì—¬ëŸ¬ê°œì¼ë•ŒëŠ” ê°ì»¤ë„ì„ ì ìš©í•œ ê°ê°ì˜ ì±„ë„ì˜ ê²°ê³¼ë¥¼ ë”í•´ì¤€ë‹¤ ë§Œì•½ ì¶œë ¥ì˜ channelì„ ëŠ˜ë¦¬ê³  ì‹¶ë‹¤ë©´??? ì»¤ë„ì˜ ê°œìˆ˜ë¥¼ ì—¬ëŸ¬ê°œ ë§Œë“¤ë©´ ëœë‹¤. feature mapì˜ ì±„ë„ ìˆ«ìë¥¼ ëŠ˜ë¦¬ëŠ” ë³´í†µ ì´ë ‡ê²Œ ë§ì´ ì‚¬ìš©í•œë‹¤ Convolution ì—°ì‚°ì˜ Backpropagation Convolutionì—°ì‚°ì€ ëª¨ë“  ì…ë ¥ë°ì´í„°ì— ê³µí†µìœ¼ë¡œ ì»¤ë„ì´ ì ìš©ë˜ê¸° ë•Œë¬¸ì— ì—­ì „íŒŒ ê³„ì‚°ì‹œì—ë„ convolutionì´ ë‚˜ì˜¤ê²Œ ëœë‹¤ Stack of Convolution MLPë•Œì™€ ë§ˆì°¬ê°€ì§€ë¡œ non-linear activationì„ ì‚¬ì´ì— ì ìš©í–ˆë‹¤ ì—°ì‚°ì„ ì •ì˜í•˜ëŠ” Parameterì˜ ìˆ«ìê°€ ì¤‘ìš” ì²«ë²ˆì§¸ Convolutional filterì˜ parameterìˆ˜ : 5*5*3*4 = 300 ê°œ ë‘ë²ˆì§¸ Convolutional filterì˜ parameterìˆ˜ : 5*5*4*10 = 1000ê°œ Convolution NN CNNì€ Convolution layer + Pooling layer + fully connected layer Convolution &amp; pooling layer : feature extraction fully connected layer : decision making ì ì  ë’¤ì˜ fully connected layerë¥¼ ì¤„ì´ëŠ” ì¶”ì„¸ reason : parameterì˜ ìˆ˜ ìš°ë¦¬ê°€ ì¼ë°˜ì ìœ¼ë¡œ ìš°ë¦¬ì˜ ëª¨ë¸ì˜ parameter ìˆ«ìê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ í•™ìŠµì´ ì–´ë µê³  generalize performaceê°€ ë–¨ì–´ì§„ë‹¤ ë”°ë¼ì„œ CNNì€ parameterìˆ˜ë¥¼ ì¤„ì´ëŠ”ë° ì§‘ì¤‘í•œë‹¤ ì–´ë–¤ ë‰´ëŸ´ë„¤íŠ¸ì›Œí¬ì— ëŒ€í•´ì„œ parameterìˆ«ìë¥¼ ê³„ì‚°í•´ë³´ì Stride &amp; Paddingskip Convolution Arithmatic ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” kernelì„ ê³„ì‚°í•´ë³´ë©´ ì¼ë‹¨ 3*3ì˜ width ì™€ heightì´ë©° kernelì˜ channelì€ ì…ë ¥ì˜ channelê³¼ ê°™ì•„ì•¼ í•˜ë¯€ë¡œ 128ì´ë‹¤ ë”°ë¼ì„œ í•˜ë‚˜ì˜ kernelì˜ size = 3*3*128ì´ë‹¤ ì´ì œ ì´ kernelì˜ ê°¯ìˆ˜ë¥¼ ì°¾ìœ¼ë ¤ë©´ outputì˜ channelì¸ 64ì´ë‹¤ ë”°ë¼ì„œ ì´ parameterì˜ ê°œìˆ˜ëŠ” 3*3*128*64 = 73728ì´ë‹¤ ì´ì œ paddingê³¼ strideëŠ” parameter ìˆ˜ì™€ëŠ” ì—°ê´€ì´ ì—†ë‹¤ ex) ì‚¬ì‹¤ alexnetì€ networkê°€ 2 pathë¡œ ë‚˜ëˆ„ì–´ì§ ì¼ë‹¨ ì²«ë²ˆì§¸ layerì˜ kernel = 11x11x3 = 363 ì´ê²Œ 48ê°œ ìˆìœ¼ë¯€ë¡œ parameterê°œìˆ˜ = 17424ê°œ ì›ë˜ëŠ” ì´ì œ 96ì§œë¦¬ channelì„ ë§Œë“¤ì—ˆì–´ì•¼ ë˜ëŠ”ë° 2ê°œë¡œ ë‚˜ëˆ„ì–´ì„œ 48 channelë¡œ ë§Œë“¤ì–´ì¤Œ ë”°ë¼ì„œ ì´ parameterìˆ˜ = 34848 kernel = 5x5x48 = 1200 ì´ê²Œ 128ê°œ ê·¸ë¦¬ê³  ì´ 2ê°œ ìˆìœ¼ë‹ˆ -&gt; 1200x128x2 = 307k kernel = 3x3x128 = 1152 ì´ê²Œ 2ê°œ -&gt; 2304 ì´ê²Œ 192ê°œ ê·¸ë¦¬ê³  ì´ -&gt; 2304x192x2 = 884k ë˜‘ê°™ì€ ë°©ë²• -&gt; 663k ì­‰ì­‰ ê·¸ëŸ¬ë‹¤ê°€ Fully connected layerì˜ parameter ê°œìˆ˜ 13x13x128x2x2048x2 = 177M 16M 4M ë³´ë©´ dense layerì—ì„œ parameterìˆ«ìê°€ ë„ˆë¬´ ì»¤ì§„ë‹¤ ê²°êµ­ì€ parameterë¥¼ ì¤„ì´ê¸° ìœ„í•´ì„œëŠ” convolution layerë¥¼ ê¹Šê²Œ ìŒ“ê³  ë’¤ì˜ dense layerë¥¼ ìµœëŒ€í•œ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ë°œì „í•˜ê³  ìˆë‹¤ 1x1 convolution ì—¬ê¸°ì„œ parameterìˆ˜ë¥¼ ê³„ì‚°í•´ë³´ë©´ 1x1x128x32 = 4096 demensionì„ ì¤„ì¸ë‹¤!!! ê¹Šì´ëŠ” ê¹Šì–´ì§€ì§€ë§Œ parameterìˆ˜ë¥¼ ì¤„ì´ëŠ” ì—­í• ì„ í•œë‹¤ e.g) bottle neck architecture Modern Convolutional Neural Networks ILSVRCì—ì„œ ìš°ìŠ¹í•˜ê±°ë‚˜ ì¢‹ì€ ì„±ëŠ¥ì„ ê±°ë‘” modelë“¤ì— ëŒ€í•œ parameter ê°œìˆ˜, depth ë“±ë“± AlexNet ILSVRC Imagenet Large-Scale Visual Recognition Challenge 1000 different categories over 1 millions images AlexNet gpuì˜ ì„±ëŠ¥ì´ ë¶€ì¡±í•´ì„œ í•œë²ˆì— ê³„ì‚°ì´ ì•ˆë˜ì„œ 2ê°œë¡œ ë‚˜ëˆ ì„œ ë”°ë¡œ trainingì„ ì‹œí‚´ Receptive field : í•˜ë‚˜ì˜ kernelì´ ë³¼ìˆ˜ìˆëŠ” ì´ë¯¸ì§€ levelì—ì„œì˜ ì˜ì—­ì€ ì»¤ì§, ê·¸ëŸ¬ë‚˜ parameterê°€ ëŠ˜ì–´ë‚˜ê²Œ ë¨ 5 Convolutional layer 3 Dense layer Key idea use ReLU function (non-linear func, ë§ˆì§€ë§‰ slopeê°€ 1ì´ë¼ gradientê°€ ì‚¬ë¼ì§€ê±°ë‚˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ë§ì¹  í™•ë¥ ì´ ì ìŒ) preserve properties of linear model overcome the gradient vanishing problem ì´ì „ì— ë§ì´ í™œìš©í•˜ë˜ tanhë‚˜ sigmoidëŠ” ê°’ì´ í¬ë©´ outputì˜ gradientê°€ 0ì— ê°€ê¹ê²Œ ë‚˜ì˜¨ë‹¤ GPI implementation (2 GPU) Overlapping Pooling, Local response normalization Data augmentation Dropout ì§€ê¸ˆ ë³´ë©´ ë³„ë¡œ ëŒ€ë‹¨í•œê²Œ ì•„ë‹ˆì§€ë§Œ, ê·¸ë‹¹ì‹œì—ëŠ” í˜ì‹ ì ì¸ ë°©ë²• ì¼ë°˜ì ì¸ standard ë¥¼ ì¡ì•˜ë‹¤! VGGNet Increasing depth with 3x3 convolution filter 1x1 convolution filter Dropout (p=0.5) VFF16,VGG19 Why 3x3????kernel sizeê°€ ì»¤ì§€ë©´ì„œ ê°€ì§€ëŠ” ì´ì  : Receptive fieldê°€ ì»¤ì§„ë‹¤ ex) 3x3ì„ 2ë²ˆ í•˜ê²Œ ë˜ë©´ outputì˜ 1ê°œì˜ ê°’ì€ inputì˜ 5x5ë¥¼ ë³´ê²Œëœë‹¤ -&gt; ì´ê²Œ ë°”ë¡œ Receptive field 3x3ì„ 3ë²ˆ í•˜ê²Œ ë˜ë©´ outputì˜ 1ê°œì˜ ê°’ì€ inputì˜ 6x6ì„ ë³´ê²Œëœë‹¤ ë”°ë¼ì„œ 3x3ì„ 2ê°œ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼, 5x5ë¥¼ 1ê°œ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ receptive fieldì˜ ê´€ì ì—ì„œëŠ” ê°™ë‹¤ ë”°ë¼ì„œ ì´ë‘˜ì˜ parameterì˜ ê°œìˆ˜ë¥¼ ë¹„êµí•´ ë³´ë©´ (chaneel : 128) 3x3 2ê°œ : 3x3x128x128x2 = 294k 5x5 1ê°œ : 5x5x128x128 = 409k ë”°ë¼ì„œ 3x3 2ê°œë¥¼ ì“°ëŠ”ê²Œ parameterì˜ ìˆ«ì ê°ì†Œ ì¸¡ë©´ì—ì„œ ì´ë“ì´ë‹¤ ì™œì´ëŸ°ì¼ì´ ì¼ì–´ë‚ ê¹Œ? ì‚¬ì‹¤ìƒ 3x3x3 = 27, 6x6 = 36 | 3x3 = 9, 5x5 = 25 ì´ëŸ° ë§¥ë½ì´ë‹¤ ë’¤ì˜ ëŒ€ë¶€ë¶„ì„ë³´ë©´ kernelì€ 7x7ì„ ë²—ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤ GoogLeNet ë³´ë©´ ì „ì²´ network ì•ˆì— ì‘ì€ network êµ¬ì¡°ë“¤ì´ ë°˜ë³µë˜ê³  ìˆë‹¤ (network in network) Inception block í™œìš© í•˜ë‚˜ì˜ ì…ë ¥ì— ëŒ€í•´ì„œ ì—¬ëŸ¬ê°œì˜ receptive fieldë¥¼ ê°€ì§€ëŠ” filterë¥¼ ê±°ì¹˜ê³  ì´ë“¤ì„ concatenation í•˜ì§€ë§Œ ê·¸ë³´ë‹¤ ì¤‘ìš”í•œê²Œ ì¤‘ê°„ì¤‘ê°„ì— ì¶”ê°€ë¡œ ë“¤ì–´ê°„ 1x1 Conv 3x3x128x128 = 147456 1x1x128x32 = 4096, 3x3x32x128 = 36864 -&gt;í•©ì€ : 40960 parameter ìˆ˜ê°€ 1/4ë¡œ ì¤„ì—ˆë‹¤ â€”-&gt; ì‚¬ìš©í•˜ëŠ”ê²Œ ì´ë“ì´ë‹¤!!! ê³¼ì—° AlexNet,VGGNet, GoogLeNet ì¤‘ parameterìˆ˜ê°€ ì‘ì€ê²ƒì€? AlexNet(8 layer) : 60M VGGNet(19-layer) : 110M GoogLeNet(22 layer) : 4M ResNet Deeper neural networks are hard to train Overfitting is usually caused by an excessive number of parameters Identity map xì™€ outputì˜ ì°¨ì›ì„ ë§ì¶°ì£¼ê¸° ìœ„í•´ì„œ 1x1 convolutionìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ”ê²ƒ Convolution ì—°ì‚°ê³¼ batch normì˜ ìˆœì„œ?????? ë” ìì„¸í•œ ResNetì˜ êµ¬ì¡°???? Bottleneck architecture3x3ì˜ ì—°ì‚°ì„ í•˜ê¸° ì „ì— channel ìˆ˜ë¥¼ ì¤„ì´ê²Œ ë˜ë©´ parameterì˜ ìˆ«ìë¥¼ ì¤„ì¼ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ? DenseNet ResNetì„ ë°”ë¼ë³´ê²Œ ë˜ë©´ ê·¸ëƒ¥ ë‘ê°œì˜ ê°’ì„ ë”í•˜ì§€ ë§ê³  concatnateì‹œí‚¤ë©´ ë˜ì§€ ì•Šì„ê¹Œ? ê³„ì† concatnateí•˜ë©´ channelì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì»¤ì§€ê¸° ë•Œë¬¸ì— ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì¤‘ê°„ì— 1x1 convë¥¼ í•´ì¤Œ","link":"/2021/02/03/2021-02-03-Boostcamp13.1/"},{"title":"RNN1","text":"RNN Sequence Data &amp; Model ì†Œë¦¬, ì£¼ê°€, ë¬¸ìì—´ ë“±ì˜ ë°ì´í„°ë¥¼ ì‹œí€€ìŠ¤ ë°ì´í„°ë¡œ ë¶„íœ´í•©ë‹ˆë‹¤ ì‹œê³„ì—´ ë°ì´í„°ëŠ” ì‹œê°„ìˆœì„œì— ë”°ë¼ ë‚˜ì—´ëœ ë°ì´í„°ë¡œ ì‹œí€€ìŠ¤ ë°ì´í„°ì— ì†í•œë‹¤ ë…ë¦½ë™ë“±ë¶„í¬ ê°€ì •ì„ ì˜ ìœ„í•´í•˜ê¸° ë•Œë¬¸ì— ìˆœì„œë¥¼ ë°”ê¾¸ê±°ë‚˜ ê³¼ê±°ì •ë³´ì— ì†ì‹¤ì´ ë°œìƒí•˜ë©´ ë°ì´í„°ì˜ í™•ë¥ ë¶„í¬ë„ ë°”ë€Œê²Œ ëœë‹¤ Markov model : first order autoregressive model ì´ë“¤ì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Latent autoregressive model hidden stateê°€ ê³¼ê±°ì˜ ì •ë³´ë“¤ì„ summerizeí•œë‹¤ ë‹¤ë£¨ëŠ” ë²• ì¡°ê±´ë¶€ í™•ë¥ ì„ ì´ìš©(ê³¼ì˜ ì •ë³´ë¥¼ ê°€ì§€ê³  ë¯¸ë˜ë¥¼ ì˜ˆì¸¡ ) ë°”ë¡œì§ì „ê¹Œì§€ì˜ ì •ë³´ S-1ë¥¼ ì‚¬ìš©í•´ì„œ í˜„ì¬ì¸ Së¥¼ ì—…ë°ì´íŠ¸ ë°˜ë“œì‹œ ëª¨ë“  ê³¼ê±°ì˜ ì •ë³´ë¥¼ ê°€ì§€ê³  ì—…ë°ì´íŠ¸ í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤ ë”°ë¼ì„œ ì¡°ê±´ë¶€ì— ë“¤ì–´ê°€ëŠ” ë°ì´í„°ì˜ ê¸¸ì´ëŠ” ê°€ë³€ì ì´ë‹¤ ê³ ì •ëœ ê¸¸ì´ì¸ $\\tau$ë§Œí¼ì˜ ì‹œí€€ìŠ¤ë§Œ í™œìš©í•˜ëŠ” ê²½ìš° Autoregressive Model(ìê¸°íšŒê·€ëª¨ë¸)ì´ë¼ê³  ë¶€ë¥¸ë‹¤ ì§ì „ê³¼ê±°ì˜ ì •ë³´ë‘ ì§ì „ì •ë³´ê°€ ì•„ë‹Œ ì •ë³´ë“¤ì„ Htë¡œ ë¬¶ì–´ì„œ í™œìš© ê¸¸ì´ê°€ ê°€ë³€ì ì´ì§€ ì•Šê³  ì´ì œ ê³ ì •ë˜ê¸° ë•Œë¬¸ì— ì—¬ëŸ¬ê°€ì§€ ì¥ì ì„ ê°€ì§€ê³  ìˆë‹¤ ì‚¬ì‹¤ì€ ê³¼ê±°ì˜ ëª¨ë“  ì •ë³´ë¥¼ ê³ ë ¤í•˜ê¸°ê°€ í˜ë“  ë¬¸ì œì ì„ ê³ ì³ì„œ ì´ì œ ì´ì „ì˜ ì •ë³´ë¥¼ ìš”ì•½í•˜ëŠ”Htë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ â€”-&gt; RNN RNN ì´í•´í•˜ê¸° ê¸°ë³¸ì ì¸ ëª¨í˜•ì€ MLPì™€ ìœ ì‚¬í•˜ë‹¤ RNNì˜ ì—­ì „íŒŒëŠ” ì ì¬ë³€ìˆ˜ì˜ ì—°ê²°ê·¸ë˜í”„ì— ë”°ë¼ ìˆœì°¨ì ìœ¼ë¡œ ê³„ì‚°í•œë‹¤ Back Propagation Through Time BTTPë¥¼ ì‚´í´ë´…ì‹œë‹¤BTTPë¥¼ í†µí•´ gradientë¥¼ ê³„ì‚°í•´ë³´ë©´ ë¯¸ë¶„ì˜ ê³±ìœ¼ë¡œ ì´ë£¨ì–´ì§„ í•­ì´ ê³„ì‚°ì´ ëœë‹¤ ê¸¸ì–´ì§€ë©´ ê³„ì‚°ì´ ë¶ˆì•ˆì •í•´ì§ìœ¼ë¡œ(gradient vanishingê³¼ ê°™ì€)ë¬¸ì œê°€ ìˆê¸° ë•Œë¬¸ì— ê¸¸ì´ë¥¼ ëŠëŠ”ê²ƒìœ¼ë¡œ truncated BPTT Gradient vanishing ë¬¸ì œì˜ í•´ê²°?? ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ê¸¸ì–´ì§€ëŠ” ê²½ìš°ì—ëŠ” BTTPë¥¼ í†µí•œ ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ì˜ ê³„ì‚°ì´ ë¶ˆì•ˆì •í•´ ì§€ë¯€ë¡œ ê¸¸ì´ë¥¼ ëŠëŠ”ê²ƒì´ ì¤‘ìš”í•˜ë‹¤ ex) LSTM, GRU â€¦.. RNNì„ ì‹œê°„ìˆœìœ¼ë¡œ ì­‰ í’€ë©´ ê²°êµ­ fully connected layer networkê°€ ëœë‹¤ ê°€ì¥ì–´ë ¤ìš´ ? ë‹¨ì  ? â€”-&gt; í•˜ë‚˜ì˜ fixed ruleë¡œ ì´ì „ì˜ ì •ë³´ë“¤ì„ summerizeí•˜ê¸° ë•Œë¬¸ì— ë¨¼ ê³¼ê±°ì˜ ì •ë³´ë“¤ì´ í˜„ì¬ì—ì„œ ì‚´ì•„ë‚¨ê¸°ê°€ í˜ë“¤ë‹¤!! ì´ê²Œ short term dependencies ê²°êµ­ ë¨¼ ê³¼ê±°ì˜ ì •ë³´ë“¤ì€ ë§ì€ ì–‘ì˜ activation functionê³¼ Wê³±ì˜ ê²°ê³¼ë¡œ vanishing or explodingë˜ëŠ” í˜„ìƒì´ ì¼ì–´ë‚˜ê²Œ ëœë‹¤ LSTM LSTMì˜ ì „ì²´ì ì¸ êµ¬ì¡° ë“¤ì–´ì˜¤ëŠ” ì…ë ¥ì´ 3ê°œ ë‚˜ê°€ëŠ”ê²Œ 3ê°œ ì‹¤ì œë¡œ ë‚˜ê°€ëŠ”ê±´ ht (hidden state)","link":"/2021/02/04/2021-02-04-Boostcamp14.1/"},{"title":"Computer Vision application","text":"Computer Vision application Fully Convolutional Network ê¸°ì¡´ì˜ CNN êµ¬ì¡° : Fully Convolutional Network : Dense layerë¥¼ ì—†ì•´ë‹¤. - &gt; convolutionize ê²°êµ­ inputê³¼ outputì€ ê°™ë‹¤ parameterë„ ê°™ë‹¤ flatì„ í•´ì„œ dense layerë¥¼ ê±°ì¹˜ë‚˜, convolutionì„ ê±°ì¹˜ë‚˜ ê°™ë‹¤ ex) 4x4x16 ì´ ì˜€ë‹¤ë©´ ì´ê±¸ 256ê°œì˜ vectorë¡œ flatten ì‹œí‚¨ë‹¤ â€‹ FCNì„ ë³´ë©´ 4x4x16ì— ë˜‘ê°™ì€ í¬ê¸°ë¥¼ ê°€ì§„ kernelì„ ì ìš©í•œë‹¤ parameter 4x4x16x10 = 2560 4x4x16x10 = 2560 ê°™ë‹¤ ì´ëŸ°ì§“ì´ convolutionization ì™œ ì´ëŸ°ê±¸ í• ê¹Œ???? image segmentation ê´€ì ì—ì„œ ìƒê°ì„ í•´ë³´ì Fully convolutional networkê°€ ê°€ì§€ëŠ” ê°€ìì€ íŠ¹ì§•ì€ ë°”ë¡œ input dimension, íŠ¹íˆë‚˜ input dimensionì˜ spacial dimensionì´ë‹¤ Transforming fully connected layer into convolutional layers enables a classification to output a hitmap outputì´ ì»¤ì§€ê²Œ ë˜ë©´ ì´ê±°ì— ë¹„ë¡€í•´ì„œ ë’·ë‹¨ì˜ spacial dimensionì´ ì»¤ì§€ê²Œ ë¨ Because of convolutionì´ ê°€ì§€ëŠ” shared parameterì˜ ì„±ì§ˆ ë•Œë¬¸ì— ì›ë˜ëŠ” ì¶œë ¥ì˜ hitmapì€ inputë³´ë‹¤ í¬ê¸°ê°€ ì¤„ì–´ë“¤ê¸°ëŠ” í•˜ë‹¤ ê·¸ë ‡ì§€ë§Œ hitmapì´ë¼ëŠ” ê°€ëŠ¥ì„±ì´ ìƒê²¼êµ¬ë‚˜!!! ê·¸ë˜ì„œ ì–´ë– í•œ inputsizeì—ë„ ëŒì•„ê°€ì§€ë§Œ ì´ëŸ¬í•œ ì‘ì•„ì§„ outputì„ ë‹¤ì‹œ input sizeë§Œí¼ ëŠ˜ë¦¬ëŠ” ë°©ë²• Deconvolution(convolution transpose) ê°„ë‹¨í•˜ê²Œ ìƒê°í•˜ë©´ convolution ì—°ì‚°ìœ¼ë¡œ ì¤„ì–´ë“  ê²°ê³¼ë¥¼ convolutionì˜ ì—­ì—°ì‚°ìœ¼ë¡œ ë‹¤ì‹œ ëŠ˜ë ¤ì£¼ëŠ” ê·¼ë° convolutionì˜ ì—­ì—°ì‚°ì´ ì¡´ì¬í•˜ë‚˜? ë¶ˆê°€ëŠ¥í•˜ë‹¤ ê·¼ë° ê·¸ëƒ¥ ê°„ë‹¨í•˜ê²Œ ì´ë ‡ê²Œ ìƒê°í•˜ëŠ”ê²Œ í¸í•˜ë‹¤ deconvolutionì€ ê²°êµ­ convolutionì´í›„ì— ê²°ê³¼ì— paddingì„ ë§ì´ ì¤˜ì„œ ì—¬ê¸°ì— ê°™ì€ sizeì— kernelì„ ì ìš©í•˜ëŠ”ê²ƒì´ë‹¤ Dectection R-CNN takes and input image extract around 2000 region proposal (using selective search) computes features for each proposal (sizeëŠ” ëª¨ë‘ ë˜‘ê°™ì´ ë§ì¶”ì–´ ì¤€ë‹¤, use AlexNet) classifies with linear SVMs SPPNet RCNNì˜ ë¬¸ì œ : 2000ê°œë¥¼ ë½‘ìœ¼ë©´ 2000ê°œë¥¼ ì „ë¶€ CNNì— ëŒë ¤ì•¼ ë˜ê¸° ë•Œë¬¸ì— ì‹œê°„ì´ ì¡¸ë¼ ë§ì´ ê±¸ë¦°ë‹¤ ë”°ë¼ì„œ ì¼ë‹¨ image ì•ˆì—ì„œ bounding boxë¥¼ ë½‘ê³  image ì „ì²´ë¥¼ CNNì— ëŒë¦¬ê³ , í•´ë‹¹í•˜ëŠ” ìœ„ì¹˜ì˜ tensorë§Œì„ í™œìš©í•˜ì Fast R-CNN selective searchë¡œ bounding boxë¥¼ ì–»ê³  ì „ì²´ ì´ë¯¸ì§€ë¥¼ CNNì— í†µê³¼ ê·¸ë¦¬ê³  ROI pooling layerë¥¼ í†µí•´ ê°ê°ì˜ regionì— ëŒ€í•´ì„œ featureë¥¼ ë½‘ëŠ”ë‹¤ ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ì— Fully connected layerì„ í†µí•´ bounding boxë¥¼ ì–´ë–»ê²Œ ì›€ì§ì´ë©´ì¢‹ì€ì§€, labelì´ ë­”ì§€ë¥¼ ì•Œì•„ë‚¸ë‹¤ Faster R-CNN Bounding boxë¥¼ ë½‘ì•„ë‚´ëŠ” Region proposalë„ í•™ìŠµì„ í•˜ì! selective searchë¥¼ í•˜ì§€ë§ê³  ì´ê²ƒë„ í•™ìŠµì„ í•˜ìëŠ” ì˜ë¯¸ ì´ networkë¥¼ region proposal Networkê°€ ì¶”ê°€ë¨ Region proposal Network ì´ë¯¸ì§€ì˜ íŠ¹ì •ì˜ì—­ì´ bounding boxë¡œì„œ ì˜ë¯¸ê°€ ìˆëŠ”ì§€ ì—†ëŠ”ì§€ë§Œ ì°¾ì•„ì£¼ëŠ” Network ì—¬ê¸°ì„œ í•„ìš”í•œê²Œ Anchor box ë¯¸ë¦¬ ì •í•´ë†“ì€ Bounding boxì˜ í¬ê¸° : anchor box ì–´ë–¤ í¬ê¸°ì˜ ë¬¼ì²´ê°€ ìˆì„ê²ƒ ê°™ë‹¤? í•´ë‹¹í•˜ëŠ” ì˜ì—­ì˜ ì´ë¯¸ì§€ì— ë¬¼ì²´ê°€ ë“¤ì–´ìˆì„ì§€ ì•ˆë“¤ì–´ìˆì„ì§€ê°€ í•´ë‹¹í•˜ëŠ” ì˜ì—­ì— ë¬¼ì²´ê°€ìˆì„ì§€ì— ëŒ€í•œ ì •ë³´ë¥¼ ì•Œê³ ìˆë‹¤ 9ê°œì˜ region sizeì¤‘ì— 1ê°œë¥¼ ê³¨ë¼ì„œ ì–¼ë§ˆë‚˜ bounding boxë¥¼ ëŠ˜ì´ê±°ë‚˜ ì¤„ì¼ì§€ , xyì— off setì„ ì¤˜ì•¼ í•´ì„œ 4ê°œì˜ parameter í•´ë‹¹ boxê°€ ì“¸ë§Œí•œì§€ ì•„ë‹Œì§€ 9x(4+2) = 54ë§Œí¼ì˜ channelì´ ë‚˜ì˜¤ëŠ” YOLO faster R-CNNë³´ë‹¤ í›¨ì”¬ ë¹ ë¦„ Regionì— í•´ë‹¹í•˜ëŠ” sub tensorì„ ë¶„ë¥˜í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ ê·¸ëƒ¥ imageë¥¼ ë•Œë ¤ ë„£ìœ¼ë©´ ë”± ë‚˜ì˜¤ê¸° ë–„ë¬¸ì— ë¹ ë¦„ no exclusive bounding box sampling (region proposal) YOLOì— ëŒ€í•´ì„œëŠ” ì¶”ê°€ë¡œ ì •ë¦¬í•´ì„œ ë”°ë¡œ posting í•˜ê² ë‹¤","link":"/2021/02/03/2021-02-03-Boostcamp13.2/"},{"title":"Generative Models","text":"Generative Models What I can not create, I do not understand https://deepgenerativemodels.github.io/ Introduction What does it mean to learn a generative model generative modelì€ ë‹¨ìˆœíˆ ìƒì„±ëª¨ë¸ì´ ì•„ë‹ˆë‹¤ Suppose we have some images of dogs We want to learn a probability distribution p(x) such that Generation : If we sample xnew ~ p(x), xnew should look like a dog implicit models Density estimation :p(x) should be high if x look like a dog (ì–´ë–¤ì´ë¯¸ì§€ì˜ í™•ë¥ ì„ ê³„ì‚°í•¨) ì´ê±´ ë§ˆì¹˜ image classification explicit models Unsupervised representation learning íŠ¹ì • imageê°€ ì–´ë–¤ íŠ¹ì§•ì„ ê°€ì§€ê³ ìˆëŠ”ì§€ë¥¼ í•™ìŠµ How can we represent p(x)?????? Bernoulli distribution D = {Heads, Tails} Specify P(X = Head) = p, P(X = Tails) = (1-p) Categorical distribution ex) Modeling and RGB joint distribution (r,g,b) ~ p(R,G,B) number of case = 256x256x256 parameters = 255x255x255 ê°œê°€ í•„ìš” í•˜ë‚˜ì˜ RGB pixelë§Œí•´ë„ parameterë¥¼ í‘œí˜„í•˜ë ¤ë©´ ì–´ë§ˆì–´ë§ˆí•œ ìˆ«ìì˜ parameterê°€ í•„ìš”í•˜ë‹¤ Structure Through IndependenceWhat if X1,â€¦.,Xn are independent and binary pixels p(x1,â€¦,xn) = p(x1)p(x2)â€¦p(xn) possible state : 2^n^ parameter : nê°œë§Œ í•„ìš” ë§Œì•½ ê°ê°ì˜ pixelì´ ë…ë¦½ì ì´ë¼ê³  ê°€ì •í•œë‹¤ë©´ ì´ë ‡ê²Œ parameterìˆ˜ê°€ ì¤„ì–´ë“ ë‹¤ ê·¼ë° ì´ê±´ ë„ˆë¬´ ë§ì´ ì•ˆëœë‹¤ ë”°ë¼ì„œ Independenceì™€ fully dependentì‚¬ì´ì˜ ì ˆì¶©ì•ˆ??? Conditional IndependenceThree Important Rule nê°œì˜ joint distrubutionì„ nê°œì˜ conditional distributionìœ¼ë¡œ ë°”ê¾¸ê³  zê°€ ì£¼ì–´ì¡Œì„ë•Œ x,yëŠ” independentí•˜ë‹¤ -&gt;ì´ê²Œ ê°€ì • ì™„ì „ xyê°€ independentí•œê²Œ ì•„ë‹ˆë¼ zê°€ ì£¼ì–´ì¡Œì„ë•Œ yëŠ” ìƒê´€ì´ì—†ë‹¤ ì´ëŸ°ëŠë‚Œ Conditional IndependenceUsing the chain rule ì´ ìˆ˜ì‹ ë„ì¶œì—ì„œ ì–´ë– í•œ ìˆ˜í•™ì ì¸ ê°€ì •ì´ ì—†ì´ chain ruleë§Œìœ¼ë¡œ êµ¬í•œ ìˆ˜ì‹ì´ë‹¤ ë”°ë¼ì„œ fully independentì™€ parameter ê°œìˆ˜ëŠ” ê°™ë‹¤ p(x1) :1ê°œ p(x2|x1) : 2ê°œ (one per for p(x2|x1 = 0) and p(x2|x1 = 1)) p(x3|x1,x2) : 4ê°œ Hence 1+2+2^2^+â€¦+2^n-1^ = 2^n^-1 i+1ë²ˆì¨° pixelì€ ië²ˆì§¸ pixelì—ë§Œ dependentí•˜ë‹¤ ê°€ì • : markov assumption ê·¸ ì¤‘ê°„ì— ìˆëŠ” ê±¸ conditional independenceë¥¼ ì˜ í™œìš©í•´ì„œ ì¤‘ê°„ì˜ parameterê°’ì„ ì–»ì–´ëƒˆë‹¤ Auto-regressive Model suppose we have 28x28 binary pixels goal : p(x) = p(x1,x2â€¦.,x784) how can we parametrize p(x) use chain rule to get joint distribution p(x1:784) = p(x1)p(x2|x1)p(x2|x1:2)â€¦â€¦ ì´ê²Œ ë°”ë¡œ auto-regressive model (ië²ˆì§¸ pixelì´ 1~i-1ê¹Œì§€ ëª¨ë“  historyì— dependentí•œ) ê°€ì¥ ì¤‘ìš”í•œê²Œ ìˆœì„œë¥¼ ë§¤ê¸°ëŠ” ê³¼ì • ì´ë¯¸ì§€ì— ìˆœì„œ???? â€”-&gt; ìˆœì„œì— ë”°ë¼ ì„±ëŠ¥ì´ë‚˜ ë°©ë²•ë¡ ì´ ë‹¬ë¼ì§ˆìˆ˜ ìˆë‹¤ NADE : Neural Autoregressive Density Estimator p(xi|x1:i-1) = ië²ˆì§¸ pixelì„ 1~i-1ì— dependentí•˜ê²Œ ë§Œë“ ë‹¤ â€”â€“&gt; dependent í•˜ë‹¤ ? 1-i-1ë²ˆì§¸ pixelê°’ì„ ì…ë ¥ìœ¼ë¡œ ë°›ê³  networkë¥¼ í†µê³¼ì‹œì¼œì„œ ë‚˜ì˜¨ outputì— sigmoidë¥¼ í†µê³¼í•´ì„œ í™•ë¥ ì´ ë‚˜ì˜¤ë„ë¡í•˜ëŠ”ê²ƒ neural networkì˜ weightì˜ ì°¨ì›ê°’ì€ ì§€ì†í•´ì„œ ëŠ˜ì–´ë‚¨ì´ì „ì…ë ¥ë“¤ì´ ê³„ì†í•´ì„œ ëŠ˜ì–´ë‚˜ê¸° ë•Œë¬¸ì— NADE is explicit model Suppose we have 784ê°œì˜ binary pixel ì•Œê³ ìˆëŠ” ê°’ë“¤ì„ ì§‘ì–´ë„£ì€ë’¤ ê³„ì‚°í•˜ê²Œ ë˜ë©´ í™•ë¥ ê°’ì´ ë‚˜ì˜´ Density estimate : í™•ë¥ ì ìœ¼ë¡œ ë¬´ì–¸ê°€ì˜ í™•ë¥ ì„ explití•˜ê²Œ ê³„ì‚°í•œë‹¤ Continousí•œ r.vë¥¼ modelingí• ë•ŒëŠ” Gaussianì´ ì‚¬ìš©ì´ ëœë‹¤ Pixel RNN Use RNNs to define an auto regressive model ì´ì „ì— ë´¤ë˜ NADEëŠ” dense layerì„ ì‚¬ìš©í•¨ í•˜ì§€ë§Œ Pixel RNNì€ RNNì„ í†µí•´ generateí•œë‹¤ orderingì˜ ìˆœì„œì— ë”°ë¼ Row LSTM Diagonal BiLTM Latent Variable ModelsVariational Auto-encoder Is an autoencoder generative model?? autoencoderì€ inputì„ ì¬ì •ì˜í•˜ëŠ” ê³¼ì •ì´ì§€ generative modelì€ ì•„ë‹ˆë‹¤ ê³¼ì—° ë¬´ì—‡ë•Œë¬¸ì— Variational Auto-Encoderì€ generation ëª¨ë¸ì¸ê°€? Variational inference (VI) The goal of VI is to optimize the variational distribution that best matches the posterior distribution posterior distribution : observationì´ ì£¼ì–´ì¡Œì„ë•Œ ë‚´ê°€ ê´€ì‹¬ìˆì–´í•˜ëŠ” r.vì˜ í™•ë¥ ë¶„í¬ posterior distributionì„ ê³„ì‚°í•˜ëŠ”ê±´ ë§¤ìš° í˜ë“¤ê¸° ë•Œë¬¸ì— Variational distributionì„ ê·¼ì‚¬í•œë‹¤ KL divergenceë¥¼ ì‚¬ìš©í•´ì„œ Variational distributionê³¼ Posterior distributionì˜ ì°¨ì´ë¥¼ ì¤„ì—¬ë³´ê² ë‹¤ How? ì›í•´ëŠ” KL divergenceë¥¼ ì¤„ì´ëŠ”ê²Œ ëª©ì ì´ì§€ë§Œ ì´ê²Œ ë¶ˆê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— ELBOë¼ê³  ë¶ˆë¦¬ëŠ” termì„ ìµœëŒ€í™” í•œë‹¤ ELBO can further be decomposed into Reconstruction Term xë¼ëŠ” ì…ë ¥ì„ latent spaceë¡œ ë³´ëƒˆë‹¤ê°€ Decoderë¡œ ëŒì•„ì˜¤ëŠ” Reconstruction lossë¥¼ ì¤„ì´ëŠ” term Latent spaceì— ì˜¬ë ¤ë†“ì€ ì ë“¤ì´ ì´ë£¨ëŠ” ë¶„í¬ê°€ Latent spaceì˜ prior distributionì™€ ë¹„ìŠ·í•˜ë‹¤? implicití•œ model Decoderì´í›„ì˜ output domainì˜ ê°’ë“¤ì´ generation resultì´ë‹¤ Auto encoderì€ ì´ê²Œ ì•„ë‹ˆë¼ generation modelì´ ì•„ë‹ˆë‹¤ Key limitation Interactable model (hard to evaluate likelihood) reconstruction termì€ ìƒê´€ì—†ëŠ”ë° KL divergenceë¥¼ ì‚¬ìš©í•œ prior distributionì—ëŠ” ë¬´ì¡°ê±´ ë¯¸ë¶„ì´ ê°€ëŠ¥í•œ distribution (like Gaussian)ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤. ë”°ë¼ì„œ diverseí•œ latent prior distributionsì—ëŠ” ì‚¬ìš©ì„ í•˜ê¸°ì— í˜ë“¤ë‹¤ In most cases, we use an isotropic Gaussian Adversarial Auto-encoder It allows us to use any arbitrary latent distributions that we can sample Prior fitting termì„ ganì„ ì‚¬ìš©í•˜ì—¬ ë¶„í¬ë¥¼ ë§ì¶”ì–´ì¤Œ samplingì´ ê°€ëŠ¥í•œ ì–´ë– í•œ ë¶„í¬ë„ ë§ì¶œìˆ˜ìˆë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤ GAN discriminatorê°€ ì ì°¨ ë°œì „í•´ ë‚˜ê°€ë©´ì„œ generatorë„ ë”°ë¼ì„œ ì„±ëŠ¥ì´ ì˜¬ë¼ê°€ëŠ” ìƒìƒì˜? GAN vs VAE GANì˜ Objective For discriminator where the optimal discriminator is For generator GANì˜ objectiveëŠ” ë‚˜ì˜ true generative distributionê³¼ ë‚´ê°€ í•™ìŠµí•˜ê³ ìí•˜ëŠ” generatorì‚¬ì´ì˜ Jenson-Shannon Divergenceë¥¼ ìµœì†Œí™” í•˜ëŠ”ê²ƒì´ë‹¤ DCGAN Info-GAN í•™ìŠµì‹œì— classë¼ëŠ” randomí•œ one-hot vectorë¥¼ ë§¤ë²ˆ ì§‘ì–´ ë„£ì–´ì¤€ë‹¤ generationì‹œì— ganì´ íŠ¹ì •ëª¨ë“œì— ì§‘ì¤‘í•  ìˆ˜ ìˆê²Œë”í•´ì¤€ë‹¤ Text2Image í…ìŠ¤íŠ¸ë¡œ ì´ë¯¸ì§€ë¥¼ generateí•˜ëŠ” ì—°êµ¬ modelì´ ë§¤ìš° ë³µì¡í•˜ë‹¤â€¦â€¦. CycleGAN ì´ cycle consistency lossê°€ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤","link":"/2021/02/05/2021-02-05-Boostcamp15.1/"},{"title":"RNN2","text":"Transformer Sequential Model ìœ„ì™€ ê°™ì€ ë¬¸ì œë¡œ, RNNê°™ì´ sequentialí•œ ë¬¸ì œë“¤ì„ í•´ê²°í•  ë•Œ, ì¤‘ê°„ì— ë‹¨ì–´ê°€ ë¹ ì§€ê±°ë‚˜ í•˜ë©´ í•´ê²°í•˜ê¸°ê°€ ì–´ë ¤ì›€ â€”&gt; ì—¬ê¸°ì„œ ë‚˜ì˜¨ê²Œ Attentionì„ ì‚¬ìš©í•œ Transformer Transformer Transformer is the first sequence transduction model based entirely on attention RNNì²˜ëŸ¼ ì¬ê·€ì ì¸ê²Œ ì•„ë‹ˆë¼ based on attention from a birdâ€™s-eye view, this is what the Transformer does for machine translation tasks ê²°êµ­ì€ sequence to sequnece (ë¶ˆì–´ -&gt; ì˜ì–´ )machine translation ì…ì¶œë ¥ sequence ëŠ” ìˆ«ìê°€ ë‹¤ë¥¼ìˆ˜ ìˆë‹¤ ëª¨ë¸ì€ í•˜ë‚˜ì„. 100ê°œê°€ ë“¤ì–´ê°€ë„ 100ë²ˆ ì¬ê·€ì ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ”ê²Œì•„ë‹ˆë¼ í•œë²ˆì— nê°œë¥¼ ì²˜ë¦¬ generationí• ë•ŒëŠ” 1ë‹¨ì–´ì”© ë§Œë“¤ê²Œ ëœë‹¤ ë™ì¼í•œêµ¬ì¡°ë¥¼ ê°€ì§€ì§€ë§Œ ê³µìœ í•˜ì§€ ì•ŠëŠ” encoderì™€ decoderê°€ stackë˜ì–´ìˆë‹¤ encoderê°€ ë°”ë€”ìˆ˜ ìˆëŠ” nê°œì˜ ë‹¨ì–´ë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ëŠ”ì§€ 1ê°œì˜ encoderì— nê°œì˜ ë‹¨ì–´ê°€ í•œë²ˆì— ë“¤ì–´ê°„ë‹¤ self attention + Feed Forward Neural Network -&gt;next encoder Self- Attention is the cornerstone of Transformer Transformer","link":"/2021/02/04/2021-02-04-Boostcamp14.2/"},{"title":"Improving Language Understanding by Generative Pre-Training","text":"ì´ë²ˆì—ëŠ” openaiì—ì„œ ë°œí‘œí•œ ë…¼ë¬¸ì¸ GPTë¥¼ reviewí•´ë³´ê² ë‹¤ GPT3ëŠ” ì´ì „ì— reviewí•œ transformerêµ¬ì¡°ë¥¼ í™œìš©í•˜ì—¬ Language understandingì„ íš¨ê³¼ì ìœ¼ë¡œ ë§Œë“¤ì—ˆë‹¤. Abstract ìì—°ì–´ë¥¼ ì´í•´ëŠ” textì¶”ë¡ , ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µ, ì˜ë¯¸ì˜ ìœ ì‚¬ì„± í‰ê°€, ë¬¸ì„œë¶„ë¥˜ë“±ì„ í¬í•¨í•˜ê³  ìˆë‹¤. ë¼ë²¨ë§ ë˜ì§€ ì•Šì€ textë“¤ì„ ë§¤ìš° ë„˜ì²˜ë‚˜ì§€ë§Œ, íŠ¹ì • taskì˜ í•™ìŠµì„ ìœ„í•´ labedëœ textë“¤ì€ ë§¤ìš° ì ê¸°ë•Œë¬¸ì— ì¢‹ì€ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ”ê²ƒì€ ë§¤ìš° í˜ë“¤ë‹¤. Language ëª¨ë¸ì„ unlabledëœ textë¡œ generative pretrainì„ í•œì´í›„ ê°ê°ì˜ taskì— ë§ê²Œ fine-tunningì„ í•˜ì˜€ë‹¤. ì´ëŸ¬í•œ ë§ì€ unlabed textë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•˜ì˜€ë‹¤. ì´ì „ì˜ ì—°êµ¬ì™€ëŠ” ë‹¬ë¦¬,í•„ìš”í•œ taskì— fine-tuningí•˜ì—¬ ì‘ìš©í•˜ëŠ” ê²ƒì´ ë§¤ìš° íš¨ê³¼ì ì´ë‹¤. 1. Introduction Raw textë¥¼ ì‚¬ìš©í•˜ì—¬ íš¨ê³¼ì ì¸ NLP í•™ìŠµì„ í•˜ê¸°ìœ„í•´ì„œëŠ” ì§€ë„í•™ìŠµì— ëŒ€í•œ ì˜ì¡´ì„±ì„ ì™„í™”í•´ì•¼ í•œë‹¤. ë§ì€ ë”¥ëŸ¬ë‹ ë°©ë²•ë“¤ì€ labeledëœ dataë¥¼ ì‚¬ìš©í•´ì•¼ í•´ì„œ í•œê³„ê°€ ì¡´ì¬í•œë‹¤. ì´ëŸ¬í•œ ìƒí™©ì—ì„œ unlabedëœ dataëŠ” ì‹œê°„ê³¼ ë…¸ë ¥ì´ í•„ìš”í•œ annotationì„ ëª¨ìœ¼ëŠ” ì‘ì—…ë“¤ì„ ëŒ€ì²´í•  ìˆ˜ ìˆë‹¤. ë§Œì•½ ê³ ë ¤ê°€ëŠ¥í•œ ì§€ë„ê°€ ê°€ëŠ¥í•œ ìƒí™©ì´ë¼ë©´, unsupervised ë°©ë²•ì€ modelì˜ ì„±ëŠ¥ì„ ì¦ê°€ì‹œí‚¬ ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ ë°©ì‹ì€ pretrainedëœ word embeddingì„ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ë†’ì´ëŠ”ê²ƒê³¼ ë¹„ìŠ·í•œ ì´ìœ ì´ë‹¤. unlabedëœ dataë¡œ word-levelì˜ ì •ë³´ë³´ë‹¤ ë§ì€ ì •ë³´ë¥¼ í™œìš©í•˜ëŠ”ê²ƒì€ 2ê°€ì§€ ì´ìœ ì—ì„œ ë§¤ìš° ì–´ë µë‹¤ ì–´ë– í•œ ì¢…ë¥˜ì˜ optimization objectiveê°€ ê°€ì¥ íš¨ê³¼ì ìœ¼ë¡œ textë¥¼ í‘œí˜„í• ìˆ˜ ìˆì„ê¹Œ ê°€ ë§¤ìš° unclearí•˜ë‹¤ ìš°ë¦¬ê°€ ì›í•˜ëŠ” íŠ¹ì • taskì— íš¨ê³¼ì ìœ¼ë¡œ ì ìš©í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ì˜ê²¬ì´ ì¼ì¹˜ê°€ ë˜ì§• ì•Šì•˜ë‹¤. í˜„ì¬ ì¡´ì¬í•˜ëŠ” ë°©ë²•ì€ modelì— íŠ¹ì •í•œ task-specificí•œ ë³€í™”ë¥¼ ê°€í•˜ëŠ” ê²ƒê³¼, ë³µì¡í•œ í•™ìŠµë°©ë²•,ê·¸ë¦¬ê³  í•™ìŠµì„ ë„ì™€ì£¼ëŠ” ëª‡ëª‡ learning objectiveë“¤ì„ ë„£ì–´ì£¼ëŠ”, ì´ëŸ¬í•œ ë°©ë²•ë“¤ì˜ combinationì´ë‹¤ ì´ëŸ¬í•œ ë¶ˆí™•ì‹¤ì„±ì€ language processingì—ì„œì˜ íš¨ê³¼ì ì¸ semi-supervised learningì„ ë°œì „ì‹œí‚¤ê¸° í˜ë“¤ê²Œ ë§Œë“ ë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” unsupervised pre-trainingê³¼ supervised fine-tunningì„ ì¡°í•©í•œ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ semi-supervised approachë¥¼ í•˜ì˜€ë‹¤. ëª©ì ì€ ê°€ì¥ ë³´í¸ì ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ì•½ê°„ì˜ ì‘ìš©ìœ¼ë¡œ ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì ìš©ì‹œí‚¤ëŠ”ê²ƒì´ë‹¤. 2-stageë¡œ ë‚˜ëˆ„ì–´ trainí•˜ì˜€ë‹¤ ì´ˆê¸° parameterë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ unlabeled dataë¥¼ ì‚¬ìš©í•˜ì—¬ pre-train í•˜ì˜€ë‹¤ / with transformer ìš°ë¦¬ëŠ” ì´ parameterë“¤ì„ íŠ¹ì •í•œ taskì— ë§ëŠ” supervised objective í•™ìŠµì— ì‚¬ìš©í•˜ì˜€ë‹¤. ë˜í•œ modelì—ì„œ Transformerë¥¼ ì‚¬ìš©í•˜ì—¬ long-term dependenciesë¥¼ í•´ê²°í•˜ì˜€ë‹¤. 2. Related WorkSemi-supervised learning for NLP ìš°ë¦¬ì˜ workëŠ” Semi-supervied learningì˜ ë²”ì£¼ì•ˆì—ìˆë‹¤. ì´ sslì€ sequence labeling, text ë¶„ë¥˜ë“±ì— ì“°ì´ë©´ì„œ í° ê´€ì‹¬ì„ ë°›ê³  ìˆë‹¤. ê°€ì¥ ì´ˆê¸°ì—ëŠ” unlabeled dataë¥¼ supervised learningì˜ featureë¡œ ì‚¬ìš©í•˜ì—¬ wordë‚˜ phrase levelì˜ í†µê³„ë¥¼ ê³„ì‚°í•˜ëŠ”ë° ì‚¬ìš©ë˜ì—ˆë‹¤. ìµœê·¼ ëª‡ë…„ë™ì•ˆ word-embeddingì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ ë°í˜€ëƒˆë‹¤. ì´ëŸ¬í•œ ì ‘ê·¼ì€ word-levelì˜ ì •ë³´ë¥¼ íŠ¹ì •í•œ high-levelì— ë§ì¶”ì–´ ì¤€ë‹¤. ìµœê·¼ì—ëŠ” word-levelì´ ì•„ë‹Œ phraseë‚˜ sentence levelì˜ embeddingì„ ì‚¬ìš©í•˜ì—¬ textë¥¼ ë‹¤ì–‘í•œ target taskì˜ vector representationì„ ë‚˜íƒ€ë‚´ ì£¼ì—ˆë‹¤. Unsupervised pre-training Unsupervised pre-trainingì€ supervised learningì„ ë°”ê¾¸ëŠ”ê±° ë³´ë‹¤ëŠ” ì¢‹ì€ initializationì„ ì°¾ëŠ”ê²Œ ëª©ì ì´ë‹¤. ê°ê°ì˜ ì—°êµ¬ë“¤ì€ image classificationê³¼ regression taskì˜ ê¸°ìˆ ì´ ì‚¬ìš©ë˜ì—ˆë‹¤. Pre-trainingì€ ì •ê·œí™” ê³¼ì •ì—ì„œ generalizationì„±ëŠ¥ì„ ì˜¬ë ¤ì¤€ë‹¤. ìš°ë¦¬ì˜ ì—°êµ¬ëŠ” language modelingìœ¼ë¡œ modelì„ pre-trainí•œí›„ taskì— ë§ê²Œ fine-tuningí•´ì£¼ëŠ” ê²ƒì´ë‹¤. Pre-trainingì´ ì–¸ì–´ì ì¸ ì •ë³´ë¥¼ ì˜ ì¡ì•„ë‚¼ìˆ˜ ìˆì§€ë§Œ,ì´ì „ì—°êµ¬ì—ì„œ ì‚¬ìš©ëœ LSTMì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ê¸´ dataë¥¼ í•´ì„í•˜ì§€ ëª»í•œë‹¤ëŠ” ë‹¨ì ì´ ì¡´ì¬í•œë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” Transformerë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.ë˜ë‹¤ë¥¸ ì—°êµ¬ì—ì„œëŠ” ëª‡ëª‡ ë³´ì¡°ì ì¸ featureë“¤ì„ ì‚½ì…í•´ì£¼ì–´ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ì§€ë§Œ, ì´ëŠ” ìƒˆë¡œìš´ parameterì˜ ì¦ê°€ë¥¼ ì•¼ê¸°í•œë‹¤. ìš°ë¦¬ì˜ GPTëŠ” transferê³¼ì •ì—ì„œ ìµœì†Œí•œì˜ ìˆ˜ì •ë§Œì„ í•„ìš”ë¡œ í•œë‹¤. Auxiliary training objectives ì—¬ëŸ¬ ë³´ì¡°ì ì¸ unsupervised trainingì€ semi-supervised learningì˜ ëŒ€ì±„ì ì¸ í˜•íƒœì´ë‹¤. ì´ì „ì˜ ì—°êµ¬ì—ì„œëŠ” ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ë³´ì¡°ì ì¸ NLPë°©ë²•ë¡ (POS tagging, chunking,ë“±ë“±ë“±)ì„ ì‚¬ìš©í•˜ì˜€ë‹¤. ìµœê·¼ ë˜ë‹¤ë¥¸ ì—°êµ¬ëŠ” ë³´ì¡°ì ì¸ language modelë¥¼ ì¶”ê°€í•˜ì—¬ sequence labelingì˜ ì„±ëŠ¥í–¥ìƒì„ ì´ì•¼ê¸° í•˜ì˜€ë‹¤. 3. Frameworkí•™ìŠµê³¼ì •ì€ 2ê°œì˜ stageë¡œ ë‚˜ëˆ„ì–´ì ¸ ìˆë‹¤ unlabeledëœ í° ë§ë­‰ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ë²”ìš©ì ì¸ language modelì„ í•™ìŠµí•˜ëŠ” stage ì´í›„ labeled dataë¥¼ ì‚¬ìš©í•œ fine-tuning stage 3.1 Unsupervised pre-training unsupervisedì˜ tokenë“¤ = ì´ ì£¼ì–´ì§€ê³ , ì´ì–´ì§€ëŠ” likelihoodë¥¼ maximizeí•˜ê¸°ìœ„í•´ ë³´í¸ì ì¸ language modelì„ ì‚¬ìš©í•œë‹¤. këŠ” contextì˜ sizeì´ê³ , conditional prob PëŠ” NNì„ ì‚¬ìš©í•˜ì—¬ modeled ì´ë“¤ì€ ëª¨ë‘ SGDë¥¼ ì‚¬ìš©í•˜ì—¬ trainingí–ˆë‹¤. multi-layer Transformer decoderë¥¼ ì‚¬ìš©í–ˆë‹¤. ì´ modelì€ input context tokensì— multi-headed self-attentionì„ í™œìš©í•˜ì˜€ê³ , ì´í›„ì— position-wise feedforward layerë¥¼ ì ìš©í•˜ì—¬ target tokenì—ëŒ€í•œ output distributionì„ êµ¬í•œë‹¤. UëŠ” tokenì˜ context vectorì´ê³ , nì€ layerì˜ ìˆ«ì, WeëŠ” token embedding matrix, WpëŠ” position embedding matrixì´ë‹¤. 3.2 Supervised fine-tuning modelì„ trainí•œí›„, supervised target testì— ë§ì¶”ì–´ì„œ parameterë¥¼ ì ìš©í•œë‹¤. labeledëœ dataset C(ê°ê°ì€ input tokenì˜ sequenceë¡œ ì´ë£¨ì–´ì§ ((x^1^,â€¦,x^m^) and label y )) Inputì€ pre-trainedëœ modelì„ í†µê³¼í•˜ì—¬ ìµœì¢… transformer blockì˜ activationì¸ hl^m^ì„ ì–»ì–´ë‚´ê³ , ì´í›„ì— linear output layerì— Wyì™€ í•¨ê»˜ ë“¤ì–´ê°„ë‹¤. ì´ëŠ” ì´í›„ì˜ objectiveë¥¼ maximizeí•˜ê²Œ í•œë‹¤. ë³´ì¡°ì ì¸ ì¥ì¹˜ë¡œ language modelingì„ ì‚¬ìš©í•˜ì—¬ fine-tuningì„ í•˜ëŠ”ê²ƒì€ (1) generalizationì„±ëŠ¥ì„ ë†’íŒë‹¤ (2) ìˆ˜ë ´ì†ë„ë¥¼ ë†’íŒë‹¤. ìš°ë¦¬ëŠ” ì•„ë˜ì˜ objectiveë¥¼ optimizeí•œë‹¤ Fine-tuningì¤‘ì— ìœ ì¼í•œ extra parameterì€ Wyì™€ êµ¬ë¶„tokenì„ ìœ„í•œ embeddingì´ë‹¤. 3.3 Task-specific input transformations text classificationê°€ ê°™ì€ ëª‡ëª‡ ë¶„ì•¼ì—ì„œ, ìœ„ì—ì„œ ë¬˜ì‚¬í–ˆë˜ëŒ€ë¡œ ìš°ë¦¬ì˜ modelì„ fine-tuneí•  ìˆ˜ ìˆì—ˆë‹¤. ì§ˆì˜ì‘ë‹µê³¼, textual entailmentì™€ ê°™ì€ ë¬¸ì œì—ëŠ” inputì„ ordered sentence pairs, triplets of document, question, answerìœ¼ë¡œ í•´ì£¼ì—ˆë‹¤. ìš°ë¦¬ì˜ pre-trained modelì´ ì—°ì†ì ì¸ sequenceì—ì„œ í•™ìŠµë˜ì—ˆê¸° ë•Œë¬¸ì—, ì´ëŸ¬í•œ ë¬¸ì œë“¤ì—ëŠ” ì•½ê°„ì˜ ë§ì¶¤ ìˆ˜ì •ì´ í•„ìš”í•˜ë‹¤. ì´ì „ì˜ ì—°êµ¬ë“¤ì€ transffered representationìœ„ì— íŠ¹ì • architectureë¥¼ ì‚½ì…í•˜ëŠ” í˜•íƒœë¡œ í•™ìŠµí•´ì™”ë‹¤. ì´ëŠ” ë§ì€ì–‘ì˜ cutomizationì´ í•„ìš”í•˜ë©° ì´ëŸ¬í•œ ì¶”ê°€ì ì¸ íŠ¹ì • architectureì—ëŠ” transfer learningì„ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë‹¤. ëŒ€ì‹  ìš°ë¦¬ëŠ” traveral-stple approach(inputì„ ì •ë ¬ëœ sequenceë¡œ ë§Œë“¤ì–´)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìš°ë¦¬ì˜ pre-trained modelì´ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•˜ì˜€ë‹¤. ì´ëŸ¬í•œ inputì˜ ì¡°ì •ì€ ë¬¸ì œìƒí™©ì— ë”°ë¼ architectureì˜ í° ìˆ˜ì •ì„ í•˜ì§€ ì•Šì•„ë„ ë˜ê²Œ í•œë‹¤. ëª¨ë“  transformationì€ randomly initializedëœ start,ending tokenì„ í¬í•¨í•œë‹¤. Textual entailment(ë¬¸ì¥ì˜ í¬í•¨ê´€ê³„) : ì „ì œ pì™€ ê°€ì„¤ h ì¤‘ê°„ì— delimiter token $ë¥¼ ì‚½ì…í•˜ì—¬ í•©ì³ì£¼ì—ˆë‹¤. Similarity (ë¬¸ì¥ì˜ ìœ ì‚¬ë„ í‰ê°€) : ë‘ê°œì˜ ë¹„êµëŒ€ìƒì€ ìˆœì„œê°€ ë”±íˆ ì—†ë‹¤. í•œë§ˆë””ë¡œ ë™ë“±í•œ levelì—ì„œ ë¹„êµí•´ì•¼ ë˜ê¸° ë•Œë¬¸ì— ëª¨ë“  ê°€ëŠ¥í•œ ìˆœì„œë¥¼ ì‚¬ìš©í•˜ê³  transformerì´í›„ì— ë‚˜ì˜¤ëŠ”2ê°œì˜ hl^m^ ì„ í•©ì³ì¤€ë‹¤. Question Answering and Commonsense Reasoning (ì§ˆì˜ì‘ë‹µ) : 3. Model Atchitecture language model -&gt; labelì´ í•„ìš”ê°€ ì—†ë‹¤ ì£¼ì–´ì§„ ë‹¨ì–´ë“¤ì„ ê°€ì§€ê³  ë‹¤ìŒë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” Generative model Generative model dataê°€ ë§ì•„ ì§ˆìˆ˜ë¡ ì •í™•ë„ê°€ ë†’ì•„ì§„ë‹¤ Discriminative model íƒ€ì´íƒ€ë‹‰ê°™ì€ ë°ì´í„°ê°€ ë§ì§€ ì•Šì„ë•Œ íŒ¨í„´íŒŒì•…ì´ ì‰¬ì›Œì„œ ë§ì´ë“¤ ì‚¬ìš©í•œë‹¤ í•œì •ëœ dataì— ê³¼ì í•© ë˜ê¸°ê°€ ì‰½ë‹¤ sampleëœ dataë¡œëŠ” ì™œê³¡ëœ íŒë‹¨ì„ í•  ìˆ˜ ìˆë‹¤ GPTëŠ” unlabeledëœ dataë¡œ Pretraining LM finefuning ë°ì´í„°ë§Œ taskê´€ë ¨ë°ì´í„°ë¡œ í•™ìŠµ modelì€ ê·¸ëŒ€ë¡œ Naural Language Inference -&gt; entailment contradictioníŒŒì•… ì§ˆì˜ì‘ë‹µ ë¹„ìŠ·í•œ ë¬¸ì¥ íŒë³„ ì£¼ì–´ì§„ ë¬¸ì¥ì„ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ë¹„ì§€ë„ í•™ìŠµ labelì´ ìˆëŠ” dataë¡œ fine tunningí•œë‹¤. ê¸°ì¡´ language model í•™ìŠµ ê³µì‹ê³¼ ê°™ë‹¤ transformerì˜ decoderë¡œ êµ¬ì„± layerì¶”ê°€ì—†ì´ pretrained LM byte pair embeddingì„ ì‚¬ìš©í•˜ì˜€ë‹¤ ì‹ ì¡°ì–´ ì˜¤íƒˆìì— ì•½í•œ word embeddingì´ ì•„ë‹Œ byte pair. â€”â€“&gt; hack,able, deep, learn, ing ì´ëŸ°ì‹ìœ¼ë¡œ embeddingì„ í•˜ì˜€ë‹¤. dataê°€ ì£¼ì–´ì¡Œì„ë–„","link":"/2021/02/14/2021-02-14-GPT/"},{"title":"Sequence to sequence with Attention","text":"Sequence to sequence \\ Seq2Seq ModelEx) Are you free tomorrow? ì„œë¡œ paramterë¥¼ shareí•˜ì§€ ì•ŠëŠ” 2ê°œì˜ ë³„ê°œì˜ RNN modelì„ (ë³´í†µ LSTM) ì“´ë‹¤. ê°ê°ì˜ RNNì„ Decoder, Encoderë¡œ ì‚¬ìš©í•œë‹¤. Encoderì˜ ë§ˆì§€ë§‰ë‹¨ì˜ outputì„ vertorize ì‹œì¼œì¤€í›„ decoderì˜ inputì—ëŠ” SOS token, hidden stateì—ëŠ” encoderì˜ outputì„ ë„£ì–´ì¤€ë‹¤. Seq2Seq with Attentionì•ì—ì„œì˜ RNNì„ ì‚¬ìš©í•œ modelì€ hidden state vectorì˜ dimesionì´ ì •í•´ì ¸ ìˆì–´ì„œ ì…ë ¥ë¬¸ì¥ì˜ ê¸¸ì´ê°€ ê¸¸ì–´ì§€ë©´ ë§ˆì§€ë§‰ time stepì— ìˆëŠ” hiddenstate vectorì— ì•ì„œ ë‚˜ì™”ë˜ ë§ì€ ì •ë³´ë“¤ì´ ì˜ ë‹´ê²¨ì ¸ ìˆì§€ ì•Šë‹¤. ì•„ë¬´ë¦¬ ì´ LSTMì—ì„œ longterm dependencyë¥¼ í•´ê²°í•˜ë ¤ í•´ë„êµ¬ì¡°ìƒì˜ ë¬¸ì œ ë•Œë¬¸ì— í•´ê²°í•˜ê¸°ì— ë§¤ìš°í˜ë“¤ë‹¤ ë”°ë¼ì„œ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ seq2seqì—ì„œ Attentionì„ í™œìš©í•  ìˆ˜ ìˆë‹¤. Attentionì€ encoderì˜ ê°ê°ì˜ hidden state vectorë¥¼ ì „ì²´ì ìœ¼ë¡œ decoderì— ì œê³µí•´ì£¼ê³  decoderì—ì„œëŠ” ê·¸ë•Œê·¸ë•Œ í•„ìš”í•œ encoderì˜ hidden state vectorë¥¼ ê°€ì ¸ê°€ì„œ ì‚¬ìš©í•œë‹¤ decoderì˜ hidden state vectorê°€ encoderì˜ ì–´ë–¤ hidden state vectorë¥¼ ê°€ì ¸ì˜¬ì§€ë¥¼ ê²°ì •í•˜ê²Œ ëœë‹¤. ì´ê±°ëŠ” ê°ê°ì„ ë‚´ì í•´ë³´ì•„ì„œ, ë‚´ì ì— ê¸°ë°˜í•œ ìœ ì‚¬ë„ë¥¼ íŒë³„í•˜ê²Œ ë˜ê³  ì´ê²°ê³¼ë¥¼ softmaxì— í†µê³¼ ì‹œì¼œì„œ í™•ë¥ ê°’ì„ ì–»ì–´ë‚´ê³  ì´ë¥¼ ê°ê°ì˜ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©í•˜ì—¬ ì´ë“¤ì˜ ê°€ì¤‘í‰ê· ìœ¼ë¡œì„œ ë‚˜ì˜¤ëŠ” í•˜ë‚˜ì˜ encoding vectorë¥¼ ì–»ì–´ë‚¼ìˆ˜ ìˆë‹¤!!!!!! ì´ëŸ¬í•œ ê°€ì¤‘í‰ê· ìœ¼ë¡œ ë‚˜ì˜¨ í•˜ë‚˜ì˜ vectorë¥¼ ìš°ë¦¬ëŠ” context vectorë¼ê³  ë¶€ë¥¸ë‹¤. ì´í›„ì— decoder hidden state vectorì™€ context vectorê°€ concatnate ë˜ì–´ output layerì˜ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ê²Œ ë˜ê³  ë‹¤ìŒë‚˜ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆê²Œ ëœë‹¤ ì´ëŸ¬í•œ ê³¼ì •ë“¤ì„ EOSê°€ ë‚˜ì˜¬ë•Œ ê¹Œì§€ ë°˜ë³µí•œë‹¤. ì˜ëª»ëœ ë‹¨ì–´ë¥¼ ì „ë‹¨ê³„ì—ì„œ ì˜ˆì¸¡ì„ í•˜ë”ë¼ë„ ë‹¤ìŒë‹¨ê³„ì—ëŠ” ì˜¬ë°”ë¥¸ ground truthë¥¼ ë„£ì–´ì£¼ê¸° ë–„ë¬¸ì— í•˜ë‚˜ê°€ í‹€ë ¤ë„ ì´í›„ê°€ ë§ê°€ì§€ì§€ ì•ŠëŠ”ë‹¤. í•™ìŠµì´ ëë‚œí›„ ì´ ì˜ëª»ëœ ë‹¨ì–´ë¥¼ ë‹¤ì‹œ ë„£ì–´ì¤€ë‹¤. ë˜í•œ Itâ€™s teacher forcing. Teacher forcingì´ ì•„ë‹Œ ë°©ì‹ì´ í•™ìŠµí›„ì— ìš°ë¦¬ê°€ ì‹¤ì œë¡œ ì‚¬ìš©í• ë•Œì™€ ë¹„ìŠ·í•˜ë‹¤. Teacher forcingë•ŒëŠ” ground truthë¥¼ ë„£ì–´ì£¼ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì—í•™ìŠµì†ë„ê°€ ë¹ ë¥´ë‹¤ í•™ìŠµì˜ ì „ë°˜ë¶€ì—ëŠ” teacher forcingì„ ì‚¬ìš©í›„ ì–´ëŠì •ë„ í•™ìŠµì´ ë˜ë©´, ì´ì „ì˜ outputì„ ë‹¤ì‹œ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ì§„í–‰í•œë‹¤. ì´ì²˜ëŸ¼ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ê³¼ì •ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë‚´ì ì€, 3ê°€ì§€ì˜ ì¢…ë¥˜ë¡œ ê³„ì‚°í•´ ë‚¼ ìˆ˜ ìˆë‹¤. 2ë²ˆì§¸ì¸ general ë°©ì‹ìœ¼ë¡œ ê²Œì‚°í•˜ëŠ” ê²ƒì„ í–‰ë ¬ìœ¼ë¡œ ìƒê°í•´ë³´ì.ë‚´ì ì„ ê¸°ë°˜í•œ ê³„ì‚°ì„ í–‰ë ¬ì˜ ê³±ìœ¼ë¡œ ìƒê°í•´ë³´ë©´, ëŒ€ê°í–‰ë ¬ì˜ ì„±ë¶„ë“¤ì€ ê°™ì€ ì°¨ì›ë¼ë¦¬ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ê³ , ë‚˜ë¨¸ì§€ ê°’ë“¤ì€ ë‹¤ë¥¸ ì°¨ì›ë¼ë¦¬ì˜ ê³±í•´ì§„ ê°’ë“¤ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤ ì´ì²˜ëŸ¼ ê°„ë‹¨í•œ ë‚´ì ìœ¼ë¡œ ì •ì˜ëœ í˜•íƒœì˜ ìœ ì‚¬ë„ë¥¼ ê·¸ê°€ìš´ë° í•™ìŠµê°€ëŠ¥í•œ parameterë¥¼ ì¶”ê°€í•¨ìœ¼ë¡œì„œ ìƒˆë¡­ê²Œ scoreë¥¼ ê³„ì‚°í–ˆë‹¤. ì´ê²Œ ë°”ë¡œ generalí•œ dot productì´ë‹¤. ë‹¤ìŒìœ¼ë¡œ concatì„ ì‚¬ìš©í•œ score ì¸¡ì • ë°©ì‹ì„ ë³´ì ì´ì²˜ëŸ¼ 2ê°œì˜ vectorë¥¼ concatì‹œì¼œ MLPì˜ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì¤€ í›„ non linear activation functionì„ ì ìš©í•˜ì—¬ ê°’ì„ êµ¬í•´ë‚¸ë‹¤. ì´ìˆ˜ì‹ì„ ê°„ë‹¨í•˜ê²Œ ë³´ë©´ WaëŠ” 1ë²ˆì§¸ layerì˜ ê°€ì¤‘ì¹˜, ê·¸ì´í›„ì— tanhë¥¼ ì ìš©í•œ í›„ vë¥¼ ê³±í•´ì£¼ëŠ”ë° ì´ëŠ” ìš°ë¦¬ê°€ ìµœì¢…ì ìœ¼ë¡œ ì–»ì–´ì•¼í•  outputì´ scalarê°’ì´ê¸° ë–„ë¬¸ì— vëŠ” rowì˜ í˜•íƒœë¥¼ ë„ì–´ì•¼ í•œë‹¤. ë”°ë¼ì„œ tranposeë¥¼ ì‹œì¼œì¤€ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì´ë“¤ì˜ paramterì€ ì–´ë– íŒ ë°©ì‹ìœ¼ë¡œ updateë ê¹Œ? ê²°êµ­ì€ ì´ëŸ¬í•œ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ëŠ”ë° í•„ìš”í•œ parameterë“¤ë˜í•œ backpropagationì„ í†µí•˜ì—¬ ì„ í˜•ë³€í™˜ í–‰ë ¬ë“¤ì´ í•™ìŠµë˜ê²Œ ëœë‹¤. Attention is great Attention significantly impoves NMT performace ì–´ë– í•œ í•œ ë¶€ë¶„ì— ì§‘ì¤‘í•  ìˆ˜ ìˆê²Œ í•´ì£¼ì—ˆë‹¤ It solves bottle neck problem encoderì˜ ë§ˆì§€ë§‰ì„ ì‚¬ìš©í–ˆì–´ì•¼ í•´ì„œ ìƒê¸°ëŠ” long term dependencyë¥¼ í•´ê²° Gradient vanishingì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ì˜€ë‹¤. Attention provides some interpretability ìš°ë¦¬ê°€ transformê³¼ì •ì—ì„œ ëª¨ë¸ì´ ì–´ë– í•œ ë¶€ë¶„ì— ì§‘ì¤‘ í–ˆëŠ”ì§€ë¥¼ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. Allignmentë¥¼ NNì´ ìŠ¤ìŠ¤ë¡œ ë°°ìš°ëŠ” í˜„ìƒì„ ë³´ì—¬ì£¼ê²Œ ëœë‹¤. Beam search testê³¼ì •ì—ì„œ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì„ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” í•˜ë‚˜ì˜ ë°©ë²• Greedy decodingê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§€ëŠ” ë‹¨ì–´ 1ê°œë¥¼ ì„ íƒí•˜ëŠ” ë°©ë²• ì´ë ‡ê²Œ ë˜ë©´ ì–´ë– í•œ ë‹¨ì–´ë¥¼ ì˜ëª» ìƒì„±í•´ë‚´ì—ˆì„ë•Œ ë‹¤ì‹œ ë’¤ë¡œ ëŒì•„ê°ˆìˆ˜ ì—†ì–´ ìµœì ì˜ ì˜ˆì¸¡ê°’ì„ ë‚´ì§€ ëª»í•˜ê²Œ ëœë‹¤ ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ë‹¤ì–‘í•œ ë°©ë²•ë“¤ì´ ì œì‹œëœë‹¤ Exhaustive Searchì²«ë²ˆì§¸ ìƒì„±í•˜ëŠ” ë‹¨ì–´ê°€ ê°€ì¥í° í™•ë¥ ì´ì˜€ë‹¤ê³  í•´ë„ ë’·ë¶€ë¶„ì—ì„œ ë‚˜ì˜¤ëŠ” í™•ë¥ ê°’ ê°€ì¥í° í™•ë¥ ê°’ì´ ì•„ë‹Œ ê²½ìš°ê°€ ë°œìƒë ìˆ˜ê°€ ìˆë‹¤. ì´ëŠ” ê²°êµ­ time step t ê¹Œì§€ì˜ ê°€ëŠ¥í•œ ëª¨ë“ ê²½ìš°ë¥¼ ë”°ì ¸ì„œ ì´ëŠ” ê³§ vocabê°€ì§€ìˆ˜ê°€ ë˜ê³  V^t^ê°€ ê°€ëŠ¥í•œ ëª¨ë“  ê²½ìš°ì˜ ìˆ˜ì´ë‹¤. ì´ëŠ” ë„ˆë¬´ í° ìˆ«ìì´ê¸° ë•Œë¬¸ì— beam searchë¥¼ ì“°ê²Œëœë‹¤ Beam searchë§¤ time stepë§ˆë‹¤ ëª¨ë“  ê²½ìš°ì˜ ìˆ˜ë¥¼ ê³ ë ¤í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼, ìš°ë¦¬ê°€ ì •í•´ë†“ì€ kê°œì˜ ê°€ëŠ¥í•˜ ê°€ì§“ìˆ˜ë¥¼ ê³ ë ¤í•˜ê³  ë§ˆì§€ë§‰ê¹Œì§€ decodingì„ ì§„í–‰í•œí›„ kê°œì˜ candidateì¤‘ì—ì„œ ê°€ì¥í™•ë¥ ê°’ì´ ë†’ì€ê±¸ ì„ íƒí•˜ëŠ” ë°©ì‹ì´ë‹¤. ì´ë¥¼ ìš°ë¦¬ëŠ” hypothesis (ê°€ì„¤)ì´ë¼ê³  ë¶€ë¥¸ë‹¤ këŠ” beam sizeì´ ì¼ë°˜ì ìœ¼ë¡œ 5~10ìœ¼ë¡œ ì„¤ì •í•˜ê²Œ ëœë‹¤. í™•ë¥ ë“¤ì˜ ê³±ì…ˆ ì•ì— logë¥¼ ë¶™ì´ê²Œ ë˜ë©´ ê³±ë“¤ì´ ëª¨ë‘ ë§ì…ˆì´ ëœë‹¤. ì—¬ê¸°ì„œ logí•¨ìˆ˜ ë‹¨ì¡°ì¦ê°€ì´ê¸° ë•Œë¬¸ì—, í°ê°’ì´ í°ê°’ì„ ê°€ì§„ë‹¤. ex) k = 2 kê°€ 2ì´ê¸° ë•Œë¬¸ì— ê°€ì¥ í™•ë¥ ê°’ì´ ë†’ì€ 2ê°œì˜ ë‹¨ì–´ë¥¼ ë½‘ëŠ”ë‹¤ ì´ì¤‘ ê°’ì´ í°ê±¸ ê³„ì†í•´ì„œ ì„ íƒí•´ ë‚˜ê° greedyì˜ ê²½ìš° end tokenì´ ë‚˜ì™”ì„ë•Œê°€ ì¢…ë£Œì´ì§€ë§Œ, beam searchì—ì„œëŠ” ì„œë¡œë‹¤ë¥¸ ì‹œì ì—ì„œ end tokenì´ ìƒì„±ë˜ê¸° ë•Œë¬¸ì—, ê°ê°ì´ ëë‚ ë•Œë§ˆë‹¤ í•œê³³ì— ì €ì¥í•´ì¤€ë‹¤. ìš°ë¦¬ê°€ì •í•œ Të¼ëŠ” ì‹œê°„ê¹Œì§€ ìˆ˜í–‰í•˜ê±°ë‚˜, ì™„ë£Œëœ hypothesisê°€ nê°œê°€ ë˜ì—ˆì„ë•Œ beam searchë¥¼ ì¤‘ë‹¨í•œë‹¤. ìš°ë¦¬ê°€ ê³ ë ¤í•˜ëŠ” hypothesesì˜ ê¸¸ì´ê°€ ë‹¤ë¥¼ë•ŒëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì§§ì€ ê¸¸ì´ì˜ í™•ë¥ ì´ ë†’ì€ê²ƒì´ê³ , ê¸¸ë©´ ë‚®ì„ê²ƒì´ë‹¤. ì´ë¥¼ ê³ ë ¤í•´ ì£¼ê¸° ìœ„í•´ì„œëŠ” ê° joint probì„ ë¬¸ì¥ì˜ ê¸¸ì´ë¡œ ë‚˜ëˆ”ìœ¼ë¡œì„œ í•´ê²°í•´ì¤„ ìˆ˜ ìˆë‹¤. BLEU score ìƒì„± modelì˜ ì ìˆ˜ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•œ ì²™ë„ ê³ ì •ëœ ìœ„ì¹˜ì—ì„œ ì •í•´ì§„ ë‹¨ì–´ê°€ ë‚˜ì™€ì•¼ ëœë‹¤ëŠ” í‰ê°€ë°©ì‹ì€ ë§¤ìš° ë‚˜ìœ ë°©ì‹ì´ë‹¤. ex) Reference : Half of my heart is in Havana ooh na na Predicted : Half as my heart is in Obama ohh na Precision(ì‹¤ì œë¡œ ìœ„ì¹˜ìƒê´€ì—†ì´ ê²¹ì¹˜ëŠ” ë‹¨ì–´ê°€ ëª‡ê°œì¸ê°€) = #(correct words)/length_of_prediction = 7/9 Recall(ì¬í˜„ë¥ ) = #(correct words)/length_of_reference = 7/10 F-measure = (precision x recall) / 0.5(precision + recall) (ë‘ ê°’ë“¤ì˜ ì¡°í™”í‰ê· ) ë³´ë‹¤ ì‘ì€ ì‘ì€ ê°’ì— ê°€ê¹ê²Œ êµ¬í•˜ëŠ” ë°©ì‹ -&gt; ì¡°í™”í‰ê·  ì´ë ‡ê²Œ êµ¬í•œ ê°’ë“¤ì€ ìˆœì„œë¥¼ ë³´ì¥í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— BLEUê°€ ë‚˜ì™”ë‹¤. BiLingual Evaluation UnderstudyNgramì´ë€ê±¸ ì‚¬ìš©í–ˆë‹¤. ì—°ì†ëœ Nê°œì˜ ë‹¨ì–´ë¡œ ì´ë£¨ì–´ì§„ ë¬¸êµ¬ë¥¼ matchingí•˜ì—¬ì ìˆ˜ë¡œ ë°˜ì˜í•˜ì˜€ë‹¤.","link":"/2021/02/17/2021-02-15-Boostcamp18.1/"},{"title":"RNNì‹¬í™”1","text":"RNNì„œë¡œë‹¤ë¥¸ time stepì—ì„œ ë“¤ì–´ì˜¤ëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ ì²˜ë¦¬í• ë•Œ, ë§¤ë²ˆ ë°˜ë³µë˜ëŠ” ë™ì¼í•œ rnn moduleì„ í˜¸ì¶œí•œë‹¤. ê° ë‹¨ì–´ë³„ë¡œ í’ˆì‚¬ë¥¼ ì˜ˆì¸¡í•´ì•¼ ë˜ëŠ” ê²½ìš° -&gt; ë§¤ time stepë§ˆë‹¤ yë¥¼ outputìœ¼ë¡œ ì–´ë– í•œ ë¬¸ì¥ì˜ ê¸ë¶€ì •ì„ íŒë³„í•˜ëŠ” ê²½ìš° -&gt; ìµœì¢… time stepì˜ yë§Œì´ outputìœ¼ë¡œ ëª¨ë“  time stepì—ì„œ ê°™ì€ parameter Wë¥¼ ê³µìœ í•œë‹¤ ì£¼ì–´ì§„ vectorê°€ 3ì°¨ì›ì˜ ì…ë ¥ë²¡í„°ë¡œ ì£¼ì–´ì¡Œì„ë•Œ ht-1ì€ 2ì°¨ì›ì´ë¼ê³  ê°€ì •í•˜ì xtì™€ ht-1ë¥¼ ê°™ì´ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ì„œ fWì— ë„£ì–´ì£¼ë©´, htê°€ ë‚˜ì˜¤ê²Œ ëœë‹¤ í˜„ì¬ timestep tì—ì„œì¶”ê°€ì ì¸ outputlayerë¥¼ ë§Œë“¤ê³  htì— Whyë¥¼ ê³±í•´ì„œ ytë¥¼ ì–»ì–´ë‚¸ë‹¤. Types of RNNOne-to-one ì…ì¶œë ¥ ëª¨ë‘ê°€ sequence dataì¸ ê²½ìš°ì— ì…ì¶œë ¥ì´ ë‹¨ 1ê°œì¸ one-to-many image captioningì—ì„œ ì´ëŸ¬í•œ êµ¬ì¡°ë¥¼ ëˆë‹¤. ì´ˆê¸°ì— ì…ë ¥ì´ í•œë²ˆ ë“¤ì–´ê°€ê³  ì´í›„ ì…ë ¥ìœ¼ë¡œëŠ” 0ìœ¼ë¡œ ì±„ì›Œì§„ tensorë¥¼ ì…ë ¥ìœ¼ë¡œ ì£¼ê²Œëœë‹¤ many-to-one ìµœì¢…ê°’ì„ ë§ˆì§€ë§‰ì—ì„œì•¼ ë‚´ì£¼ëŠ” ex) I love movieì—ì„œ RNNì´ ì²˜ë¦¬í•œí›„ ë§ˆì§€ë§‰ì˜ htë¥¼ ë´„ìœ¼ë¡œì„œ ê¸ë¶€ì •ì„ ì˜ˆì¸¡í•˜ê²Œ ëœë‹¤. ê¸¸ì´ê°€ ë‹¬ë¼ì§„ë‹¤ë©´ RNN CELLì´ ê·¸ë§Œí¼ í™•ì¥ì´ëœë‹¤ many-to-many ex) machine translation Ex) POS, vidioì˜ frameì´ sequenceëŒ€ë¡œ ì£¼ì–´ì§ˆë•Œ Character-level Language Model Example of training sequence â€œhelloâ€ vocab = [h,e,l,o] ê°ê°ì˜ characterì€ one-hot-vectorë¡œ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤ Back propagation through time (BPTT)Whh,Why,Wxh ì™€ ê°™ì€ parameterë“¤ì„ í•™ìŠµí•œë‹¤ sequenceì „ì²´ë¥¼ í•œë²ˆì— í•™ìŠµí•˜ê¸°ì—ëŠ” physicalì ì¸ í•œê³„ê°€ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì— êµ°ë°êµ°ë° ì§¤ë¼ì„œ ì œí•œëœ ê¸¸ì´ì˜ sequenc ë§Œìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•œë‹¤ ë§¤ time stepë§ˆë‹¤ hidden state vectorê°€ ê±°ì˜ ëª¨ë“  ì •ë³´ë¥¼ ë‹´ê³  ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ë§Œì•½ì— hidden stateì˜ ì°¨ì›ì´ 3ì°¨ì›ì´ë¼ë©´, ìš°ë¦¬ê°€ ì›í•˜ëŠ” ì •ë³´ê°€ ê·¸ì¤‘ ì–´ëŠ nodeì— ë‹´ê²¨ì ¸ ìˆì„ê¹Œ? ì´ê±¸ ì—­ì¶”ì . ì²«ë²ˆì§¸ htì˜ nodeë¥¼ ê³ ì •í•´ ë†“ê³  ì´í›„ì˜ ë³€í™”ë“¤ì„ ë´„ ì •ì‘ ì§€ê¸ˆê¹Œì§€ ë°°ìš´ vanila RNNì€ ì˜ í™œìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. ì´ìœ ëŠ” ë§Œì•½ ê¸´ê±°ë¦¬ì— ìˆëŠ” ì •ë³´ê°€ ë§¤ìš° ì¤‘ìš”í•  ê²½ìš° back propagtionìœ¼ë¡œ êµ¬í•´ì§€ê¸° ë•Œë¬¸ì— gradient vanishingì´ë‚˜ gradient explodeê°€ ì¼ì–´ë‚˜ê²Œ ëœë‹¤. gradientê°’ì´ ì¦í­ë˜ê³ ìˆë‹¤ LSTM &amp; GRULong short-term Memoryë³´ë‹¤ íš¨ê³¼ì ìœ¼ë¡œ long term dependencyë¥¼ ì²˜ë¦¬í• ìˆ˜ ìˆê²Œë”í•˜ê¸° ìœ„í•´ htë¥¼ ë‹¨ê¸° ê¸°ì–µì†Œìë¡œ ìƒê°í•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŸ¬í•œ ë‹¨ê¸°ê¸°ì–µì„ ì–¼ë§ˆë‚˜ ê¸¸ê²Œ ëŒê³ ê°ˆ ê²ƒì´ì§€ë¥¼ íŒë³„í•´ì£¼ëŠ” ì—­í• ë“¤ì„ ê°€ì§„ gateë“¤ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤ ì „ time stepì—ì„œ ë„˜ì–´ì˜¤ëŠ” ì •ë³´ê°€ 2ê°€ì§€ì˜ ì„œë¡œë‹¤ë¥¸ vectorê°€ ë“¤ì–´ì˜¤ê²Œ ëœë‹¤. ìœ„ì— ë“¤ì–´ì˜¤ëŠ” vector : Ct ì•„ë˜ìª½ì— ë“¤ì–´ì˜¤ëŠ” vecor : ht Ã Ct-1 ì´ì „ cell stateì™€ ì´ì „ stateì˜ hidden stateë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ í˜„ì¬ì˜ csì™€ ssë¥¼ ë‚´ì¤€ë‹¤. Hidden state vectorì€ cell state vectorì¤‘ì— ë…¸ì¶œë˜ëŠ” ì •ë³´ë¥¼ ë‹´ì€, í•œë²ˆ í•„í„°ë§ ëœ vectorì´ë‹¤. ì—¬ê¸°ì„œ sigmoidì˜ ê²°ê³¼ì™€ ê³±í•´ì§€ë©´ ì–¼ë§ˆë§Œí¼ ì´ì „ì˜ ì›ë˜ê°’ì„ ë°˜ì˜í• ì§€ë¥¼ ê²°ì •í•˜ëŠ” ì—­í• ì„ í•œë‹¤. ë§ˆì§€ë§‰ tanhë¥¼ í†µí•´ ë‚˜ì˜¤ëŠ” ê°’ì€ í˜„ì¬ time stepì—ì„œ LSTMì—ì„œ ê³„ì‚°ë˜ëŠ” ìœ ì˜ë¯¸í•œ ì •ë³´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. Forget gate ìœ„ë¥¼ ë³´ë©´ ì´ì „ì˜ hidden stateì™€ í˜„ì¬ì˜ xtë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ sigmoid ì ìš©í›„ 3ì°¨ì›ì˜ vectorê°€ ë‚˜ì˜¤ê²Œ ë˜ì—ˆë‹¤. ì´ë ‡ê²Œ ë‚˜ì˜¨ vectorì™€ ì´ì „ì˜ cell stateì˜ element wise productë¥¼ í•´ì£¼ì–´ì„œ ì´ì „ì˜ cell stateë¥¼ ì–¼ë§ˆë§Œí¼ ë°˜ì˜í• ì§€ë¥¼ ê²Œì‚°í•´ ì£¼ì—ˆë‹¤. Gate gate Ctì— ë”í•´ì£¼ì–´ì•¼ í•˜ëŠ” ê°’ì„ ë°”ë¡œ ë”í•´ì£¼ì§€ ì•Šê³  itë¥¼ ê³±í•´ì„œ ë”í•´ì¤€ë‹¤ Output gate ì´ì œ cell state vector Ctë¡œ hidden state vector htë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤. ì•ì„œ sigmoidë¥¼ ì ìš©í•œ ê°’ë˜í•œ tanhë¥¼ ê±°ì¹œ Celll stateì— ê³±í•œê°’ì— ê³±í•´ì£¼ì–´ ì ì ˆí•œ ë¹„ìœ¨ë§Œí¼ ê°’ì„ ì‘ê²Œ ë§Œë“¤ì–´ì£¼ì–´ ìµœì¢…ì ì¸ htë¥¼ ë§Œë“¤ì–´ì£¼ê²Œ ëœë‹¤. htëŠ” ë‹¤ìŒ rnnì˜ hidden stateë¡œ ë“¤ì–´ê°€ëŠ” ë™ì‹œì— í˜„ì¬ time stepì—ì„œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í• ë•Œ ì´ê±¸ output layerì— ë„˜ê²¨ì£¼ì–´ ì˜ˆì¸¡ê°’ì„ ìƒì„±í•´ ë‚¸ë‹¤ GRULSTMì—ì„œ 2ê°€ì§€ ì¢…ë¥˜ì˜ vectorë¡œ ì¡´ì¬í•˜ë˜ cell stateì™€ hidden state vectorë¥¼ ì¼ì›í•˜ í•˜ì—¬ í•˜ë‚˜ì˜ vectorë§Œì´ ì¡´ì¬í•˜ê²Œ í•œë‹¤ëŠ”ê²Œ íŠ¹ì§•ì´ë‹¤. í•˜ì§€ë§Œ ì „ì²´ì ì¸ ë™ì‘ì›ë¦¬ëŠ” ê±°ì˜ ë¹„ìŠ· forget gateëŒ€ì‹  1-ztë¥¼ ì‚¬ìš©, itëŒ€ì‹  ztë¥¼ ì‚¬ìš© input gateê°€ ì»¤ì§ˆìˆ˜ë¡ forget gateì˜ ê°’ì´ ì ì°¨ ì‘ì•„ì§€ê²Œ ë˜ì–´ ê²°ê³¼ì ìœ¼ë¡œ ì´ì „ hidden state vectorë¥¼ ë” ì ê²Œ ë°˜ì˜í•˜ëŠ” ê²ƒì´ê³ , vice versa hidden stateë¥¼ ì¼ì›í™” í•˜ì˜€ë‹¤ 2ê°œì˜ ë…ë¦½ëœ gateë¥¼ í†µí•˜ì—¬ ë™ì‘ë˜ì—ˆë˜ modelì„ í•˜ë‚˜ì˜ gateë§Œìœ¼ë¡œ ì¤„ì—¬ ê³„ì‚°ëŸ‰ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì˜€ë‹¤. ì •ë³´ë¥¼ ì£¼ë¡œë‹´ëŠ” cell stateê°€ updateë˜ëŠ” ê³¼ì •ì´ í–‰ë ¬ì˜ ê³„ì†ì ì¸ ê³±ì˜ ì—°ì‚°ì´ ì•„ë‹ˆë¼ ê·¸ë•Œê·¸ë•Œ ì„œë¡œë‹¤ë¥¸ gateë¥¼ ê±°ì³ê°€ë©° updateë˜ê¸° ë•Œë¬¸ì— gradient vanishingì´ ì‚¬ë¼ì§„ë‹¤. ë§ì…ˆì—°ì‚°ì€ ì´ì „ì˜ stateë¥¼ ë³µì‚¬í•´ì£¼ì–´ gradientë¥¼ ìœ ì§€í•˜ëŠ” ì—­í• ì„ í•œë‹¤ê³  ë³¼ ìˆ˜ë„ ìˆë‹¤. RNNì€ ë‹¤ì–‘í•œ ê¸¸ì´ë¥¼ ê°€ì§ˆìˆ˜ ìˆëŠ” ìœ ì—°í•œ í˜•íƒœì˜ deep learningêµ¬ì¡°.","link":"/2021/02/16/2021-02-15-Boostcamp16.1/"},{"title":"Graph2","text":"ê²€ìƒ‰ì—”ì§„ì—ì„œì˜ ê·¸ë˜í”„ í˜ì´ì§€ë­í¬ì˜ ë°°ê²½1.1 ì›¹ê³¼ ê·¸ë˜í”„ì›¹(ë°©í–¥ì„±ì´ ìˆëŠ” ê·¸ë˜í”„) = ì›¹í˜ì´ì§€(node) + í•˜ì´í¼ë§í¬(edge) ì›¹í˜ì´ì§€ëŠ” ì¶”ê°€ì ìœ¼ë¡œ í‚¤ì›Œë“œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³ ìˆë‹¤. 2.2 êµ¬ê¸€ì´ì „ì˜ ê²€ìƒ‰ì—”ì§„ ì›¹ì„ ê±°ëŒ€í•œ ë””ë ‰í† ë¦¬ë¡œ ì •ë¦¬ ì›¹í˜ì´ì§€ì˜ ìˆ˜ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ì¹´í…Œê³ ë¦¬ ìˆ˜ë„ ë¬´í•œì • ì»¤ì§€ëŠ” ë¬¸ì œê°€ ìˆë‹¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ê°€ ëª¨í˜¸í• ìˆ˜ê°€ ìˆë‹¤. í‚¤ì›Œë“œì— ì˜ì¡´í•œ ê²€ìƒ‰ì—”ì§„ ì•…ì˜ì ì¸ ì›¹í”¼ì´ì§€ì— ì·¨ì•½í•˜ë‹¤ ë”°ë¼ì„œ page rankë¼ëŠ” í•˜ë‚˜ì˜ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬ê¸€ì´ ë§Œë“¤ì—ˆë‹¤ í˜ì´ì§€ë­í¬ì˜ í•µì‹¬ì€ íˆ¬í‘œì´ë‹¤. ì›¹í˜ì´ì§€ëŠ” í•˜ì´í¼ ë§í¬ë¥¼ í†µí•´ íˆ¬í‘œë¥¼í•˜ê²Œ ëœë‹¤. ì‚¬ìš©ìê°€ ì…ë ¥í•œ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ì›¹í˜ì´ì§€ì—ì„œ uê°€ vì— ì—°ê²°ë˜ì–´ìˆë‹¤ë©´ vëŠ” ì‹ ë¢°ê°€ëŠ¥ ë§ì´ ì¸ìš©ëœ ë…¼ë¬¸ì„ ì‹ ë¢°í•˜ëŠ”ê²ƒê³¼ ë¹„ìŠ·í•œ ì•Œê³ ë¦¬ì¦˜ ì›¹í˜ì´ì§€ë¥¼ ì—¬ëŸ¬ê°œ ë§Œë“¤ì–´ì„œ ê°„ì„œì˜ ìˆ˜ë¥¼ ë¶€í’€ë¦´ìˆ˜ ìˆë‹¤. ì´ëŸ°ì‹ì˜ ì•…ìš©ì€ ì˜¨ë¼ì¸ snsì—ì„œë„ í”íˆ ë°œê²¬ì´ ëœë‹¤. ì´ëŸ¬í•œ ì•…ìš©ì„ ë§‰ê¸°ìœ„í•´ ê°€ì¤‘íˆ¬í‘œë¥¼ ì‚¬ìš©í•œë‹¤. ì¸¡ì •í•˜ë ¤ëŠ” ì›¹í˜ì´ì§€ì˜ ê´€ë ¨ì„±ê³¼ ì‹ ë¢°ë„ ìì‹œì˜ ì ìˆ˜ / ë‚˜ê°€ëŠ” ì´ì›ƒì˜ ìˆ˜ ë˜í•œ í˜ì´ì§€ ë­í¬ëŠ” ì„ì˜ë³´í–‰ì˜ ê´€ì ì—ì„œë„ ì •ì˜ í•  ìˆ˜ìˆë‹¤. ì›¹ì„œí¼ëŠ” í˜„ì¬ í•˜ì´í¼ë§í¬ì¤‘ í•˜ë‚˜ë¥¼ ê· ì¼í•œ í™•ë¥ ë¡œ í´ë¦­í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì›¹ì„ ì„œí•‘í•œë‹¤. ì´ ê³¼ì •ì„ ë¬´í•œíˆ ìˆ˜í–‰í•˜ë©´ p(t) = p(t+1)ì´ëœë‹¤. ìˆ˜ë ´í•œ pëŠ” ì •ìƒë¶„í¬ë¼ê³  ë¶€ë¥¸ë‹¤. ê²°êµ­ ì´ë¥¼ ì •ë¦¬í•´ë³´ë©´ íˆ¬í‘œê´€ì ì—ì„œì˜ pagerankì •ì˜ ìˆ˜ì‹ê³¼ ë¹„ìŠ·í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë”. í˜ì´ì§€ë­í¬ì˜ ê³„ì‚° ë°˜ë³µê³± Power iterationì€ ê°ì›¹í˜ì´ì§€ì— í˜ì´ì§€ ë­í¬ë¥¼ êµ¬í• ë•Œ ì‚¬ìš©ëœë‹¤ ê°ì›¹í˜ì´ì§€ iì˜ í˜ì´ì§€ ë í¬ ì ìˆ˜ë¥¼ ë™ì¼í•˜ê²Œ (1/ì›¹í˜ì´ì§€ìˆ˜) ë¡œ ì´ˆê¸°í™” í•œë‹¤ ì•„ë˜ì˜ ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì›¹í˜ì´ì§€ì˜ ì ìˆ˜ë¥¼ ê°±ì‹ í•œë‹¤ í˜ì´ì§€ ë­í¬ ì ìˆ˜ê°€ ìˆ˜ë ´í• ë•Œê¹Œì§€ ê³„ì‚°ì„ ë°˜ë³µí•œë‹¤. ê³¼ì—° ë°˜ë³µê³±ì´ í• ìƒ ìˆ˜ë ´ì„ í• ê¹Œìš”? ìˆ˜ë ´í•˜ì§€ ì•Šì„ìˆ˜ê°€ ìˆë‹¤. -&gt; ë“¤ì–´ì˜¤ëŠ” ì •ì ì€ ìˆì§€ë§Œ ë‚˜ê°€ëŠ” ê°„ì„ ì´ ì—†ëŠ” spider trapì— ì˜í•´ ì¼ì–´ë‚¨ ê³¼ì—° ìˆ˜ë ´ì„ í•œë‹¤ê³  í•´ë„ ì´ ê²°ê³¼ê°€ í•©ë¦¬ì ì¸ ê°’ì¼ê¹Œìš”? ì´ ë¬¸ì œì ë“¤ì— ëŒ€í•œ í•´ê²°ì±… : ìˆœê°„ì´ë™ ì„ì˜ ë³´í–‰ê´€ì ì—ì„œ (1) í˜„ì¬ ì›¹í˜ì´ì§€ì— í•˜ì´í¼ë§í¬ê°€ ì—†ë‹¤ë©´ ì„ì˜ì˜ ì›¹í˜ì´ì§€ë¡œ ìˆœê°„ì´ë™ì„ í•œë‹¤ (2) í˜„ì¬ ì›¹í˜ì´ì§€ì— í•˜ì´í¼ë§í¬ê°€ ìˆë‹¤ë©´ $\\alpha$ì˜ í™•ë¥ ë¡œ íŒŒì´í¼ ë§í¬ì¤‘ í•˜ë‚˜ë¥¼ ê· ì¼í•œ í™•ë¥ ë¡œ ì„ íƒí•˜ê³  í´ë¦­í•œë‹¤ (1-$\\alpha$)ì˜ í™•ë¥ ë¡œ ì„ì˜ì˜ ì›¹í˜ì´ì§€ë¡œ ìˆœê°„ì´ë™ í•œë‹¤ ì´ë¡œ ì¸í•´ spider trapì´ë‚˜ dead endì— ê°‡íˆëŠ” ì¼ì´ ì‚¬ë¼ì¡Œë‹¤. ìˆœê°„ì´ë™ì˜ ë„ì…ì€ ìˆ˜ì‹ì´ ë°”ë€ë‹¤ ê° ë§‰ë‹¤ë¥¸ ì •ì ì—ì„œ ë‹¤ë¥¸ëª¨ë“  ì •ì ìœ¼ë¡œ ê°€ëŠ” ê°„ì„ ì„ ì¶”ê°€í•œ í›„ ì´ì œëŠ” ì´ ìˆ˜ì‹ì„ ì‚¬ìš©í•œë‹¤. ê·¸ë˜í”„ë¥¼ ì´ìš©í•œ ë°”ì´ëŸ´ ë§ˆì¼€íŒ…ì˜ì‚¬ê²°ì • ê¸°ë°˜ì˜ ì „íŒŒì£¼ë³€ì˜ ì˜ì‚¬ê²°ì •ì„ ê³ ë ¤í•˜ì—¬ ì˜ì‚¬ê²°ì •ì„ í• ë•Œ ì˜ì‚¬ê²°ì • ê¸°ë°˜ì˜ ì „íŒŒëª¨í˜•ì„ ì‚¬ìš©í•œë‹¤ â€”&gt; ì„ í˜•ì„ê³„ì¹˜ëª¨í˜• (linear threshold model) í™•ë¥ ì  ì „íŒŒì½”ë¡œë‚˜ì˜ ì „íŒŒë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ë•ŒëŠ” í™•ë¥ ì  ëª¨í˜•ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤. Independent Cascade model ë°”ë¦¬ëŸ´ ë§ˆì¼€íŒ…ì´ë€? ì†Œë¹„ìë¡œ í•˜ì—¬ê¸ˆ ìƒí’ˆì—ëŒ€í•œ ê¸ì •ì ì¸ ì…ì†Œë¬¸ì„ ë‚´ê²Œ í•˜ëŠ” ê¸°ë²• ë°”ì´ëŸ´ ë§ˆì¼€íŒ…ì„ ìœ„í•´ì„œëŠ” ì†Œë¬¸ì˜ ì‹œì‘ì ì´ ì¤‘ìš”í•˜ë‹¤. ì‹œë“œ ì§‘í•©ì´ ì „íŒŒì— ë§ì€ ì˜í–¥ì„ ë¯¸ì¹œë‹¤ ê·¸ë˜í”„. ì „íŒŒëª¨í˜•, ì‹œë“œì§‘í•©ì˜ í¬ê¸°ê°€ ì£¼ì–´ì¡‹ì„ë•Œ ì „íŒŒì˜ ìµœëŒ€í™”ë¥¼ ìœ„í•œ ì‹œë“œì§‘í•©ì€ ì „íŒŒìµœëŒ€í™” ë¬¸ì œì´ë‹¤. ì–´ë ¤ìš´ ë¬¸ì œì´ë‹¤. ê·¸ë˜í”„ì— Vê°œì˜ ì •ì ì´ ìˆëŠ”ê²½ìš° ì‹œë“œì§‘í•©ì´ kê°œì¼ë•Œ ê²½ìš°ì˜ ìˆ˜ëŠ” vCk ì´ë‹¤ ì´ë¡ ì ìœ¼ë¡œ ì „íŒŒìµœëŒ€í™” ë¬¸ì œëŠ” í’€ê¸°ê°€ í˜ë“  ë¬¸ì œì„ì´ ì¦ëª…ì´ë˜ì–´ìˆë‹¤. ëŒ€í‘œì  íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì •ì ì˜ ì¤‘ì‹¬ì„± ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì¦‰ì‹œë“œ í¬ê¸°ê°€ kê°œë¡œ ê³ ì •ì´ë˜ì–´ìˆì„ë•Œ ì •ì ì˜ ì¤‘ì‹¬ì„±ì´ ë†’ì€ìˆ˜ìœ¼ë¡œ kê°œ ì •ì ì„ ì„ íƒí•˜ëŠ” ë°©ë²•ì´ë‹¤. ì •ì ì˜ ì¤‘ì‹¬ì„±ìœ¼ë¡œëŠ” í˜ì´ì§€ ë­í¬ ì ìˆ˜, ì—°ê²° ì¤‘ì‹¬ì„±, ê·¼ì ‘ ì¤‘ì‹¬ì„±, ë§¤ê°œ ì¤‘ì‹¬ì„±ë“±ì´ìˆë‹¤. íƒìš• ì•Œê³ ë¦¬ì¦˜ ì‹œë“œ ì§‘í•©ì˜ ì›ì†Œë¥¼ í•œë²ˆì— í•œëª…ì”© ì„ íƒì„ í•œë‹¤. ì •ì ì˜ ì§‘í•© : {1,2,â€¦.,V} ê° ì§‘í•©ì— ëŒ€í•´ ì‹œë®¬ë ˆì´ì…˜ì„ ë°˜ë³µí•˜ì—¬ í‰ê· ê°’ì„ ì‚¬ìš©í•œë‹¤. xë¼ëŠ”ì •ì ì´ ìµœì´ˆì˜ì „íŒŒìë¡œ ì„ ì •ì´ ë˜ì–´ìˆë‹¤. ì´ëŸ° ë¹„êµë¥¼ í†µí•´ ë½‘íŒ ì§‘í•©ì€ xë¼ê³  í•˜ì. ì´ì œ xë¥¼ í¬í•¨í•œ í¬ê¸°ê°€ 2ì´ ì‹œë“œ ì§‘í•©ì„ ì°¾ëŠ”ë‹¤.ì´ë¥¼ ëª©í‘œì˜ í¬ê¸°ê¹Œì§€ ë°˜ë³µí•œë‹¤. ìµœì´ˆì „íŒŒìê°„ì˜ ì¡°í•©ì„ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤. íƒìš• ì•Œê³ ë¦¬ì¦˜ì€ í•­ìƒ ìµœê³ ì˜ ì‹œë“œ ì§‘í•©ì„ ì°¾ëŠ”ë‹¤ëŠ” ë³´ì¥ì´ ì—†ëŠ” ê·¼ì‚¬ì˜ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤ í•­ìƒ ìµœì ì˜ ê°’ì´ ì•„ë‹ˆë¼ëŠ” ë§ì´ë‹¤. í•˜ì§€ë§Œ ì ì–´ë„ ì–´ëŠì •ë„ì˜ ì‹œë“œì§‘í•©ì€ ì°¾ì„ ìˆ˜ ìˆë‹¤.","link":"/2021/02/23/2021-02-23-Boostcamp22/"},{"title":"Graph","text":"ê·¸ë˜í”„ë€ ì •ì ê³¼ ê°„ì„ ìœ¼ë¡œ ì´ë£¨ì–´ì§„ êµ¬ì¡° í•˜ë‚˜ì˜ ê°„ì„ ì€ ë°˜ë“œì‹œ ë‘ê°œì˜ ì •ì ì„ ì—°ê²°í•œë‹¤ ì •ì  : vertex,node ê°„ì„  : Edge,link ìš°ë¦¬ì˜ ì‚¬íšŒë° ëª¨ë“  ë‹¤ì–‘í•œ ê²ƒë“¤ì€ êµ¬ì„±ìš”ì†Œê°„ì˜ ë³µì¡í•œ ì‚´í˜¸ì‘ìš©ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë³µì¡ê³„ì´ë‹¤ ì´ê²ƒì„ í‘œí˜„í•˜ëŠ” ë°©ì‹ì´ ë°”ë¡œ ê·¸ë˜í”„ì´ë‹¤ ê·¸ë˜í”„ë€ ë³µì¡ê³„ë¥¼ ê°„ë‹¨í•˜ê²Œ í‘œí˜„í•˜ëŠ” ë°©ì‹ì´ë‹¤ ì •ì  ë¶„ë¥˜ë¬¸ì œ (node classification) ex) ì–´ë– í•œ ê³„ì •ì´ ì–´ë– ê±¸ ë¦¬íŠ¸ìœ—í–ˆëŠ”ì§€ë¥¼ ê°„ì„ ìœ¼ë¡œ í‘œí˜„. ì‚¬ëŒ(node)ì˜ ë³´ìˆ˜ì„±, ì§„ë³´ì„±ì„ íŒë³„í•˜ëŠ” ë­í‚¹ ë° ì •ë³´ê²€ìƒ‰ë¬¸ì œ : ì›¹ì´ë¼ëŠ” ê±°ëŒ€í•œ ê·¸ë˜í”„ë¡œë¶€í„° ì–´ë–»ê²Œ ì¤‘ìš”í•œ ì›¹í˜ì´ì§€ë¥¼ ì°¾ì•„ë‚¼ê¹Œ? êµ°ì§‘ë¶„ì„ë¬¸ì œ : ì—°ê²°ê´€ê³„ë¡œ ë¶€í„° ì‚¬íšŒì  ë¬´ë¦¬(êµ°ì§‘)ì„ ì°¾ì•„ë‚¼ ìˆ˜ ìˆì„ê¹Œ? ì •ë³´ì „íŒŒ &amp; ë°”ì´ëŸ´ ë§ˆì¼€íŒ… ë¬¸ì œ : ì •ë³´ë¼ëŠ” ê²ƒì´ ì–´ë–»ê²Œ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ ì „íŒŒê°€ ë ê¹Œ? ë³¸ê°•ì˜ì—ì„œëŠ” ìœ„ì˜ ë¬¸ì œë“¤ì„ í•´ê²°í•˜ëŠ” ê¸°ìˆ ë“¤ì„ ë°°ìš°ê²Œë  ì˜ˆì • 1ì£¼ì¼ì´ë¼ëŠ” ì§§ì€ ì‹œê°„ì´ë¼ ê¸°ì´ˆë¥¼ ë°°ìš°ê³  ì§ê´€ì ì¸ ë°©ë²•ë¡ ì ì¸ ì„¤ëª… ê°„ì„ ì— ë°©í–¥ì´ ìˆëŠ” directed graph vs undirected graph í˜‘ì—…ê´€ê³„ê·¸ë˜í”„, í˜ì´ìŠ¤ë¶ ì¹œêµ¬ê·¸ë˜í”„ : undirectied ì¸ìš©ê·¸ë˜í”„, íŠ¸ìœˆí„° íŒ”ë¡œìš° ê·¸ë˜í”„ : directed ê°„ì„ ì— ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ê·¸ë˜í”„ : ì „í™”ê·¸ë˜í”„, ìœ ì‚¬ë„ ê·¸ë˜í”„ ê°„ì„ ì— ê°€ì¤‘ì¹˜ê°€ ì—†ëŠ” ê·¸ë˜í”„ : í˜ì´ìŠ¤ë¶ ì¹œêµ¬ ê·¸ë˜í”„, ì›¹ê·¸ë˜í”„ ë™ì¢… ê·¸ë˜í”„ vs ì´ì¢…ê·¸ë˜í”„ ì´ì¢…ê·¸ë˜í”„ëŠ” ë‘ì¢…ë¥˜ì˜ nodeë¥¼ ê°€ì§„ë‹¤ . ì„œë¡œë‹¤ë¥¸ ì •ì  ì‚¬ì´ì—ë§Œ ê°„ì„ ì´ ì—°ê²°ëœë‹¤. Ex) ì‚¬ìš©ì,ìƒí’ˆì‚¬ì´ì˜ ì „ììƒê±°ë˜ ë‚´ì—­ ë™ì¢…ê·¸ë˜í”„ëŠ” ë‹¨ì¼ì¢…ë¥˜ì˜ ì •ì ì„ ê°€ì§„ë‹¤ nodeì˜ ì§‘í•© V, edgeì˜ ì§‘í•© : E, G = (V,E) Nout(1), Nin(1)ì´ëŸ°ê±° ê·¸ë˜í”„ì˜ í‘œí˜„ ë° ì €ì¥ Networkxë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜í”„ë¥¼ í‘œí˜„ snap.pyë¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë„ ë§ì´ ì‚¬ìš©í•œë‹¤. 1 ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ 1 : [2.5] 2 : [1,3,5] 3: [2] 5: [1,2] ì¸ì ‘í–‰ë ¬ ê°„ì„ ì´ ìˆìœ¼ë©´ 1, ì—†ìœ¼ë©´ 0 ë°©í–¥ì„±ì´ ì—†ìœ¼ë©´ ëŒ€ê°ìœ¼ë¡œ ëŒ€ì¹­ ìˆìœ¼ë©´ ë‹¤ë¦„ í¬ì†Œí–‰ë ¬ì„ ì‚¬ìš©í•˜ë©´ ì €ì¥ê³µê°„ì„ ì ˆì•½í•  ìˆ˜ ìˆìŒ (ëŒ€ë¶€ë¶„ì˜ ì›ì†Œê°€ 0ì¼ë•Œ) 1. ì‹¤ì œê·¸ë˜í”„ vs ëœë¤ê·¸ë˜í”„ì‹¤ì œ ê·¸ë˜í”„ë€ ë‹¤ì–‘í•œ ë³µì¡ê³„ë¡œë¶€í„° ì–»ì–´ì§„ ê·¸ë˜í”„ë¥¼ ì˜ë¯¸í•œë‹¤ ë³¸ìˆ˜ì—…ì—ì„œëŠ” MSN ë©”ì‹ ì € ê·¸ë˜í”„ë¥¼ ì‹¤ì œ ê·¸ë˜í”„ì˜ ì˜ˆì‹œë¡œ ì‚¬ìš©í•˜ê² ë‹¤. ëœë¤ê·¸ë˜í”„ë€ í™•ë¥ ì  ê³¼ì •ì„ í†µí•´ ìƒì„±ëœ ê·¸ë˜í”„ë¥¼ ì˜ë¯¸í•œë‹¤ ì—ë¥´ë˜ìŠ¤-ë ˆë‹ˆ ëœë¤ê·¸ë˜í”„ì„ì˜ì˜ ë‘ nodeì‚¬ì´ì˜ ê°„ì„  ì¡´ì¬ì—¬ë¶€ê°€ ë™ì¼í•œ í™•ë¥ ë¶„í¬ë¡œ ë‚˜íƒ€ë‚´ì–´ì§ G(n,p)ëŠ” nê°œì˜ ì •ì , ë‘ê°œì˜ ì •ì  ì‚¬ì´ì— ê°„ì„ ì´ ì¡´ì¬í•  í™•ë¥  = p 2. ì‘ì€ ì„¸ìƒ íš¨ê³¼ì •ì  uì™€ vì‚¬ì´ì˜ ê²½ë¡œë€ uì—ì„œ ì‹œì‘í•´ì„œ vì—ì„œ ëë‚˜ì•¼ í•œë‹¤ ìˆœì—´ì—ì„œ ì—°ì†ëœ ì •ì ì€ ë°˜ë“œì‹œ ê°„ì„ ìœ¼ë¡œ ì—°ê²°ë˜ì–´ìˆì–´ì•¼ í•œë‹¤. ê²½ë¡œ, ê±°ë¦¬, ë° ì§€ë¦„ ê²½ë¡œëŠ” ì—¬ëŸ¬ê°€ì§€ì§€ë§Œ ì´ì¤‘ ê°€ì¥ ì§§ì€ ê²½ë¡œì˜ ê¸¸ì´ê°€ ê±°ë¦¬ì´ë‹¤ ê·¸ë˜í”„ì—ì„œ ì§€ë¦„ì€ ì •ì ê°„ ê±°ë¦¬ì˜ ìµœëŒ“ê°’ì´ë‹¤. ì‘ì€ ì„¸ìƒ íš¨ê³¼ ì„ì˜ì˜ ë‘ì‚¬ëŒì„ ê³¨ëì„ ë•Œ ì´ë“¤ì€ ëª‡ë‹¨ê³„ì˜ ì§€ì¸ì„ ê±°ì³ì•¼ ì—°ê²°ë˜ëŠ”ê°€? ìœ„ì¹˜íƒ€ì—ì„œ ë³´ìŠ¤í„´ê¹Œì§€ ì§€ì¸ì„ 6ë‹¨ê²Œê±°ì¹˜ë©´ ê°€ëŠ¥ MSNì—ì„œë„ ì •ì ê°„ì˜ í‰ê· ê±°ë¦¬ëŠ”7ì •ë„ë°–ì— ë˜ì§€ ì•ŠëŠ”ë‹¤ ë‹¨ ê±°ëŒ€ì—°ê²°êµ¬ì¡°ë§Œì„ ê³ ë ¤í•˜ì˜€ë‹¤. ì´ëŸ¬í•œ í˜„ìƒì„ ì‘ì€ì„¸ìƒíš¨ê³¼ë¼ê³  í•œë‹¤. ì‘ì€ ì„¸ìƒíš¨ê³¼ëŠ” ë†’ì€ í™•ë¥ ë¡œ ëœë¤ê·¸ë˜í”„ì—ë„ ì¡´ì¬í•œë‹¤. ì²´ì¸ ì‚¬ì´í´ ê²©ìê·¸ë˜í”„ì—ëŠ” ì´ ì‘ì€ì„¸ìƒê·¸ë˜í”„íš¨ê³¼ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤. ì—°ê²°ì„±ì— ë‘í„°ìš´ ê¼¬ë¦¬ë¶„í¬ì—°ê²°ì„±? ì •ì ì˜ Degreeë€ ê·¸ì •ì ê³¼ ì—°ê²°ëœ ê°„ì„ ì˜ ìˆ˜ : |N(v)| ë¼ê³  í‘œí˜„í•˜ê¸°ë„ í•¨ ëœë¤ê·¸ë˜í”„ì˜ ì—°ê²°ì„± ë¶„í¬ëŠ” ë†’ì€ í™•ë¥ ë¡œ ì •ê·œë¶„í¬ì™€ ìœ ì‚¬í•˜ë‹¤ ì‹¤ì œ ê·¸ë˜í”„ëŠ” ì—°ê²°ì„±ì´ ë‘í„°ì›Œì„œ hub ì •ì ì´ ì¡´ì¬í•  ìˆ˜ìˆëŠ”ë° ëœë¤ê·¸ë˜í”„ì—ì„œëŠ” ì •ê·œë¶„í¬ë¥¼ ëŒ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ ì—°ê²°ìš”ì†Œ ì—°ê²°ìš”ì†Œì— ì†í•˜ëŠ” ì •ì ë“¤ì€ ê²½ë¡œë¡œ ì—°ê²°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 1ì˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ë©´ì„œ ì •ì ì„ ì¶”ê°€í•  ìˆ˜ ì—†ë‹¤. ì‹¤ì œê·¸ë˜í”„ì—ëŠ” ëŒ€ë‹¤ìˆ˜ì˜ ì •ì ì„ í¬í•¨í•˜ëŠ” ê±°ëŒ€ì—°ê²°ìš”ì†Œê°€ ì¡´ì¬í•œë‹¤ MSNë©”ì‹ ì € ê·¸ë˜í”„ì—ëŠ” 99,9%ì˜ ì •ì ì´ í•˜ë‚˜ì˜ ê±°ëŒ€ì—°ê²°ìš”ì†Œì— í¬í•¨ëœë‹¤ ì •ì ë“¤ì˜ í‰ê·  ì—°ê²°ì„±ì´ 1ë³´ë‹¤ ì¶©ë¶„íˆ í°ê²½ìš°, ëœë¤ê·¸ë˜í”„ì—ë„ ë†’ì€ í™•ë¥ ë¡œ ê±°ëŒ€ì—°ê²° ìš”ì†Œê°€ ì¡´ì¬í•œë‹¤. êµ°ì§‘ì´ë€ ì •ì ë“¤ì˜ ì§‘í•© ê°™ì€ êµ°ì§‘ì•ˆì—ì„œì˜ ì •ì  ì‚¬ì´ì—ëŠ” ë§ì€ edgeê°€ ì¡´ì¬ ì§€ì—­ì  êµ°ì§‘ ê³„ìˆ˜ : ê·¸ ì •ì ì´ êµ°ì§‘ì„ í˜•ì„±í•˜ë ¤ëŠ” ì •ë„ Ci = ì •ì  iì˜ ì´ì›ƒìŒì¤‘ ê°„ì„ ìœ¼ë¡œ ì§ì ‘ ì—°ê²°ëœ ê²ƒì˜ ë¹„ìœ¨ ì •ì iì˜ ì§€ì—­ì  êµ°ì§‘ê³„ìˆ˜ê°€ ë†’ìœ¼ë©´ ì´ì›ƒë“¤ì´ ì—°ê²°ë˜ì–´ìˆë‹¤.-&gt; ì •ì  iì™€ ì´ì›ƒë“¤ì´ êµ°ì§‘ì„ í˜•ì„±í•œë‹¤ ì „ì—­ êµ°ì§‘ ê³„ìˆ˜ ì „ì²´ ê·¸ë˜í”„ì—ì„œ êµ°ì§‘ì˜ í˜•ì„±ì •ë„ë¥¼ ì¸¡ì • ê° ì •ì ì—ì„œ ì§€ì—­ì  êµ°ì§‘ê³„ìˆ˜ì˜ í‰ê· ì´ë‹¤. ë‹¨ ì§€ì—­ì  êµ°ì§‘ê³„ìˆ˜ê°€ ì •ì˜ê°€ ì•ˆë˜ë©´ ì§¤ ì„¸ìƒì—ëŠ” ë§ì€ êµ°ì§‘ì´ ì¡´ì¬í•œë‹¤ homophily : ìœ ì‚¬í•œ ì •ì ë¼ë¦¬ëŠ” ê³µí†µì´ì›ƒì´ ìˆëŠ”ê²½ìš° ê³µí†µì´ì›ƒì´ ë‘ì •ì ì„ ë§¤ê°œí•˜ëŠ” ì—­í• ","link":"/2021/02/22/2021-02-22-Boostcamp21.1/"},{"title":"Transformerì‹¬í™”","text":"Transformer Self-Attentionex) I go home Iì— ëŒ€í•œ input vectorê°€ hidden stateì²˜ëŸ¼ ì—­í• ì„ í•˜ì—¬ì„œ Iì™€ ê°ê°ì˜ ë‹¨ì–´ì— ëŒ€í•œ ë‚´ì ì„ í•œí›„ ì´ì—ëŒ€í•œ softmaxë¥¼ êµ¬í•˜ì—¬ ê°€ì¤‘í‰ê· ì„ êµ¬í•œë‹¤. ì´ë ‡ê²Œ encoding vectorê°’ì„ êµ¬í•˜ê²Œ ë˜ë©´ ê²°êµ­ ìê¸°ìì‹ ê³¼ ë‚´ì í•œ ê°’ì´ í°ê°’ì„ ê°€ì ¸, ìê¸° ìì‹ ì— ëŒ€í•œ íŠ¹ì„±ë§Œì´ dominantí•˜ê²Œ ë‹´ê¸¸ê²ƒì´ë¯€ë¡œ, ì´ë¥¼ í•´ê²°í•´ì£¼ê¸° ìœ„í•´ ë‹¤ë¥¸ architectureë¥¼ ì“´ë‹¤ ê° vectorë“¤ì´ 3ê°€ì§€ì˜ ì—­í• ì„ í•˜ê³ ìˆëŠ” ê²ƒì´ë‹¤. ë™ì¼í•œ setì˜ vectorì—ì„œ ì¶œë°œí–ˆë”ë¼ë„ ê°í˜í• ì— ë”°ë¼ vectorê°€ ì„œë¡œë‹¤ë¥¸í˜•íƒœë¡œ ë³€í™˜í• ìˆ˜ìˆê²Œí•´ì£¼ëŠ” linear transformation matrixê°€ ìˆë‹¤. í•œë§ˆë””ë¡œ ê°ê°ì˜ inputì´ ì„œë¡œë‹¤ë¥¸ matrixì— ì ìš©ì´ë˜ì–´ ê°ê°ì´ key, quary,valueê°€ ëœë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. I ë¼ëŠ” wordê°€ ì„œë¡œë‹¤ë¥¸ matrixì— ë”°ë¼ quart, key, valueê°’ì´ ë§Œë“¤ì–´ì§€ê³  ì¿¼ë¦¬ëŠ” 1ê°œì´ê³  ì´ ì¿¼ë¦¬ ë²¡í„°ì™€ ê°ê°ì˜ key vectorì™€ì˜ ë‚´ì ê°’ì„ êµ¬í•˜ê³  ê²°ê³¼ë¥¼ softmaxì— í†µê³¼ì‹œì¼œ ê°€ì¤‘ì¹˜ë¥¼ êµ¬í•œí›„ , ì´ê°’ê³¼ value vectorë¥¼ ê°ê° ê³±í•´ì£¼ì–´ ì´ë“¤ì˜ ê°€ì¤‘í‰ê· ìœ¼ë¡œ ìµœì¢…ì ì¸ vectorë¥¼ êµ¬í•˜ë‹¤. ê²°êµ­ ì´ vectorê°€ featureë“¤ì´ ë‹´ê¸´ encoding vectorì´ë‹¤. ì´ëŸ¬í•˜ê²Œ í–‰ë ¬ì—°ì‚°ìœ¼ë¡œ ìœ„ì˜ ê³¼ì •ì„ í•œë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤. Multi-head Attention ì¿¼ë¦¬,key,valueë¥¼ ë§Œë“¤ë•Œ ì—¬ëŸ¬ setì˜ matrixë¥¼ ì ìš©í•˜ì—¬ ì—¬ëŸ¬ attentionì„ ìˆ˜í–‰í•œë‹¤. ì´ëŸ¬í•œ ì„œë¡œë‹¤ë¥¸ ì„ í˜•ë³€í™˜ matrixë¥¼ headë¼ê³  ë¶€ë¥¸ë‹¤. ë™ì¼í•œ sequenceì—ì„œ íŠ¹ì •í•œ quaryì— ëŒ€í•´ì„œ ì—¬ëŸ¬ì¸¡ë©´ìœ¼ë¡œ ì •ë³´ë¥¼ ë½‘ì•„ì•¼í•˜ëŠ” ê²½ìš°ê°€ ìˆë‹¤. ì´í›„ í•˜ë‚˜ì˜ ì„ í˜•ë³€í™˜ layerë¥¼ ì¶”ê°€í•˜ì—¬ ìš°ë¦¬ê°€ ì›í•˜ëŠ” shapeì˜ outputì„ ì–»ì–´ë‚¸ë‹¤. ì™œ ì´ëŸ¬í•œ shapeìœ¼ë¡œ ë³€í™˜í•´ì•¼í• ê¹Œ? for residual connection Residual connectionì„ ì‚¬ìš©í–ˆë‹¤. ì´ëŠ” CVì—ì„œ ë„ë¦¬ì“°ì´ë˜ Resnetì—ì„œ ì‚¬ìš©í•œ residueê°œë…ì„ í™œìš©í•˜ì—¬, attention ê²°ê³¼ì˜ encoded vectorì™€ ì›ë˜ ì…ë ¥ vectorë¥¼ ë”í•œë‹¤. ì´ëŸ¬í•œ ê³¼ì •ì„ í†µí•´ gradient vanishingê³¼ í•™ìŠµì˜ ì†ë„ë¥¼ í•´ê²°í•˜ì˜€ë‹¤. Layer Normalization ì£¼ì–´ì§„ sampleì— ëŒ€í•´ì„œ ê·¸ê°’ë“¤ì˜ í‰ê· ì„ 0 ë¶„ì‚°ì„ 1ë¡œ ë§Œë“¤ì–´ì¤€í›„ ìš°ë¦¬ê°€ ì›í•˜ëŠ” í‰ê· ê³¼ ë¶„ì‚°ì„ ë§Œë“¤ì–´ì£¼ëŠ” ì„ í˜•ë³€í™˜ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ìˆë‹¤. í‘œì¤€í™”ëœ í‰ê· ê³¼ ë¶„ì‚°ìœ¼ë¡œ ë§Œë“¤ì–´ì¤Œ. ì´í›„ affine transformation (ex : y = 2x+3)ì„ ìˆ˜í–‰í•  ê²½ìš° í‰ê· ì€ 3 ë¶„ì‚°ì€ 4ê°€ ëœë‹¤. ì—¬ê¸°ì„œì˜ 2ì™€ 3ì€ NNì´ optimizeí•´ì•¼í•˜ëŠ” paramterê°€ ëœë‹¤. ìœ„ì˜ transformerì—ë„ ì´ëŸ°ì‹ìœ¼ë¡œ ì ìš©ì´ëœë‹¤ affine transformationì€ ì´ì œ parameterì´ë¼ í•™ìŠµí•˜ëŠ”? ì™œì„±ëŠ¥ì´ ì˜¬ë¼ê°ˆê¹Œ? Transformerì—ì„œì˜ self attentionì€ ìˆœì„œì˜ ì •ë³´ë¥¼ ë‹´ê³ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì— ì¶”ê°€ì ì¸ ì‘ì—…ì´ í•„ìš”í•˜ë‹¤. ì´ ì‘ì—…ì„ transformerì€ postition encodingì— sinusodial functionì„ ì ìš©í•˜ì˜€ë‹¤. Optimizerì€ graident descentê°€ ì•„ë‹Œ Adamì„ ì‚¬ìš©í•˜ì˜€ë‹¤. Learning rateì„ ê³ ì •í•œ ê°’ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  í•™ìŠµì¤‘ì— lrì„ ë³€ê²½ì‹œì¼œì£¼ì—ˆë‹¤. Decoder, ë‚˜ëŠ”, ì§‘ì—, ê°„ë‹¤ Masked Multi-Head Attentionì˜ ê²°ê³¼ê°€ê°€ ì–»ì–´ì¡Œë‹¤ë©´ ì´ë¥¼ ë‹¤ì‹œ multi-head attentionì— ë„£ì–´ì¤€ë‹¤. ê·¸ë° ì´ì œ quaryì—ë§Œ ì´ê²Œ ì‚¬ìš©ë˜ê³  encodingë‹¨ì˜ encoded vectorê°€ ì´ì œ keyì™€ valueê°’ì— ë“¤ì–´ê°€ê²Œ ëœë‹¤. ì´ì œ target languageì˜ vocab sizeì— ë§ëŠ” vectorë¥¼ ìƒì„±í•˜ëŠ” linear transformationì„ ê±¸ì–´ì¤€ë‹¤. ê·¸ê³³ì— soft maxë¥¼ ì·¨í•´ì„œ ë‹¤ìŒ wordë¥¼ ì°¾ì•„ë‚¸ë‹¤. ì´ì œ ground truthì™€ì˜ softmax lossë¥¼ êµ¬í•´ì„œ backpropagationìœ¼ë¡œ í•™ìŠµí•´ ë‚˜ê°„ë‹¤. Masked Self Attention ì „ì²´ sequenceì— ëŒ€í•œ ì •ë³´ë¥¼ í—ˆìš©í•˜ê²Œ ë˜ë©´ ì²«ë²ˆì§¸ time stepì—ì„œ SOSë§Œì´ ì£¼ì–´ì¡ŒëŠ”ë° ë‚˜ëŠ”ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•˜ëŠ”ë° ë‚˜ëŠ”ì´ë¼ëŠ” ê°’ê³¼ sosì‚¬ì´ì˜ í–‰ë ¬ê³±ê°’ì´ ìˆê¸° ë•Œë¬¸ì— ì´ìƒí™©ì—ëŠ” ì´ë¥¼ maskingí•´ì£¼ì–´ì•¼ í•œë‹¤. ì´ë ‡ê²Œ maskingí•´ì¤€í›„ normalizeë¥¼ í•´ì£¼ê²Œ ëœë‹¤. ê°€ì¤‘í‰ê· ì˜ í•©ì´ 1ì´ ë˜ë„ë¡","link":"/2021/02/18/2021-02-18-Boostcamp19.1/"},{"title":"Week-1","text":"BOOST campì˜ ì²«ì£¼ê°€ ëì´ ë‚¬ë‹¤ì •ì‹ ì—†ì´ ì§€ë‚´ë‹¤ ì•„ì¹¨ì— ì¼ì–´ë‚˜ì„œ ì˜¤í›„ê¹Œì§€ ì»´í“¨í„°ë§Œ ë³´ë‹¤ë³´ë‹ˆ 5ì¼ì´ ì§€ë‚˜ê°”ë‹¤.ì•ìœ¼ë¡œì˜ ì£¼ë§ê³„íšì€ ë…¼ë¬¸ 1í¸ê³¼ 1ì£¼ì¼ì¹˜ ë‚´ìš© ë³µìŠµì´ë‹¤ë‹¤ìŒì£¼ì— ë°œí‘œì¸ í”„ë¡œì íŠ¸ëŠ” ì–´ì°Œë ì§€ ëª¨ë¥´ê² ë‹¤. ì—´ì‹¬íˆ í•´ë³´ë ¤ í–ˆëŠ”ë° ìƒê°ë³´ë‹¤ ì‰½ì§€ê°€ ì•Šë„¤â€¦.ë§ì€ ì‚¬ëŒë“¤ê³¼ ì˜ê²¬ì„ ë‚˜ëˆ„ê³  í† ë¡ í•˜ëŠ”ê²Œ ì°¸ ì¢‹ë‹¤. week 1 ì •ë¦¬ ì‹œì‘ 5. Variableë¨¼ì € 5ê°• Variableì—ì„œ ìƒˆë¡­ê²Œ ì•ˆ ì‚¬ì‹¤ë“¤ì´ë‹¤ ì´ ë¦¬ìŠ¤íŠ¸ì˜ ë³µì‚¬, 123a = [1,2,3,4,5]b = [5,4,3,2,1]a = b ìœ„ì™€ ê°™ì´ ì½”ë“œë¥¼ í–ˆì„ë•Œ bê°€ ê°€ë¥´í‚¤ëŠ” ë©”ëª¨ë¦¬ ì£¼ì†Œë¥¼ a ê°€ ê°€ë¥´í‚¤ëŠ” í˜•íƒœì´ë‹¤ ì›ë˜ íŒŒì´ì¬ì—ì„œ ì´ëŸ° ì²˜ë¦¬ë¥¼ í•´ì£¼ì—ˆì„ë•Œ ì•ì€ ë³µì‚¬ê°€ ì¼ì–´ë‚¨ì„ ì•Œê³ ìˆì—ˆë‹¤. ì–•ì€ ë³µì‚¬ëŠ” í•™êµ ê°ì§€í”„ ì‹œê°„ì— ë°°ì›Œì„œ ê°œë…ì€ ì•Œê³ ìˆì—ˆë‹¤. ê·¸ë•Œ í”„ë¦°í„°ê¸°ê¸°ì™€ ì›ë³¸, ì‚¬ë³¸ì˜ ì˜ˆì‹œë¡œ êµìˆ˜ë‹˜ê»˜ì„œ ì„¤ëª…í•˜ì…¨ë˜ ê²ƒ ê°™ë‹¤. íŒŒì´ì¬ì—ì„œë„ Cì˜ í¬ì¸í„°ì™€ ë¹„ìŠ·í•œ ê°œë…ì´ ìˆëŠ”ë“¯ í•˜ë‹¤. Cì—ì„œì˜ ì–•ì€ ë³µì‚¬ë˜í•œ í¬ì¸í„°ê°€ ê°™ì€ ë©”ëª¨ë¦¬ë¥¼ ê°€ë¥´í‚¨ë‹¤. aê°€ ë…ë¦½ì ìœ¼ë¡œ bì˜ ê°’ì„ ê°€ì§€ê²Œ í•˜ê¸° ìœ„í•´ì„œ ê¹Šì€ë³µì‚¬ë‚˜ ì•„ë‹ˆë©´ b[:]ì´ëŸ°ì‹ìœ¼ë¡œ í•´ì¤€ë‹¤ 6. Function and Console I/Oì—¬ê¸°ì„œëŠ” f - stringì„ ì–»ì–´ê°€ì 1234name = 'cho'age = 24print(f&quot;hello my name is {name} and I'm {age} years old&quot;)# hello my name is cho and I'm 24 years old ì´ê±¸ ì´ ë§ì´ì“´ë‹¤ê³  í•œë‹¤. ë‚˜ë„ ì›ë˜ëŠ” formatì„ ë§ì´ ì¼ì—ˆëŠ”ë° ì´ì°¸ì— f-stringì„ ì¨ë´ì•¼ ê² ë‹¤ 8. String and advanced function conceptë¬¸ìì—´ í•¨ìˆ˜ì¤‘ì— ì¢€ ìµìˆ™í•˜ì§€ ì•Šì€ê²ƒë“¤ a.titile() a.startswith a.endswith a.isdigit() í°ë”°ì˜´í‘œ ì„ ì–¸ì´í›„ ì‘ì€ë”°ì˜´í‘œ ì‚¬ìš© or back slash \\ 2ì¤„ ì´ìƒì €ì¥ë²• : â€˜â€™â€™ 3ë²ˆì‚¬ìš© ì§€ì—­ë³€ìˆ˜ì™€ ì „ì—­ë³€ìˆ˜ì˜ ê°œë…ì„ íŒŒì´ì¬ì—ì„œ ì•Œì•„ë³´ì•˜ë‹¤ globalì„ ë¶™íˆë©´ í•¨ìˆ˜ë‚´ë¶€ì—ì„œ ì „ì—­ë³€ìˆ˜ë¥¼ ì‚¬ìš©ê°€ëŠ¥ ì´ë¶€ë¶„ function type hintëŠ” ì˜ ì‚¬ìš©í•˜ì§€ ì•Šë˜ê±°ë¼ ìŠê³ ìˆì—ˆë‹¤. ë‹¤ì‹œ remind í•„ìš” 1234def type_hint_example(name:str)-&gt;str: return f&quot;Hello, {name}&quot;type_hint_example('cho') ì´ë ‡ê²Œ ì‚¬ìš©ìê°€ íƒ€ì…ì„ ì•Œìˆ˜ìˆë„ë¡ (var:íŒŒë¼ë©”í„° íƒ€ì…) -&gt;ë¦¬í„´íƒ€ì…ì„ í‘œì‹œ ê·¸ë¦¬ê³  í•¨ìˆ˜ì˜ ì„¤ëª…ì„ ìœ„í•œ docstring ì„¸ê°œì˜ ë”°ì˜´í‘œë¡œ docstring ì˜ì—­í‘œì‹œ(í•¨ìˆ˜ëª… ì•„ë˜) 123456789101112131415161718def add_binary(a,b): ''' ì—¬ê¸°ì— docstringì„ ì‘ì„± ëŒ€ì¶© ì‘ì„±í•´ë³´ë©´ Returns the sum of two decimal numbers Parameters: a(int) : A decimal integer b(int) : Another decimal integer Returns : binary_sum(str) ì´ëŸ°ëŠë‚Œìœ¼ë¡œ parameterê³¼ ë¦¬í„´ê°’ë“¤ í•¨ìˆ˜ì˜ ì—­í• ì„ ì‘ì„±í•´ì¤€ë‹¤ ''' ì´ì œê¹Œì§€ ë‚¨ë“¤ì´ ë³´ëŠ” ì½”ë“œë¼ê³  ìƒê°ì•ˆí•˜ê³  ì´ëŸ°ê±¸ ëª°ëì—ˆë‹¤ dynamic typingìœ¼ë¡œ ì¸í•´ í•¨ìˆ˜ì˜ interfaceë¥¼ ì•Œê¸° í˜ë“¤ê¸° ë•Œë¬¸ì— ì•„ë˜ì™€ ê°™ì´ type hintë¥¼ ì¤„ìˆ˜ ìˆë‹¤ 12def do_function(var_name: var_type) -&gt; return_type: pass ì‚¬ìš©ìì—ê²Œ ëª…í™•í•˜ê²Œ ì•Œë ¤ì¤„ ìˆ˜ ìˆë‹¤. mypyë˜ëŠ” IDE,linterë“±ì„ í†µí•´ ì½”ë“œì˜ ë°œìƒê°€ëŠ¥í•œ ì˜¤ë¥˜ë¥¼ ì‚¬ì „ì— í™•ì¸ ì‹œìŠ¤í…œì˜ ì „ì²´ì ì¸ ì•ˆì •ì„± í•¨ìˆ˜ì‘ì„± ê°€ì´ë“œ ë¼ì¸ í•¨ìˆ˜ëŠ” ê°€ëŠ¥í•˜ì§€ë§Œ ì§§ê²Œ ì—¬ëŸ¬ê°œë¥¼ ë§Œë“ ë‹¤ í•¨ìˆ˜ ì´ë¦„ì— í•¨ìˆ˜ì˜ ì—­í• , ì˜ë„ê°€ ëª…í™•íˆ ë“¤ì–´ë‚¼ ê²ƒ í•˜ë‚˜ì˜ í•¨ìˆ˜ì—ëŠ” ìœ ì‚¬í•œ ì—­í• ì„ í•˜ëŠ” ì½”ë“œë§Œ í¬í•¨ ì¸ìë¡œ ë°›ì€ ê°’ ìì²´ë¥¼ ë°”ê¾¸ì§„ ë§ê³  ë³µì‚¬ë¡œ ì²˜ë¦¬ë¥¼ í•´ì„œ ë‹¤ë£¨ì–´ ì£¼ì Coding Convention ì¼ë°˜ì ìœ¼ë¡œ 4spaceë¥¼ ê¶Œì¥í•¨ í•œì¤„ì€ 79ìê¹Œì§€ ì—°ì‚°ìëŠ” 1ì¹¸ì´ìƒ ì•ˆ ë„ì›€ ì£¼ì„ì€ í•­ìƒ ê°±ì‹  ì½”ë“œì˜ ë§ˆì§€ë§‰ì—ëŠ” í•œì¤„ ì¶”ê°€ ì†Œë¬¸ì ì—˜ ëŒ€ë¬¸ì ì˜¤ ëŒ€ë¬¸ì ì•„ì´ì‚¬ìš©ê¸ˆì§€ ê¸°ì¤€ : flack8 ëª¨ë“ˆë¡œ ì²´í¬ 9. Data structureì—¬ê¸°ì„œëŠ” namedtupleë§Œ ì§šê³  ë„˜ì–´ê°€ì named tupleì´ë€ Tupleì˜ í˜•íƒœë¡œ dataì˜ variableë“¤ì„ ì‚¬ì „ì— ì§€ì •í•´ì„œ ì €ì¥í•œë‹¤. 1234567from collections import namedtuple# Basic examplePoint = namedtuple('Point', ['x', 'y'])#ê°ì²´ì˜ ì´ë¦„ì„ ì ì–´ì£¼ê³  ë’¤ì— variableì˜ ì´ë¦„ì„ ì ì–´ì¤Œp = Point(11, y=22)print(p[0] + p[1]) 10. Pythonic Codeì´ë¶€ë¶„ Pythonic Codeê°€ ìƒë‹¹íˆ ìƒì†Œí–ˆë‹¤ ì´ë¶€ë¶„ì„ ì˜ ì§šê³  ë„˜ì–´ê°€ì•¼ ì¶”í›„ ì½”ë“œë¥¼ ë³¼ë•Œ ë‹¤ì‹œ ì¸í„°ë„·ì„ ë’¤ì§€ê±°ë‚˜ ìë£Œë¥¼ ì°¾ì•„ë³´ëŠ” ì¼ì´ ì—†ì„ê²ƒ ê°™ë‹¤ ê·¸ë¦¬ê³  ì´ê±¸ ì˜í•˜ë©´ ê°„ì§€ë‚˜ê³  ìˆì–´ë³´ì„ List comprehensionì—ì„œ ì£¼ì˜í•´ì•¼ í• ì ì€ ì´ì°¨ì› ë¦¬ìŠ¤íŠ¸ FIlter ì´ì°¨ì› ë¦¬ìŠ¤íŠ¸ë¥¼ ë³´ë©´ 123456case1 = ['A','B','C']case2 = ['D','E','F']result = [i+j for i in case1 for j in case2]# ['AD','AE','AA','BD','BE','BA','CD','CE','CA']result = [[i+j for i in case1] for j in case2]# [['AD', 'BD', 'CD'], ['AE', 'BE', 'CE'], ['AA', 'BA', 'CA']] ê·¸ë¦¬ê³  enumerateì™€ zipì€ ì •ë§ ë§ì´ ì“°ì´ë‹ˆ ë‹¤ì‹œí•œë²ˆ indexì™€ ìš”ì†Œë“¤ì„ tupleë¡œ ë¬¶ì–´ ë°˜í™˜í•´ì¤Œ 1{i:j for i,j in enumerate('hello my name is cho'.split())} ì´ ì½”ë“œê°€ ì¢€ í•¨ì¶•ì ìœ¼ë¡œ ë‚˜íƒ€ë‚´ê³  ìˆì§€ í¸í•œ ê¸°ëŠ¥ë“¤ì„ zip : 2ê°œì´ìƒì˜ listì—ì„œ í•˜ë‚˜ì”© ì¶”ì¶œí•´ì„œ tuple í˜•íƒœë¡œ ë¦¬í„´í•´ì¤Œ map í•¨ìˆ˜ëŠ” iterableí•œ ê°ì²´ (ë°˜ë³µê°€ëŠ¥í•œ íƒ€ì…)ê³¼ í•¨ìˆ˜ë¥¼ ì£¼ì–´ ë°˜í™˜ê°’ë“¤ì„ í•˜ë‚˜í•˜ë‚˜ ê³„ì‚°í•´ì„œ ë¬¶ì–´ì¤Œ, ê·¼ë° ì´í›„ì— listë“  ë­ë“  ë³€í™˜ì´ í•„ìš” ë˜ mapí•¨ìˆ˜ëŠ” iterationì„ ìƒì„±í•˜ê¸° ë•Œë¬¸ì— generatorë¡œ ì ì´ì œ ì§„ì§œ ì¤‘ìš”í•œë° ëª°ëë˜ê±° iter ê³¼ nextí•¨ìˆ˜ 12for i in ['a','b','c']: print(i) ì´ëŸ° ë°˜ë³µë¬¸ì—ì„œ ë‚´ë¶€ì ìœ¼ë¡œ __iter__ê³¼ __next__ê°€ ì‚¬ìš©ëœë‹¤ iterableí•œ ê°ì²´ì— nextì™€ iterí•¨ìˆ˜ë¥¼ ì ìš©ì‹œì¼œ ë³´ì iterì„ ì‚¬ìš©í•˜ë©´ 123456cities = [&quot;Seoul&quot;, &quot;Busan&quot;, &quot;Jeju&quot;]iter_obj= iter(cities)print(next(iter_obj))print(next(iter_obj))print(next(iter_obj))next(iter_obj) Generator iterable objectì˜ íŠ¹ìˆ˜í•œ í˜•íƒœ ì–˜ê°€ ì—¬ê¸°ì— ì €ì¥ë˜ì–´ìˆëŒ€, ì´ê²ƒë§Œ ì•Œê³  í˜¸ì¶œì‹œì— ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê° yieldë¥¼ ì“°ë©´ í•œë²ˆì— í•˜ë‚˜ì˜ element return ë¦¬í„´ì„ yeildëŠ” ë©”ëª¨ë¦¬ì˜ ì£¼ì†Œê°’ë§Œ ê°€ì§€ê³ ìˆë‹¤ê°€ yieldê°€ ë˜ì ¸ì¤Œ ì œë„ˆë ˆì´í„°ë¥¼ ì“°ëŠ”ê²½ìš° listí˜•íƒœì˜ ë°ì´í„°ë¥¼ ë°˜í™˜í•´ì£¼ëŠ” í•¨ìˆ˜ ë°ì´í„°ì˜ í¬ê¸°ê°€ ë§¤ìš°ì»¤ì„œ ì“¸ë•Œ, í˜¸ì¶œí• ë•Œë§Œ ê·¸ê±¸ ë©”ëª¨ë¦¬ì— ì˜¬ë ¤ì„œ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì•„ë˜ ì½”ë“œì˜ ì°¨ì´ë¥¼ ì•Œì•„ë³´ì 12345678910111213141516171819202122232425262728293031323334x = [i for i in range(10)]y = (j for j in range(10))for i in x: print(i)for i in x: print(i) for j in y: print(j)for j in y: print(j) '''012340123401234ë³´ë©´ listë¡œ ë§Œë“¤ì–´ì¤€ xëŠ” 2ë²ˆ forë¬¸ì„ ì‹¤í•¼í–ˆì„ë•Œ ëª¨ë‘ ì¶œë ¥ì´ ë˜ì—ˆëŠ”ë°generatorë¡œ ë§Œë“¤ì–´ì¤€ yëŠ” ì²«ë²ˆì§¸ for ë¬¸ì€ ì¶œë ¥ì´ ë˜ëŠ”ë° ë‘ë²ˆì§¸ forë¬¸ì€ ì¶œë ¥ì´ ì•ˆëœë‹¤''' Module and Project ì—¬ê¸°ì„œ ì¢€ ìƒì†Œí•œë° ì¤‘ìš”í•œ ê°œë…ì¸ packageê°€ ë‚˜ì˜¨ë‹¤ í•˜ë‚˜ì˜ ëŒ€í˜• í”„ë¡œì íŠ¸ë¥¼ ë§Œë“œëŠ” ì½”ë“œì˜ ë¬¶ìŒ ë‹¤ì–‘í•œ ëª¨ë“ˆë“¤ì˜ í•©, í´ë”ë¡œ ì—°ê²°ë¨ __init__, mainë“± í‚¤ì›Œë“œ íŒŒì¼ëª…ì´ ì‚¬ìš©ë¨ ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ë“¤ì´ packageë¡œ ê´€ë¦¬ê°€ ëœë‹¤ __init__ í˜„ì¬ í´ë”ê°€ íŒ¨í‚¤ì§€ì„ì„ ì•Œë¦¬ëŠ” ì´ˆê¸°í™” ìŠ¤í¬ë¦½íŠ¸ ì—†ì„ ê²½ìš° íŒ¨í‚¤ì§€ë¡œ ê°„ì£¼í•˜ì§€ ì•ŠìŒ í•˜ìœ„í´ë”ì™€ pyíŒŒì¼ì„ ëª¨ë‘ í¬í•¨í•¨ import ì™€ allì„ ì‚¬ìš©í•œë‹¤ 1234__all__ = ['image', 'sound', 'stage']from . import imagefrom . import soundfrom . import stage ì´ëŸ°ëŠë‚Œìœ¼ë¡œ í˜„ì¬ì‚¬ìš©í•  __main__ 123456from sound import echoif __name__ == 'main': print(&quot;hello game&quot;) print(echo.echo_play())#ì´ëŸ° íŒŒì¼ë¡œë‹¤ê°€ from game.graphic.render impoer render_test() from .render import render_test() í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€ from ..sound.echo ..2ê°œë©´ ìƒìœ„ ë””ë ‰í† ë¦¬ë¡œ ê°€ì File/Exeption/Log Handling ì˜ˆìƒë¶ˆê°€ëŠ¥í•œ ì˜ˆì™¸ ì¸í„°í”„ë¦¬í„° ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸, ê°œë°œì ì‹¤ìˆ˜ ë¦¬ìŠ¤íŠ¸ì˜ ë²”ìœ„ë¥¼ ë„˜ì–´ê°€ëŠ” ê°’ í˜¸ì¶œ, ì •ìˆ˜ 0ìœ¼ë¡œ ë‚˜ëˆ” ìˆ˜í–‰ ë¶ˆê°€ì‹œ ì¸í„°í”„ë¦¬í„°ê°€ ìë™ í˜¸ì¶œ ì˜ˆìƒì´ ë¶ˆê°€ëŠ¥ í•œ ì˜ˆì™¸ ë°œìƒì‹œ exception handlingì˜ ëŒ€ì²˜ê°€ í•„ìš”í•˜ë‹¤ 12345678910111213141516171819202122232425262728try: ì˜ˆì™¸ ë°œìƒ ê°€ëŠ¥ ì½”ë“œexcept &lt;Exception Type&gt;: ì˜ˆì™¸ ë°œìƒì‹œ ëŒ€ì‘ë˜ëŠ” ì½”ë“œ for i in range(10): try: print(10/i) except ZeroDivisionError: print(&quot;Not divided by 0&quot;) except IndexError as e: print(e) except Exception as e: print(e) #ì¼ë°˜ì ìœ¼ë¡œ ì „ì²´ë¥¼ Exceptionìœ¼ë¡œ ì¡ëŠ”ê±°ëŠ” ì¢‹ì§€ ì•Šë‹¤ finally: print(i,'-----') while True: value = input for difit in value: if digit not in '0123456789': raise ValueError('ìˆ«ìê°€ ì•„ë‹™ë‹ˆë‹¤') print('ì •ìˆ˜=',int(value))# raise êµ¬ë¬¸ ì‚¬ìš©ì‹œ ë¬´ì¡°ê±´ errorë¥¼ ë°œìƒì‹œí‚¨ë‹¤# assert isinstance(decimal_number, int) íŒŒì´ì¬ì—ì„œëŠ” try exceptêµ¬ë¬¸ì„ ê¶Œì¥í•œë‹¤ Python File I/Oread() txt íŒŒì¼ ì•ˆì— ìˆëŠ” ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ ë°˜í™˜ f = open(â€˜i_have_a_dream.txtâ€™,â€™râ€™) contents = f.read() ì—¬ê¸°ì„œ fëŠ” ì£¼ì†Œë¥¼ ê°€ì§€ê³  ìˆëŠ”ê±°ì„ print(contents) f.close() íŒŒì´ì¬ì—ì„œ withëŠ” ë¬´ì—‡ì¸ê°€ë¥¼ ì‚¬ìš©í›„ ë°˜ë‚©í•´ì•¼ í•˜ëŠ” ê²½ìš° ì£¼ë¡œ ì‚¬ìš©ì´ ëœë‹¤ íŒŒì¼ì„ ì—´ê³  ë‹«ì„ë•Œ ë‹«ëŠ”ê±¸ ê¹Œë¨¹ì„ ìˆ˜ ìˆìœ¼ë‹ˆ íŒŒì´ì¬ì˜ withêµ¬ë¬¸ê³¼ í•¨ê»˜ ì‚¬ìš©í•œë‹¤ 123with open(&quot;i_have_a_dream.txt&quot;,'r') as my_file: contents = my_file.read() print(type(contents),contents) withê°€ ëë‚ ë•Œ ìë™ìœ¼ë¡œ closeí•´ì¤Œ indentationì´ ìˆì„ë•ŒëŠ” withì•ˆì˜ ì˜ì—­ì´ ê³„ì† ì‚¬ìš©ë˜ê³  ëë‚˜ë©´ ìë™ìœ¼ë¡œ ë°˜ë‚©í•˜ëŠ” ê°œë… 123with open(&quot;i_have_a_dream.txt&quot;,'r') as f: content_list = f.readlines() #readlines í•¨ìˆ˜ì‚¬ìš©ì‹œ í•œì¤„ì”© ì½ì–´ì˜´ type(content_list) #listí˜•íƒœë¡œ ìˆœì°¨ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° ë•Œë¬¸ì— listê°€ ì¶œë ¥ë¨ 12345678with open(&quot;i_have_a_dream.txt&quot;,'r') as f: i = 0 while 1: line = f.readline() # readlinesê°€ ì•„ë‹Œ readlineì„ ì“°ë©´ í•œì¤„ë§Œì˜¬ë¦¼ if not line: break print(str(i) + '===' + line.replace('\\n','')) i+=1 dataê°€ ë„ˆë¬´ì»¤ì„œ í•œë²ˆì— ë©”ëª¨ë¦¬ì— ì˜¬ë¦´ìˆ˜ ì—†ë‹¤ë©´ í•œì¤„ì”© ì½ì–´ì„œ í•´ì£¼ê³  ì‹¶ì€ ì‘ì—…ì„ í•´ì¤€ë‹¤ encoding = â€˜utf8â€™ ì´ê±¸ ì´ë ‡ê²Œ ì •í•´ì£¼ì–´ì•¼ì§€ í—™ì—…ë„ í•˜ê³  ê·¸ëŸ¼ 123456789 with open(&quot;i_have_a_dream.txt&quot;,mode = 'a', encoding) as f:#ìœ„ì™€ ê°™ì´ modeë¥¼ a ë¡œ ì„¤ì •í•˜ë©´ ì“°ê¸° ì½ê¸° ë‹¤ ê°€ëŠ¥í•˜ë‹¤ import ostry: os.mkdir('abc') #í˜„ì œ ë””ë ‰í† ë¦¬ì— ìƒˆë¡œìš´ dirë¥¼ ë§Œë“ ë‹¤except FileExistsError as e: print(&quot;Already created&quot;)os.path.isfile(file.ipynb) ìœ„ì™€ ê°™ì€ ì˜ˆì œë¥¼ ë°˜ë“œì‹œ í™•ì¸í•´ ë³´ì ë¨¼ì € if not os.path.isdirì„ ì‚¬ìš©í•˜ë©´ í˜„ì¬ dirì— logë¼ëŠ” dirì´ìˆëŠ”ì§€ í™•ì¸í•œë‹¤ ê·¸ì´í›„ logì•ˆì— count_logê°€ ì—…ë‹¤ë©´ ìƒì„±í•´ì£¼ê³  ë‹«ëŠ”ë‹¤ withêµ¬ë¬¸ ì‹œì‘ì‹œ logë¥¼ ì—´ì–´ì„œ ê·¸ìœ„ì— forë¬¸ì„ ìˆœíšŒí•˜ë©° count_logì— ë¬¸ì¥ì„ ì¶”ê°€í•œë‹¤ PickleíŒŒì´ì¬ì˜ ê°ì²´ë¥¼ ì˜ì†í™”í•˜ëŠ” built in ê°ì²´ ê°ì²´ëŠ” ì›ë˜ memoryì— ìˆì–´ì•¼ í•œë‹¤ ì´ memoryì— ìˆëŠ”ê±°ë¥¼ ì´ ê°ì²´ë¥¼ ì‹¤í–‰ì¤‘ ì •ë³´ë¥¼ ì €ì¥í•˜ê³  ì´í›„ì— ë¶ˆëŸ¬ì˜¬ë•Œ ì‚¬ìš©í•œë‹¤ ê°ì²´ëŠ” ì›ë˜ interpreterê°€ ëë‚ ë•Œ ê°™ì´ ì‚¬ë¼ì§€ê¸° ë–„ë¬¸ì— ì´ë¥¼ ì €ì¥í•  ìˆ˜ ìˆëŠ” pickleì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤ 1234567891011import picklef = open(&quot;list.pickle&quot;, &quot;wb&quot;) # pickleì€ binary íŒŒì¼ì´ë¼ wbtest = [1,2,3,4]pickle.dump(test,f)f.close()f = open(&quot;list.pickle&quot;,'rb')test_pickle = pickle.load(f)test_picklef.close()#ì´ëŸ¬ë©´ pickleì„ ë¶ˆëŸ¬ì™€ì„œ ì´ì „ì˜ testê°’ì„ í”„ë¡œê·¸ë¨ ì¢…ë£Œí›„ì—ë„ ì½ì–´ì˜¬ ìˆ˜ ìˆë‹¤ Loggingë ˆë²¨ë³„ë¡œ ê¸°ë¡ì„ ë‚¨ê¸¸ í•„ìš”ê°€ìˆìŒ 1234567import logginglogging.debug(&quot;í‹€ë ¸ìë‚˜&quot;) #debugginglogging.info(&quot;í™•ì¸í•´&quot;) #logging.warning(&quot;ì¡°ì‹¬í•´&quot;)logging.error(&quot;ì—ëŸ¬ë‚¬ì–´&quot;)logging.critical(&quot;ë§í–ˆë‹¤&quot;) #í”„ë¡œê·¸ë¨ì´ ì™„ì „íˆ ì˜ë„ì¹˜ì•Šê²Œ ì¢…ë£Œê°€ ë˜ì—ˆì„ë•Œ ê°œë°œì‹œì ,ìš´ì˜ì‹œì ë§ˆë‹¤ ë‹¤ë¥¸ logê°€ ë‚¨ì„ ìˆ˜ ìˆë„ë¡ ì§€ì›í•¨ Debug/info/warning/error/critical debug : ê°œë°œì‹œ ê¸°ë¡ì„ ë‚¨ê²¨ì•¼ í•˜ëŠ” ë¡œê·¸ ì •ë³´ë¥¼ ë‚¨ê¹€ Info : ì²˜ë¦¬ê°€ ì§„í–‰ë˜ëŠ” ë™ì•ˆì˜ ì •ë³´ë¥¼ ì•Œë¦¼ warning : ì‚¬ìš©ìê°€ ì˜ëª» ì •ë³´ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ì²˜ë¦¬ëŠ” ê°€ëŠ¥í•˜ë‚˜ ê°œë°œì‹œ ì˜ë„ì¹˜ ì•ŠëŠ” ì •ë³´ê°€ ë“¤ì–´ì™”ì„ë•Œ error : ì˜ëª»ëœ ì²˜ë¦¬ë¡œ ì¸í•´ ì—ëŸ¬ê°€ ë‚¬ìœ¼ë‚˜, í”„ë¡œê·¸ë¨ ë™ì‘ì€ ê°€ëŠ¥ í•  ë•Œ critical : ì˜ëª»ëœ ì²˜ë¦¬ë¡œ ë°ì´í„° ì†ì‹¤ì´ë‚˜ ë”ì´ìƒ í”„ë¡œê·¸ë¨ì´ ë™ì‘í•  ìˆ˜ ì—†ìŒì„ ì•Œë¦¼ ìœ„ì˜ ì½”ë“œë¥¼ ëŒë ¤ë³´ë©´ ì¡°ì‹¬í•´,ì—ëŸ¬ë‚¬ì–´,ë§í–ˆë‹¤ë§Œ í‘œì‹œê°€ ëœë‹¤ ê·¸ ì´ìœ ëŠ” logging levelì´ warningì´ìƒìœ¼ë¡œ settingì´ ë˜ì–´ìˆì–´ì„œ ì´ë‹¤ íŒŒì´ì¬ì˜ ë¡œê¹…ë ˆë²¨ì€ ê¸°ë³¸ì ìœ¼ë¡œ warningë¶€í„° ì´ë‹¤ logging.basicConfig(level = logging.DEBUG) steam_handler = logging.FileHandler('my.log',mode = 'w',encoding = 'utf8') logger.addHandler(steam_handler) ì´ëŸ¬í•œ ì½”ë“œ ì‘ì„±ì‹œ 1. configparser ì‹¤í–‰ ì„¤ì •ì„ íŒŒì¼ì— ì €ì¥ 2. argparser ì‹¤í–‰ì‹œì ì— configparser í”„ë¡œê·¸ë¨ì˜ ì‹¤í–‰ ì„¤ì •ì„ fileì— ì €ì¥í•¨ section, key,valueê°’ì˜ í˜•íƒœë¡œ ì„¤ì •ëœ íŒŒì¼ì„ ì‚¬ìš© ì„¤ì •íŒŒì¼ì„ Dict Typeìœ¼ë¡œ í˜¸ì¶œí›„ ì‚¬ìš© configparserë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶ˆëŸ¬ì˜¤ê³  argparser console ì°½ì—ì„œ í”„ë¡œê·¸ë¨ ì‹¤í–‰ì‹œ settingì •ë³´ë¥¼ ì €ì¥í•¨ comand-Line-Optionì´ë¼ê³  ë§ì´ë¶€ë¦„ ë³´í†µ cmd ì°½ì—ì„œ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•  ë•Œ -ë‚˜ â€“ë¥¼ ë¶™í˜€ì„œ logíŒŒì¼ì„ ì‹¤í–‰í•œë‹¤ ex) ls â€“help helpí•˜ëŠ” ì´ë¦„ìœ¼ë¡œ ì‹¤í–‰ì„ ì‹œì¼œë³´ì•„ë¼ ì´ëŸ°ëŠë‚Œ argê°’ì„ ë„£ì–´ì¤€ë‹¤ ë³´í†µ -í•œê°œëŠ” ì§§ì„ ë•Œ, â€“ëŠ” ê¸¸ë•Œ parser.add_argumentì‚¬ìš©ì‹œ ì…ë ¥ì„ ë°›ì•„ argsì— ì €ì¥í•´ì¤€ë‹¤ ê·¸ë•Œ ì§€ì •í•´ì¤€ formatì¸ -aë©´ aì— ì €ì¥ì„ í•´ì£¼ê³  -bë‚˜ â€“b_valueë©´ bì— ì €ì¥í•´ì¤€ë‹¤ ì‚¬ìš©ìê°€ cmdì°½ì—ì„œ ì‹¤í—˜ì„ í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤ epochê°’, lrê°’,momentumê°’,cudaì—¬ë¶€ë“±ì„ ë°”ë¡œ ì‹¤í–‰ì‹œì— ì§€ì •í•´ì¤„ìˆ˜ ìˆë‹¤. Python data handlingì´ë¶€ë¶„ì—ì„œ xml, json,csvíŒŒì¼ë“¤ì— ëŒ€í•œ ê°œë…ì„ ì •ë¦¬í•˜ê³  ê°„ë‹¤ ì •ê·œì‹ë„ ê¼­ ëª°ëë˜ ë¶€ë¶„ì´ë‹ˆ ì§šê³  ë„˜ì–´ê°€ì Comma Separate Value ì—‘ì…€ ì–‘ì‹ì˜ ë°ì´í„°ë¥¼ í”„ë¡œê·¸ë¨ì— ìƒê´€ì—†ì´ ì“°ê¸° ìœ„í•œ ë°ì´í„° í˜•ì‹ì´ë¼ê³  ìƒê°í•˜ë©´ ì‰¬ì›€ TSV,SSVë“±ìœ¼ë¡œ êµ¬ë¶„í•´ì„œ ë§Œë“¤ê¸°ë„ í•¨ notepadë¡œë„ ì—´ ìˆ˜ ìˆê³ , ì‰¼í‘œë¡œ êµ¬ë¶„ì´ ë˜ì–´ìˆë‹¤. ìœ„ì—ì„œ data_headerë¼ëŠ”ê²Œ ìˆë‹¤. ì—¬ê¸°ì—ëŠ” ë°ì´í„°ì˜ í•„ë“œê°€ ë‹´ê²¨ìˆã…‡ë©° ë°ì´í„° ì €ì¥ì‹œ ,ë¡œ ë¶„ë¦¬ë¥¼ í•˜ëŠ” ì½”ë“œì´ë‹¤ ì²«ë²ˆì§¸ dataëŠ” ë¬´ì¡°ê±´ dataì˜ í•„ë“œì´ë‹¤ ex) data,indexì´ëŸ° ëŠë‚Œìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ì´ë‹¤ ë”°ë¼ì„œ ì²«ì¤„ì´ë¼ë©´ ,ë¡œ ë‚˜ëˆ„ì–´ì„œ data_headerë¼ëŠ” ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•´ì¤€ë‹¤ ì´í›„ ì¤„ë¶€í„°ëŠ” ,ë¡œ ë‚˜ëˆ„ì–´ì„œ í•œì¤„ì”© ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•´ì¤€ë‹¤ textíŒŒì¼ í˜•íƒœë¡œ ë°ì´í„° ì²˜ë¦¬ì‹œ ë¬¸ì¥ë‚´ì— ë“¤ì–´ê°€ìˆëŠ” â€œ,â€ ë“±ì— ëŒ€í•´ ì „ì²˜ë¦¬ ê³¼ì •ì´ í•„ìš”í•˜ë‹¤ íŒŒì´ì¬ì—ì„œëŠ” ê°„ë‹¨íˆ CSVíŒŒì¼ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ csvê°ì²´ë¥¼ ì œê³µí•¨ 12import csvreader = csv.reader(f, delimiter = ',', quotechar = '&quot;',quoting = csv.QUOTE_ALL) delimiter : ê¸€ìë¥¼ ë‚˜ëˆ„ëŠ” ê¸°ì¤€ (default : â€˜,â€™ ) lineterminator : ì¤‘ë°”ê¿ˆê¸°ì¤€ (default : \\r\\n) quotechar : ë¬¸ìì—´ì„ ë‘˜ëŸ¬ì‹¸ëŠ” ì‹ í˜¸ ë¬¸ì (default : â€œ ) quoting : ë°ì´í„°ë¥¼ ë‚˜ëˆ„ëŠ” ê¸°ì¤€ì´ quotecharì— ì˜í•´ ë‘˜ëŸ¬ì‹¸ì¸ ë¼ë²¨ 123456789101112131415161718192021222324import csvseoung_nam_data = []header = []rownum = 0with open(&quot;korea_floating_population_data.csv&quot;,&quot;r&quot;, encoding=&quot;cp949&quot;) as p_file: csv_data = csv.reader(p_file) #csv ê°ì²´ë¥¼ ì´ìš©í•´ì„œ csv_data ì½ê¸° for row in csv_data: #ì½ì–´ì˜¨ ë°ì´í„°ë¥¼ í•œ ì¤„ì”© ì²˜ë¦¬ if rownum == 0: header = row #ì²« ë²ˆì§¸ ì¤„ì€ ë°ì´í„° í•„ë“œë¡œ ë”°ë¡œ ì €ì¥ location = row[7] #â€œí–‰ì •êµ¬ì—­â€í•„ë“œ ë°ì´í„° ì¶”ì¶œ, í•œê¸€ ì²˜ë¦¬ë¡œ ìœ ë‹ˆì½”ë“œ ë°ì´í„°ë¥¼ cp949ë¡œ ë³€í™˜ if location.find(u&quot;ì„±ë‚¨ì‹œ&quot;) != -1: seoung_nam_data.append(row) #â€í–‰ì •êµ¬ì—­â€ ë°ì´í„°ì— ì„±ë‚¨ì‹œê°€ ë“¤ì–´ê°€ ìˆìœ¼ë©´ seoung_nam_data Listì— ì¶”ê°€ rownum +=1with open(&quot;seoung_nam_floating_population_data.csv&quot;,&quot;w&quot;, encoding=&quot;utf8&quot;) as s_p_file: writer = csv.writer(s_p_file, delimiter='\\t', quotechar=&quot;'&quot;, quoting=csv.QUOTE_ALL) # csv.writerë¥¼ ì‚¬ìš©í•´ì„œ csv íŒŒì¼ ë§Œë“¤ê¸° delimiter í•„ë“œ êµ¬ë¶„ì # quotecharëŠ” í•„ë“œ ê° ë°ì´í„°ëŠ” ë¬¶ëŠ” ë¬¸ì, quotingëŠ” ë¬¶ëŠ” ë²”ìœ„ writer.writerow(header) #ì œëª© í•„ë“œ íŒŒì¼ì— ì“°ê¸° for row in seoung_nam_data: writer.writerow(row) #seoung_nam_dataì— ìˆëŠ” ì •ë³´ listì— ì“°ê¸° ìœ„ëŠ” ìœ ë™ì¸êµ¬ ë°ì´í„°ì¤‘ ì„±ë‚¨ì˜ ë°ì´í„°ë§Œì„ ìˆ˜ì§‘í•˜ëŠ” ì½”ë“œì´ë‹¤ windowì—ì„œ ê´€ë¦¬ë˜ëŠ” ì½”ë“œëŠ” cp949ì´ë‹¤ vscodeëŠ” utf8ì´ê¸° ë–„ë¬¸ì— encodingì„ ë°”ê¾¸ì–´ ì£¼ì–´í– í•œë‹¤ ë”°ë¼ì„œ ì½ì„ ë•Œ cp949ë¡œ ì½ëŠ”ë‹¤ê³  ë³„ë„ë¡œ ì§€ì •í•œë‹¤ encodingì€ cp949, utf8ë¡œ ì™ ë§Œí•˜ë©´ ì €ì¥í•˜ê¸° ì™ ë§Œí•˜ë©´ ì‘ì€ â€˜ ì´ê±¸ë¡œ ë‚˜ëˆ„ê³  ë³´í†µ csvëŠ” ë‹¤ë¥¸ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì§€ê¸ˆì€ ì´ê±°ì— ì§‘ì°©í•  í•„ìš”ê°€ ì—†ë‹¤ ì™ ë§Œí•˜ë©´ pandasë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— WebHTML ì œëª©, ë‹¨ë½, ë§í¬ ë“± ìš”ì†Œ í‘œì‹œë¥¼ ìœ„í•´ Tagë¥¼ ì‚¬ìš© ëª¨ë“  ìš”ì†Œë“¤ì€ êº¾ì‡  ê´„í˜¸ ì•ˆì— ë‘˜ëŸ¬ ìŒ“ì—¬ìˆìŒ &lt;title&gt; Hello, World &lt;/title&gt; ë³´í†µ HTMLë„ ì¼ì¢…ì˜ í”„ë¡œê·¸ë¨ì´ê¸° ë–„ë¬¸ì— ê·œì¹™ì„ ë¶„ì„í•˜ì—¬ ë°ì´í„°ì˜ ì¶”ì¶œì´ ê°€ëŠ¥í•˜ë‹¤ ì¶”ì¶œëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•˜ì—¬ ë‹¤ì–‘í•œ ë¶„ì„ì´ ê°€ëŠ¥ string, ì •ê·œì‹ (regex), beautifulSoup Refular expression ë³µì¡í•œ ë¬¸ìì—´ íŒ¨í„´ì„ ì •ì˜í•˜ëŠ” ë¬¸ì í‘œí˜„ ê³µì‹ íŠ¹ì •í•œ ê·œì¹™ì„ ê°€ì§„ ë¬¸ìì—´ì˜ ì§‘í•©ì„ ì¶”ì¶œ ê¸°ë³¸ë¬¸ë²• ë¬¸ìí´ë˜ìŠ¤ [ ] : [ì™€] ì‚¬ì´ì˜ ë¬¸ìë“¤ê³¼ ë§¤ì¹˜ë¼ëŠ” ì˜ë¯¸ ex) [abc] : í•´ë‹¹ê¸€ìê°€ a,b,cì¤‘ í•˜ë‚˜ê°€ ìˆë‹¤ - : ë²”ìœ„ë¥¼ ì§€ì •ê°€ëŠ¥ ì •ê·œì‹ í‘œí˜„ì„ ìœ„í•´ ì›ë˜ì˜ë¯¸ê°€ ì•„ë‹Œ ë‹¤ë¥¸ìš©ë„ë¡œ ì‚¬ìš©ë˜ëŠ” ë¬¸ìë“¤ . ^ $ * + ? { } [ ] : ( ) .ì€ ì „ì²´ë¥¼ ì˜ë¯¸í•œë‹¤ * ëŠ” ì•ì—ìˆëŠ”ê¸€ìë¥¼ì„ ë°˜ë³µí•´ì„œ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ +ë¥¼ í•˜ê²Œ ë˜ë©´ ì•ì— ìˆëŠ” ê¸€ìë¥¼ ìµœì†Œ 1íšŒì´ìƒ ë°˜ë³µ ì•ì—ìˆëŠ”ê¸€ìë¥¼ ìµœì†Œí•œ 1íšŒ ì´ìƒ ë°˜ë³´ê¸°ëƒì¶”ë°° {ìˆ«ì} : ë°˜ë³µíšŸìˆ˜ë¥¼ ì§€ì •í•´ì¤„ ìˆ˜ ìˆë‹¤ ? : ë°˜ë³µíšŸìˆ˜ê°€ 1íšŒ | : or ex) (0|1){3} 0ì´ë‚˜ 1ì´ 3ë²ˆ ë°˜ë³µ ì •ê·œì‹ ì¶”ì¶œ ì—°ìŠµ Expressionë¶€ë¶„ì„ ìˆ˜ì •í•´ê°€ë©´ Zipë¡œ ëë‚˜ëŠ” íŒŒì¼ëª…ë§Œ ì¶”ì¶œ Expressionì— (http)(.+)(zip) ì •ê·œì‹ in python reëª¨ë“ˆì„ import í•˜ì—¬ ì‚¬ìš© : import re í•¨ìˆ˜ : search - í•œê°œë§Œ ì°¾ê¸°, findall - ì „ì²´ì°¾ê¸° ì¶”ì¶œëœ íŒ¨í„´ì€ tupleë¡œ ë°˜í™˜ë¨ 123456789import reimport urllib.requesturl= &quot;https://bit.ly/3rxQFS4&quot;html= urllib.request.urlopen(url)html_contents= str(html.read())id_results= re.findall(r&quot;([A-Za-z0-9]+\\*\\*\\*)&quot;,html_contents)#findallì „ì²´ì°¾ê¸°, íŒ¨í„´ëŒ€ë¡œë°ì´í„°ì°¾ê¸°for result in id_results: print(result) urllibì˜ requestí•œì˜ urlopení•¨ìˆ˜ë¥¼ ì“°ë©´ í•´ë‹¹ ì£¼ì†Œì— í•´ë‹¹í•˜ëŠ” cotentsë¥¼ ê°€ì ¸ì˜´ findallë¡œ í•´ë‹¹íŒ¨í„´ì„ ê°€ì§„ ëª¨ë“  dataë¥¼ ì°¾ì•„ì„œ tupleì˜ í˜•íƒœë¡œ listì— ë‹´ê¸°ê²Œë¨ ë”°ë¼ì„œ for loopìœ¼ë¡œ ì°ì–´ë‚´ë©´ í•˜ë‚˜ì”© ë‚˜ì˜¨ë‹¤ XMLtagì–´ã…£ tagì‚¬ì´ì— ê°’ì´ í‘œì‹œë˜ê³  êµ¬ì¡°ì ì¸ ì •ë³´ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆìŒ HTMLê³¼ ë¬¸ë²•ì´ ë¹„ìŠ·, ëŒ€í‘œì ì¸ ë°ì´í„° ì €ì¥ë°©ì‹ XMLì€ ì»´í“¨í„°ê°„ì˜ ì •ë³´ë¥¼ ì£¼ê³ ë°›ê¸° ë§¤ìš°ìœ ìš©í•œ ì €ì¥ë°©ì‹ìœ¼ë¡œ ì“°ì„ ì •ê·œí‘œí˜„ã……ã„±ìœ¼ë¡œ Parsingì´ ê°€ëŠ¥í•¨ ê°€ì¥ë§ì´ ì“°ì´ëŠ” parserì¸ beautifulsoup lxmlê³¼ htmlì˜ from bs4 import BeautifulSoup soup = BeautifulSOup(book_xml,â€™lxmlâ€™) soup.find_all(â€œauthorâ€) tagë¥¼ ì°¾ëŠ” í•¨ìˆ˜ find_all 123456789101112131415161718192021222324252627import urllibfrom bs4 import BeautifulSoupwith open(&quot;US08621662-20140107.XML&quot;, &quot;r&quot;, encoding=&quot;utf8&quot;) as patent_xml: xml = patent_xml.read() # Fileì„ Stringìœ¼ë¡œ ì½ì–´ì˜¤ê¸°soup = BeautifulSoup(xml, &quot;xml&quot;) # xml parser í˜¸ì¶œinvention_title_tag = soup.find(&quot;invention-title&quot;)print(invention_title_tag.get_text())publication_reference_tag = soup.find(&quot;publication-reference&quot;)p_document_id_tag = publication_reference_tag.find(&quot;document-id&quot;)p_country = p_document_id_tag.find(&quot;country&quot;).get_text()p_doc_number = p_document_id_tag.find(&quot;doc-number&quot;).get_text()p_kind = p_document_id_tag.find(&quot;kind&quot;).get_text()p_date = p_document_id_tag.find(&quot;date&quot;).get_text()print(p_doc_number, p_kind, p_date)application_reference_tag = soup.find(&quot;application-reference&quot;)a_document_id_tag = publication_reference_tag.find(&quot;document-id&quot;)a_country = p_document_id_tag.find(&quot;country&quot;).get_text()a_doc_number = p_document_id_tag.find(&quot;doc-number&quot;).get_text()a_date = p_document_id_tag.find(&quot;date&quot;).get_text()print(a_country, a_doc_number, a_date) ì´ì¤‘ìœ¼ë¡œ tagê°€ ìŒ“ì—¬ìˆëŠ” ì •ë³´ëŠ” ì´ì¤‘ìœ¼ë¡œ ì½”ë“œë„ ì‘ì„±í•´ ì£¼ì–´ì•¼ í•œë‹¤ ì—°ìŠµ ipg140107.xmlë¶„ì„ ë¶„í• ëœ íŠ¹í—ˆë¬¸ì„œë¡œ ë¶€í„° íŠ¹í—ˆì˜ ë“±ë¡ë²ˆí˜¸,ë“±ë¡ì¼ì,ì¶œì›ë²ˆí˜¸ë“±ë“±ì„ ë½‘ì•„ CSVíŒŒì¼ë¡œ ë§Œë“¤ì–´ë³´ì JSONíŒŒì´ì¬ì—ì„œ json ëª¨ë“ˆì„ ì´ìš©í•˜ì—¬ì†ì‰½ê²Œ íŒŒì‹± ë° ì €ì¥ ê°€ëŠ¥ ë°ì´í„° writeì™€ readëŠ” ëª¨ë‘ dict ì™€ ìƒí˜¸í˜¸í™˜ ê°€ëŠ¥ ëŒ€ë¶€ë¶„ ìš”ì¦˜ì€ jsonì„ ì‚¬ìš©í•œë‹¤","link":"/2021/01/22/2021-01-22-Boostcamp_week1/"},{"title":"Attention Is All You Need","text":"AbstractSeuence transduction modelë“¤ì€ í˜„ì¬ ë³µì¡í•œ recurrentí•œ êµ¬ì¡° (RNN) ì´ë‚˜ encoder decoderë¥¼ í¬í•¨í•œ CNNì´ ì£¼ë¥¼ ì´ë£¬ë‹¤. ê°€ì¥ ì¢‹ì€ì„±ëŠ¥ì„ ë‚´ëŠ” modelë˜í•œ attention mechanismì„ ì´ìš©í•˜ì—¬ encoderì™€ decoderë¥¼ ì—°ê²°í•˜ëŠ” í˜•íƒœì´ë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ìƒˆë¡œìš´ ë°©ë²•ì¸ Transformerë¥¼ ì œì•ˆ ì´ëŠ” ì˜¤ë¡œì§€ attention mechanismë§Œì„ ì‚¬ìš©! ì´ëŠ” RNNì´ë‚˜ CNNë³´ë‹¤ ë” ë³‘ë ¬í™”ê°€ ê°€ëŠ¥í•˜ê³  trainí•˜ëŠ”ë° ì ì€ ì‹œê°„ì´ ê±¸ë¦°ë‹¤! WMT 2014 English to-German dataë¥¼ ì‚¬ìš©í•˜ì—¬ BLEUë¼ëŠ” scoreì—ì„œ 28.4ì ì„ ì–»ì—ˆë‹¤.(ì—¬ëŸ¬ ë…¼ë¬¸ì„ ì½ë‹¤ë³´ë©´ ìì£¼ ë“±ì¥í•˜ëŠ” ì´ BLUE scoreì€ ì •ë¦¬í•´ ë†“ì€ê²Œ ìˆëŠ”ë° ì¶”í›„ì— posting ) ì´ëŠ” ì•™ìƒë¸”ì„ í¬í•¨í•œ ì´ì „ì˜ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ë³´ë‹¤ 2BLUEê°€ ë†’ë‹¤. 1. Introduction RNNëª¨ë¸ (LSTMì´ë‚˜ GRU)ëŠ” machine translationê³¼ ê°™ì€ sequence modelingì˜ State of the artí•œ(ìµœì‹ ì˜ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì˜) ì ‘ê·¼ë°©ì‹ìœ¼ë¡œ ì•Œë ¤ì ¸ìˆë‹¤. RNNì€ inputê³¼ outputì˜ ìœ„ì¹˜ë¥¼ ê³„ì‚°í•œ ê²°ê³¼ë¥¼ ë‹´ê³ ìˆë‹¤. ê³„ì‚°í•˜ëŠ” ì‹œê°„ì´ë‚˜ ìˆœì„œì— ì˜í•´ ì •ë ¬ëœ ìœ„ì¹˜ë“¤ì€ ì´ì „ì˜ hidden state ht-1ë¡œ í‘œí˜„ëœ ì—°ì†ì ì¸ hidden state htë¥¼ ìƒì„±í•œë‹¤. ì´ê²ƒì€ ë³¸ì§ˆì ìœ¼ë¡œ training examplesì˜ ë³‘ë ¬í™”ë¥¼ ë°°ì œí•˜ë©°, ì´ë¡œì¸í•´ memoryì˜ í•œê³„ë¡œ ì¸í•œ batch sizeì˜ í•œê³„ ë•Œë¬¸ì— ê¸´ sequence lengthì— êµ‰ì¥íˆ criticalí•œ ìš”ì†Œë¡œ ì‘ìš©í•œë‹¤. ìµœê·¼ì˜ ì—°êµ¬ë“¤ì€ factorizationê³¼ conditional computation(ì´ê²ƒì— ëŒ€í•œ ë…¼ë¬¸: Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks, Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer )ì„ ì´ìš©í•œ ê³„ì‚°ê³¼ì •ì˜ íš¨ìœ¨í™”ë¡œ í° ë°œì „ì„ ì´ë£¨ì–´ëƒˆë‹¤. í•˜ì§€ë§Œ ìˆœì°¨ì ì¸ ê³„ì‚°ì— ì˜í•œ ì œì•½ì€ ì•„ì§ ë‚¨ì•„ìˆë‹¤ Attention mechanismì€ inputê³¼ outì‚¬ì´ì˜ ê¸¸ì´ì— ìƒê´€ì—†ì´ dependenciesë¥¼ modelingí• ìˆ˜ìˆë‹¤ëŠ” ë¶€ë¶„ì—ì„œ sequence modelingê³¼ transduction modelingì˜ í•„ìˆ˜ì ì¸ ë¶€ë¶„ì´ ë˜ì—ˆë‹¤. í•˜ì§€ë§Œ ëª‡ëª‡ ê²½ìš°ì—ì„œëŠ” ì•„ì§ attention mechanismê³¼ RNNì„ í•©ì³ì„œ ì‚¬ìš©í•˜ê³  ìˆë‹¤. ì´ ì—°êµ¬ì—ì„œëŠ” Transformerë¼ëŠ” attention mechanismì—ë§Œ ì˜ì¡´í•˜ì—¬ inputê³¼ outputì˜ dependencyë¥¼ ì´ëŒì–´ë‚´ëŠ” architectureì„ ì œì•ˆí•œë‹¤. Transformerì€ ë³‘ë ¬í™”ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ê³ , ì„±ëŠ¥ì„ ë”ìš± í–¥ìƒì‹œí‚¬ìˆ˜ ìˆë‹¤ ìš”ì•½ : ìš°ë¦¬ì˜ transformerê°€ training exampleì˜ ë³‘ë ¬í™”ë¡œ ì¸í•œ ì†ë„ í–¥ìƒê³¼ ì¢‹ì€ ì ìˆ˜ë¥¼ ë‚¸ë‹¤. 2. Backgroundencoder , decoderì—ëŒ€í•œ background ì¤‘ê°„ì˜ latent spaceëŠ” inputì´ë‚˜ outputë³´ë‹¤ í›¨ì”¬ ìµœì†Œí™”ëœ vectorì´ë‹¤. Sequentialí•œ ê³„ì‚°ì„ ì¤„ì´ëŠ” ëª©í‘œëŠ” ê¸°ë³¸ì ì¸ building blockì—ì„œ CNNì„ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ì ìœ¼ë¡œ inputê³¼ outputì˜ hidden representationì„ ê³„ì‚°í•˜ëŠ” ByteNetì´ë‚˜ ConvS2Sì˜ ê¸°ë°˜ì„ ì´ë£¨ê³ ìˆë‹¤. ìœ„ì™€ ê°™ì€ modelì—ì„œëŠ” 2ê°œì˜ inputì´ë‚˜ outputì˜ ê¸¸ì´ê°€ ì¦ê°€í• ìˆ˜ë¡ ê³„ì‚°ëŸ‰ì´ ëŠ˜ì–´ë‚œë‹¤.(ByteNetì€ logì ìœ¼ë¡œ, ConvS2SëŠ” linearí•˜ê²Œ). ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ê±°ë¦¬ì— ë”°ë¥¸ dependenciesë“¤ì„ í•™ìŠµí•˜ê¸°ì— ë”ìš± ì–´ë µê²Œ ë§Œë“ ë‹¤. â€» (inputê³¼ outputì‚¬ì´ì˜ ê¸¸ì´ê°€ ê¸¸ì–´ì§€ë©´ ê³„ì‚°ëŸ‰ì´ ì¦ê°€í•´ ì„œë¡œì˜ ì—°ê´€ê´€ê³„ë¥¼ í•™ìŠµí•˜ê¸°ê°€ ì–´ë µë‹¤ëŠ” ëœ» cnnì€ í•œë²ˆì— kernel sizeë¥¼ ì§„ì§œ ì»¤ë´¤ì ìµœëŒ€ 7x7ì„ ì“°ê¸° ë•Œë¬¸ì— ë§Œì•½ inputì´ ì—„ì²­ ê¸¸ë‹¤ë©´ CNNì—°ì‚°ì‹œ ê³„ì‚°ëŸ‰ì´ ì¦ê°€í•˜ê²Œ ë˜ê³  ìœ„ì¹˜ì— ëŒ€í•œ ì •ë³´ì˜ ì¼ë¶€ë§Œì´ ë‹´ê¸°ê²Œë¨. ì˜ˆë¥¼ ë“¤ì–´ ê°„ë‹¨í•˜ê²Œ she is pretty and good at playing piano with her own pianoì™€ ê°™ì€ ë¬¸ì¥ì—ì„œ ë’¤ì— herê³¼ ì²˜ìŒ sheëŠ” ê¸´ ê±°ë¦¬ë¥¼ ê°€ì§€ê²Œ ë˜ì–´ì„œ ì´ ì •ë³´ë¥¼ ë‹´ê¸°ì— CNNì€ ë¶€ì ì ˆ?). Transformerì—ì„œëŠ” linearë‚˜ logmatricí•˜ê²Œ ê³„ì‚°ëŸ‰ì´ ì¦ê°€í•˜ì§€ ì•Šê³  constantí•œ numberë¡œ ì¦ê°€í•œë‹¤.Attention-weigheted positionì˜ í‰ê· ì„ ì‚¬ìš©í•˜ì—¬ Effectiveí•œ **í•´ìƒë„?**ê°€ ê°ì†Œí•¨ì—ë„Multi-Head Attentionê³¼ ìƒí˜¸ì‘ìš© í•¨ìœ¼ë¡œì„œ ê³„ì‚°ëŸ‰ì„ ì¤„ì˜€ë‹¤. Self-attentionì€ ì„œë¡œ ë‹¤ë¥¸ positionì— ìˆëŠ” sequenceë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´ ì„œë¡œë¥¼ relatingí•œë‹¤. Self-attentionì€ reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representationê³¼ ê°™ì€ ë¶„ì•¼ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ì‚¬ìš©ë˜ì–´ì ¸ ì™”ë‹¤. End-to-end memory networkì€ ìˆœì„œì— ë”°ë¼ ì •ë ¬ëœ recurrenceê°€ ì•„ë‹Œ recurrent attention mechanismì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³ ìˆê³ , ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤. Transformerì€ ì²˜ìŒìœ¼ë¡œ RNNì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì˜¤ë¡œì§€ self-attentionë§Œì´ ì“°ì¸ ì²«ë²ˆì§¸ ë³€ì—­ modelì´ë‹¤. ìš”ì•½ : Sequenceí•œ ë¬¸ì œì—ì„œì˜ ëª¨ë¸ â€‹ RNNì˜ ë‹¨ì  : ë³‘ë ¬í™”ì˜ ì–´ë ¤ì›€ìœ¼ë¡œ ì¸í•œ ê³„ì‚°ì˜ ë³µì¡ë„ ì¦ê°€, train ì‹œê°„ì˜ ì¦ê°€, â€‹ (ê·¸ë¦¬ê³  ê°•ì˜ì—ì„œ ë§Œì•½ sequentialí•œ ë°ì´í„°ì¤‘ ì¤‘ê°„ì— ì–´ëŠ í•˜ë‚˜ê°€ë¹ ì§„ë‹¤ë©´ í•´ê²°í•˜ê¸°ê°€ ì–´ë µë‹¤ê³  í–ˆë‹¤) â€‹ CNNì˜ ë‹¨ì  : ë³‘ë ¬ì ì¸ ê³„ì‚°ì€ ì´ë£¨ì–´ ì§€ì§€ë§Œ, inputì´ë‚˜ outputì˜ ê¸¸ì´ê°€ ì¦ê°€í• ìˆ˜ë¡ ê³„ì‚°ë„ ë§ê³  ë‹¨ì–´ê°„ì˜ ê´€ ê³„íŒŒì•…ì´ ë¹¡ì…ˆ â€‹ ë”°ë¼ì„œ Attentionë§Œ ì“´ Transformer ì§± 3. Model Atchitectureê°€ì¥ ê²½ìŸë ¥ì´ ì¢‹ì€ neural sequence transduction modelì€ encoder-decoder êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆë‹¤. Encoderì€ ì…ë ¥ sequenceë¥¼ x = (x1,â€¦.xn)ìœ¼ë¡œ í‘œí˜„í•˜ì˜€ê³  ì´ë¥¼ z = (z1,â€¦.zn)ìœ¼ë¡œ mapí•œë‹¤. ì£¼ì–´ì§„ zë¡œ decoderê°€ output sequenceì¸ (y1,â€¦,ym)ì„ ìƒì„±í•´ ë‚¸ë‹¤.(ë³´í†µ ì¤‘ê°„ì˜ latent ì¸µì€ inputê³¼ outputì— ë¹„í•´ ì‘ì€ dimensionì„ ê°€ì§„ë‹¤ê³  ì¡°êµë‹˜ê»˜ì„œ ì„¤ëª…) ê° stepë§ˆë‹¤ modelì€ auto-regressiveí•˜ë©°, ë¬¸ì¥ì„ ìƒì„±í• ë•Œ ì´ì „ì— ìƒì„±ëœ symbolì„ additional inputìœ¼ë¡œ ê°€ì •í•œë‹¤. â€» Auto regressive ë³µìŠµ (ê³ ì •ëœ ê¸¸ì´ì¸ $\\tau$ë§Œí¼ì˜ ì‹œí€€ìŠ¤ë§Œ í™œìš©í•˜ëŠ” ê²½ìš° Autoregressive Model(ìê¸°íšŒê·€ëª¨ë¸)ì´ë¼ê³  ë¶€ë¥¸ë‹¤ ì§ì „ê³¼ê±°ì˜ ì •ë³´ë‘ ì§ì „ì •ë³´ê°€ ì•„ë‹Œ ì •ë³´ë“¤ì„ Htë¡œ ë¬¶ì–´ì„œ í™œìš©) Transformerì€ stackedëœ self-attentionì„ ì‚¬ìš©í•˜ê³  ìˆê³ , encoderì™€ decoderë¶€ë¶„ì— ëª¨ë‘ fully connected layerë¥¼ ì‚½ì…í•˜ì˜€ë‹¤. 3.1 Encoder and Decoder StacksEncoder : encoderì€ N=6 (6ê°œ)ì¸ ê°ê°ì˜ identicalí•œ layerë“¤ì´ ì¸µì¸µì´ ìŒ“ì—¬ìˆë‹¤. ê°ê°ì˜ layerë“¤ì€ 2ê°œì˜ sub-layerë¡œ êµ¬ì„±ë˜ì–´ìˆë‹¤. ì²«ë²ˆì§¸ëŠ” Multi-Head Attentionì´ê³ , ë‘ë²ˆì§¸ëŠ” ê°„ë‹¨í•œ fully-connectedëœ feed-forward networkì´ë‹¤. ìš°ë¦¬ëŠ” ê°ê°ì˜ sublayerì— residual connectionì„ ë“¤ì–´ ì£¼ì—ˆë‹¤. â€»ì—¬ê¸°ì„œ ê³¼ì—° residual connectionì„ ë„£ì€ ì´ìœ ê°€ ë­˜ê¹Œ? overfitting ë°©ì§€ like ResNet?? ê°ê°ì˜ sub-layerì˜ outputì€ LayerNorm(x + Sublayer(x)). ëª¨ë“  sublayer modelê³¼ embedding layerì˜ outputì˜ ì°¨ì›ì€ dmodel = 512 ì´ë‹¤. Decoder : Decoderë˜í•œ N=6ì¸ ê°ê°ì˜ identicalí•œ layerë“¤ì´ ì¸µì¸µì´ ìŒ“ì—¬ìˆë‹¤. Encoderì˜ 2ê°œì˜ ê° sub-layerì— Decoderì€ encoder stackì˜ outputì— ëŒ€í•œ multi-Head attentionì„ ìˆ˜í–‰í•˜ëŠ” ì¸µì´ ì¶”ê°€ê°€ ë˜ì—ˆë‹¤. Encoderì™€ ê°™ì´ sublayerì— residual connectionì„ ë§Œë“¤ì–´ ì£¼ì—ˆë‹¤. í•˜ìœ„ positionì´ attendí•˜ëŠ”ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ self-attention-layerì„ ì•½ê°„ ìˆ˜ì •í•˜ì˜€ë‹¤**(ì´ê²Œ masking). ì´ maskingì€ iìœ„ì¹˜ì˜ ì˜ˆì¸¡ì´ ië³´ë‹¤ ê³¼ê±°ì˜ ê²ƒìœ¼ë¡œë§Œ êµ¬í•´ì§€ê²Œ í•˜ê¸° ìœ„í•¨ì´ë‹¤.** â€» Maskingì€ NLP ë¬¸ì œì—ì„œ êµ‰ì¥íˆ ë§ì´ ì“°ì¸ë‹¤ê³  í•œë‹¤. ì•Œì•„ë‘ì 3.2 AttentionAttention functionì€ queryì™€ set of key-value pairë“¤ì„ outputì— mappingí•˜ëŠ” í•¨ìˆ˜ì´ë‹¤. (query,key,value,outputì€ ëª¨ë‘ vector). Outputì€ valueë“¤ì˜ weighted sumìœ¼ë¡œ ê³„ì‚°í•˜ë©°, ì´ weightëŠ” queryì™€ ë‹¤ë¥¸ëª¨ë“  keyê°’ë“¤ì˜ compatibility functionìœ¼ë¡œ ì •í•´ì§„ë‹¤. â€»ì—¬ê¸°ì„œ compatibility functionì´ë€? ë’¤ì—ì„œ sumì˜ í˜•íƒœì™€ dot productë¡œ ë‚˜ëˆ„ì–´ ì§„ë‹¤. ì´ë“¤ì˜ ì°¨ì´ì ì€ ë’¤ì— ê¸°ìˆ  3.2.1 Scaled Dot-Product Attentionìš°ë¦¬ëŠ” ì´ëŸ¬í•œ attentionì„ â€œScaled Dot-Product Attentionâ€ì´ë¼ê³  ë¶€ë¥¸ë‹¤. Inputì€ dkì˜ ì°¨ì›ì„ ê°€ì§€ëŠ” keysì™€ queries(ë‘˜ì€ ì—°ì‚°(ë‚´ì )ì„ ìœ„í•´ ê°™ì€ ì°¨ì›ì„ ê°€ì§„ë‹¤)ì™€ dvì˜ ì°¨ì›ì„ ê°€ì§€ëŠ” valuesë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤. ìš°ë¦¬ëŠ” í•˜ë‚˜ì˜ query ë¥¼ ë‹¤ì€ ëª¨ë“  keysë“¤ê³¼ ë‚´ì í•˜ê³ (ì´ ê²°ê³¼ ê°’ì´ ë°”ë¡œ ê°•ì˜ì—ì„œ score) sqrt(dk)ë¡œ ë‚˜ëˆ„ì–´ ì¤€ë‹¤. ì´í›„ softmax functionì„ ì ìš©í•˜ì—¬ valueì˜ weightë¥¼ ì–»ì–´ë‚¸ë‹¤. â€» ië²ˆì§¸ ë‹¨ì–´ì— ëŒ€í•œ score vector ê³„ì‚°ì‹œ iì˜ ì¿¼ë¦¬ vectorì™€ ë‹¤ë¥¸ëª¨ë“  key vectors ì‚¬ì´ì˜ ë‚´ì  (Matmul) ìœ„ì˜ ê³¼ì •ë“¤ì„ queriesë“¤ì„ Q matrix, keys and valuesë¥¼ ê°ê° K and Vë¼ê³  í•œë‹¤ë©´ ì•„ë˜ì˜ ì‹ìœ¼ë¡œ í‘œí˜„ê°€ëŠ¥ ê°€ì¥ ë§ì´ì“°ëŠ” attention functionì˜ í•¨ìˆ˜ëŠ” additive attentionê³¼ ìœ„ì™€ ê°™ì€ dot-product attentionì´ë‹¤. ìš°ë¦¬ëŠ” dot-product attentionì„ ì¼ë‹¤. Additive attentionì€ í•˜ë‚˜ì˜ hidden layerì™€ feed forward networkë¥¼ ì‚¬ìš©í•˜ì—¬compatibility functionì„ ê³„ì‚°í•œë‹¤. ì´ë‘ê°€ì§€ëŠ” ë³µì¡ë„ ì¸¡ë©´ì—ì„œ ë¹„ìŠ·í•˜ì§€ë§Œ, dot-product attentionì´ ë”ë¹ ë¥´ê³  ê³µê°„ ì ˆì•½ì ì´ë‹¤.(ì´ìœ ëŠ” í–‰ë ¬ì˜ ê³„ì‚°ìœ¼ë¡œ í‘œí˜„ê°€ëŠ¥) ì‘ì€ ê°’ì„ ê°€ì§€ëŠ” dkì—ì„œì˜ ë‘ mechanismì€ ìœ ì‚¬í•˜ê² ì§€ë§Œ, í° dkë¡œ ë‚˜ëˆ„ì–´ ì£¼ì§€ ì•Šìœ¼ë©´ additive attentionì´ dot-productì˜ ì„±ëŠ¥ì„ ë„˜ëŠ”ë‹¤. í° dkëŠ” dot -productëŠ” í°ê°’ì„ ê°€ì§€ê²Œ ë˜ê³ , ì´ëŠ” softmax functionì´ ë§¤ìš° ì‘ì€ gradientë¥¼ ê°€ì§€ê²Œ í•œë‹¤. ì´ëŸ¬í•œ ì˜í–¥ì„ ì¤„ì´ê¸° ìœ„í•´ sqrt(dk)ë¡œ ë‚˜ëˆ„ì–´ ì£¼ì—ˆë‹¤. (scale) 3.2.2 Multi-Head Attentiondmodel(max sequence)ì˜ ì°¨ì›ì„ ê°€ì§€ëŠ” keys,values,queriesìœ¼ë¡œ ì´ë£¨ì–´ì§„ single attentionì„ ìˆ˜í–‰í•˜ëŠ”ê²ƒì´ ì•„ë‹ˆë¼, ìš°ë¦¬ëŠ” queries, keys, valuesë“¤ì— ê°ê° hë²ˆ dk,dk,dvë¥¼ ê³±í•˜ì—¬ projectí•œê°’ì´ ë”ìš± ì¢‹ì€ê²ƒì„ ì•Œì•„ë‚´ì—ˆë‹¤. ì´ëŸ¬í•œ projectionì„ ê±°ì¹˜ë©´ attention functionì„ ë³‘ë ¬ì ìœ¼ë¡œ ìˆ˜í–‰ í•  ìˆ˜ìˆìœ¼ë©°, dvì˜ ì°¨ì›ì„ ê°€ì§€ëŠ” outputì„ ì–»ì–´ë‚¼ ìˆ˜ ìˆë‹¤. ì´ outputì€ ë‹¤ì‹œ í•˜ë‚˜ë¡œ concatnatedë˜ì–´ projectedëœë‹¤. Multi-Head Attentionì€ ì„œë¡œë‹¤ë¥¸ ìœ„ì¹˜ì—ì„œì˜ ì„œë¡œë‹¤ë¥¸ subspaceì˜ í‘œí˜„ì„ jointly attend í•˜ê²Œ í•œë‹¤. ìš°ë¦¬ëŠ” h = 8ê°œì˜ parallel attention layerì„ ì‚¬ìš©í•˜ì˜€ê³ ,dk,dv,dmodel/h = 64 ê°ê°ì˜ headì—ì„œ ì°¨ì›ì„ ì¤„ì„ìœ¼ë¡œì„œ ì „ì²´ì ì¸ ê³„ì‹¼ë¹„ìš©ì´ single head attentionê³¼ ë¹„ìŠ·í•˜ê²Œ ë§Œë“¤ì—ˆë‹¤??????? â€» í•œë§ˆë””ë¡œ ì´ì œ dmodelì˜ ì°¨ì›ì„ hë§Œí¼ parrel layerì— ë‚˜ëˆ„ì–´ì„œ ë„£ì—ˆìœ¼ë‹ˆ ê²°êµ­ single head attentionê³¼ ë¹„ìŠ·í•˜ë‹¤ëŠ” ì´ì•¼ê¸°ì¸ê°€?? 3.2.3 Applications of Attention in our ModelTransformerì€ multi-head attentionì„ 3ê°€ì§€ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•˜ê³  ìˆë‹¤ â€œEncoder - Decoder attentionâ€ layerì—ì„œ queriesëŠ” ì´ì „ì˜ decoder layerì—ì„œ ì˜¤ê³ , encoderì˜ outputì—ì„œ ì˜¤ëŠ” keyì™€ valueë“¤ì„ ì €ì¥í•œë‹¤. ì´ê²ƒì€ input sequenceì˜ ëª¨ë“  positionë“¤ì„ ëª¨ë“  positionì˜ decoderê°€ attend í•˜ê²Œ í•´ì¤€ë‹¤. í•œë§ˆë””ë¡œ decoderì„ ì¿¼ë¦¬ë§Œ ë“¤ê³ ìˆì–´ë„ ëœë‹¤. Self attention layerë¥¼ í¬í•¨í•˜ëŠ” encoder. Self attention layerì—ì„œëŠ” ì´ì „ encoderì˜ layerì˜ ê²°ê³¼ì—ì„œë¶€í„° ë‚˜ì˜¨ ìœ„ì¹˜ì™€ ê°™ì€ ìœ„ì¹˜ì—ì„œ ëª¨ë“  key, values, and queriesê°€ ë‚˜ì˜¨ë‹¤. encoderì†ì˜ ê°ê°ì˜ ìœ„ì¹˜ë“¤ì€ ì´ì „ encoderì˜ ì´ì „ layerì˜ ëª¨ë“  ìœ„ì¹˜ì— ì§‘ì¤‘í•œë‹¤. (ëª¨ë“  í˜„ì¬ layerì˜ ìœ„ì¹˜ê°€ ì´ì „ layerì˜ ëª¨ë“  position ì •ë³´ë“¤ì„ ê°€ì§„ë‹¤? ì´ëŸ°ëŠë‚Œ?) ë¹„ìŠ·í•˜ê²Œ decoderì˜ self attention layerë˜í•œ ëª¨ë“  decoderì•ˆì˜ ëª¨ë“ (ìê¸°ìì‹ ê¹Œì§€) positionì— ì§‘ì¤‘í•œë‹¤.Auto regressive íŠ¹ì„±ì„ ë³´ì¡´ì‹œí‚¤ê¸° ìœ„í•´ ì™¼ìª½ì˜ ì •ë³´ë“¤ì´ decoderë¡œ flow in í•˜ëŠ”ê±¸ ë§‰ì•„ì£¼ì–´ì•¼ í•œë‹¤. ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ê±¸ scaled dot product attentionì•ˆì˜ softmaxì˜ output ê°’ì—masking outí•¨ìœ¼ë¡œì„œ í•´ê²°í•œë‹¤ ì´ê±¸ ë‹¤ì‹œí•œë²ˆ ìƒê°í•´ ë³´ì•„ì•¼ ê² ë‹¤ Auto regressiveí•œ íŠ¹ì„±ì´ë€ ì´ì „ì˜ ì •ë³´ë“¤ ë§Œìœ¼ë¡œ í˜„ì¬ê°’ì„ ë„ì¶œí•´ë‚´ëŠ” íŠ¹ì„± ê·¸ë‹ˆê¹Œ ê²°êµ­ì€ ìœ„ì—(1)ì‹ì—ì„œ softmaxì˜ ê²°ê³¼ê°’ì— ë¯¸ë˜ì˜ ì •ë³´ë“¤ì€ ëª¨ë‘ masking í•´ì¤€ë‹¤ëŠ”ê²ƒì´ë‹¤. ê·¸ë‹ˆê¹Œ ê²°êµ­ì—” ì´ì‹ì—ì„œ ë¯¸ë˜ì˜ ì •ë³´ë“¤ê¹Œì§€ Kì™€ Qì˜ ë‚´ì ê²°ê³¼ê°€ ë‹¤ë‹´ê³  ìˆìœ¼ë‹ˆê¹Œ ë¯¸ë˜ì˜ ì •ë³´ëŠ” ë§ˆìŠ¤í‚¹ í•´ì¤€ë‹¤. 3.3 Position-wise Feed Forward Networksencoderì™€ decoderì•ˆì˜ ê° layerì—ëŠ” fully connected feed forward networkë¥¼ ê°€ì ¸ì•¼ í•œë‹¤. ì´ fully connected feed forward networkëŠ” ê°ê°ì˜ ìœ„ì¹˜ì— ë…ë¦½ì ìœ¼ë¡œ ë”°ë¡œ ì ìš©ëœë‹¤ ìœ„ì‹ì„ ë³´ë©´ 2ê°œì˜ linear transformationê³¼ ReLU activationì„ ê·¸ì‚¬ì´ì— ì‚¬ìš©í•˜ì˜€ë‹¤. Linear transformationì„ ê°ê°ì˜ positionì— ê°™ì€ ê±¸ ì ìš©í•´ì•¼ í•œë‹¤. ê·¸ë¦¬ê³  layerê³¼ layerì‚¬ì´ì—ëŠ” ë‹¤ë¥¸ parameterë¥¼ ì ìš©í•´ì•¼ í•œë‹¤. ë˜ë‹¤ë¥¸ ë°©ë²•ì€ 1ì˜ kernel sizeë¥¼ ê°€ì§€ëŠ” 2ê°œì˜ convoltionì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. â€» ê·¸ë‹ˆê¹Œ í•˜ë‚˜ì˜ layerì—ëŠ” ê°™ì€ weightë¥¼ ì ìš©í•˜ê³  ë‹¤ë¥¸ layerì‚¬ì´ì—ëŠ” ë‹¤ë¥¸ weightë¥¼ ì ìš©í•œë‹¤ëŠ” ëœ»??? dmodel = 512 inner-layerâ€™s dimension dff = 2048 3.4 Embedding and Softmaxì—¬íƒ€ ë‹¤ë¥¸ ë²ˆì—­ ëª¨ë¸ê³¼ ê°™ì´ ì—¬ê¸°ì„œë„ í•™ìŠµëœ embaddingì„ ì‚¬ìš©í–ˆë‹¤. Decodeë˜ì–´ ë‚˜ì˜¨ê²°ê³¼ë„ í•™ìŠµëœ linear transformationê³¼ softmaxí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ tokenì˜ í™•ë¥ ì„ ê³„ì‚°í•˜ì˜€ë‹¤. embedding layerë“¤ì—ëŠ” ê°™ì€ weightì™€ presoftmax linear transformationì„ ì‚¬ìš©, weightì˜ ê²°ê³¼ì— sqrt(dmodel)ì„ ì‚¬ìš©í–ˆë‹¤. 3.5 Positional Encodingìš°ë¦¬ì˜ modelì€ recurrenceë„ ì—†ê³  convolutionë„ ì—†ì–´ì„œ ê°ê°ì˜ modelì´ sequenceì˜ ìˆœì„œë¥¼ ì‚¬ìš©í•˜ê²Œ í•˜ì—¬ë©´ position ì •ë³´ë¥¼ ì‚½ì…í•´ ì£¼ì–´ì•¼ í•œë‹¤. ë”°ë¼ì„œ ì´ì™€ ê°™ì´ ê°ê° embeddingëœ vectorì— positional encodingëœ vectorë¥¼ ë”í•´ì¤€ë‹¤ ì´ ì—°êµ¬ì—ì„œëŠ” cosê³¼ sin í•¨ìˆ˜ë¥¼ ì‚¬ìš©í–ˆë‹¤ pos ëŠ” position iëŠ” ì°¨ì› ê°ê°ì˜ ìœ„ì¹˜ê°€ sinusodialí•˜ê²Œ encoding ë˜ë„í˜¹ í•˜ì˜€ë‹¤. ì£¼ê¸°ê°€ ì¡°ì˜¬ë¼ ê¸¸ì–´ì„œ ë‹¤ë¥¸ìœ„ì¹œë° ì£¼ê¸°ì„± ë•Œë¬¸ì— ê°™ì€ ê°’ì„ ê°€ì§€ëŠ” ê²½ìš°ëŠ” ë“œë¬¼ë‹¤ 4. Why Self-Attentionì´ ë¶€ë¶„ì—ì„œëŠ” self attention layerë¥¼ RNNê³¼ CNNì— ë”ìš± ìì„¸íˆ ë¹„êµí•œë‹¤. ì•ì—ì„œ ì„¤ëª…í•œê±°ì— ëŒ€í•œ ë³´ì¶©ì„¤ëª… í•œ layerì—ì„œ ê³„ì‚° ë³µì¡ë„ì—ì„œì˜ ì´ë“ ë³‘ë ¬í™” ë  ìˆ˜ ìˆëŠ”ê³„ì‚°ì˜ ì´ëŸ‰ ê¸¸ì´ê°€ ê¸°ì´ì´ì¼ì–´ì¡Œì„ë•Œ ì–¼ë§ˆë‚˜ networkì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ê¸¸ì´ê°€ ì¡¸ë¼ë¦¬ ê¸¸ì–´ì¡Œì„ë•Œ dependenciesëŠ” ë§¤ìš° ì¤‘ìš”í•˜ë‹¤. ì´ì— ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ë¯¸ì¹˜ëŠ” ì˜í–¥ì¤‘ í•˜ë‚˜ê°€ signalí•˜ë‚˜ê°€ networkë¥¼ ìˆœíšŒí•˜ëŠ” ê¸¸ì´ì´ë‹¤?? ì´ê²Œ ì§§ì•„ì§ˆìˆ˜ë¡ ê¸´ ê¸¸ì´ì— ëŒ€í•œ ìƒí˜¸ì ì¸ ê´€ê³„ê°€ ë” ì˜ í•™ìŠµëœë‹¤. ë”°ë¼ì„œ ê° modelë§ˆë‹¤ modelì—ì„œ inputê³¼ output ì‚¬ì´ì˜ ê±°ë¦¬? ë­í•˜ì´íŠ¼ ê·¸ ì–¼ë§ˆë‚˜ modelì´ compactí•œê°€ ìœ„ tableì„ ë³´ë©´ computationalí•œ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ self attention ëª¨ë¸ì€ í•´ë‹¹í•˜ëŠ” output ìœ„ì¹˜ì˜ ì˜¤ì§ size r ë§Œí¼ì˜ input sequence ì£¼ìœ„ë¥¼ ê³ ë ¤í•˜ë„ë¡ ì œí•œë˜ì–´ìˆë‹¤. ì´ê±°ì— ëŒ€í•œ ì—°êµ¬ëŠ” ì¶”í›„ì— ë°œí‘œí•˜ê² ë‹¤ê³  ì í˜€ìˆë‹¤. ê·¸ëŸ¼ ì´ë¯¸ ë‚˜ì™€ìˆê² ì§€? í•˜ì´íŠ¼ ìœ„ì—êº¼ ë¹„êµí•´ë³´ë©´ ëª¨ë“  ì¸¡ë©´ì—ì„œ self attentionì´ ì™€ë”° 5. Training1. Training data and batchingWMT 2014ë¥¼ ì‚¬ìš©í•˜ì—¬ trainí•¨ ê·¸ë¦¬ê³  ë¬¸ì¥ì€ target vocabì™€ 37000ê°œë¥¼ ê³µìœ í•˜ëŠ” byte-pair encodingì´ë¼ëŠ” ë°©ì‹ì„ ì‚¬ìš©í–ˆìŒ ê° traning batchëŠ” 25000ê°œì˜ source tokenê³¼ 25000ê°œì˜ target tokenì„ í¬í•¨í•˜ëŠ” ë¬¸ì¥ì˜ setìœ¼ë¡œ ì •í•´ì£¼ì—ˆë‹¤. 2. OptimizerAdamì„ ì¼ê³ , lrì„ ë‹¨ê³„ì ìœ¼ë¡œ ë³€í™”ì‹œì¼°ë‹¤.ì•„ë˜ì™€ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ 3. Regularization Residual Dropout ë”í•´ì§€ê³  normalized ë˜ê¸°ì „ì— ê°ê°ì˜ sublayerì˜ outputì— dropoutì„ ì ìš©í•˜ì˜€ë‹¤ ê·¸ë¦¬ê³  ë˜ embeddingëœ vectorì™€ positionì˜ í•©ì´í›„ì—ë„ ì ìš©í•˜ì˜€ë‹¤ Pdropì€ 0.1 Label Smoothing??? ì´ëŸ°ê±¸ ì ìš©í–ˆë‹¤ê³  í•˜ëŠë„¤ ì´ê²Œ ì•½ê°„ ì˜ˆì¸¡ë¶ˆê°€ëŠ¥í•œê±¸ ë”í•´ì¤˜ì„œ modelì´ ë” ìƒˆë¡œìš´ê²ƒì„ ë°°ìš°ê²Œë”í•˜ëŠ” ê±°ë¼í•˜ëŠ”ë° ê± ê°„ë‹¨í•˜ê²Œ ë‚˜ì™€ìˆë‹¤ 6. ResultsBLEU score ì˜ë‚˜ì™”ë‹¤ ì–´ì©Œêµ¬ ì €ì©Œêµ¬ í•˜ë‹¤ê°€ base modelì—ì„œëŠ” 5ê°œì˜ checkpointë¥¼ ë§Œë“¤ì–´ì„œ ê·¸ê²ƒì˜ í‰ê· ì„ ë‚¸ í•˜ë‚˜ì˜ modelì„ ì¼ê³ , ê° check pointëŠ” 10ë¶„ë§ˆë‹¤ í•œë²ˆì”© intervalì„ ì£¼ì—ˆë‹¤. ì´ê²Œ ë‚´ê°€ ì¡°êµë‹˜í•œí…Œ ì§ˆë¬¸í–ˆë˜ ë¶€ë¶„ê³¼ ì¢€ ì—°ê´€ì„±ì´ ìˆë‹¤. ì´ë ‡ê²Œ ì¤‘ê°„ì¤‘ê°„ì— modelì„ ê¸°ë¡í•˜ê³  í‰ê· ì„ ë‚´ëŠ” ë°©ì‹ë„ ìˆêµ¬ë‚˜ ê·¸ë¦¬ê³  ì´ beam searchë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤ê³  í•œë‹¤ ì´ê±´ ì´ì „ì˜ ë…¼ë¬¸ë“¤ì„ ì½ì„ë•Œë„ ìì£¼ ì‚¬ìš©í–ˆë˜ ê¸°ë²•ì´ë‹¤ ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•˜ë©´ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ Kê°œì„ ì„ íƒí•˜ë©° ì§„í–‰í•˜ëŠ” ê²ƒì´ë‹¤ greedyë°©ë²•ë³´ë‹¤ íš¨ìœ¨ì ì´ê³  scoreê°€ ì˜ë‚˜ì˜¨ë‹¤ê³  ë“¤ì—ˆë‹¤ 6.2 Model Variationsì•„ë˜ tableì˜ (A)ë¥¼ ë³´ë©´ attentionì˜ headì˜ ê°œìˆ˜ì™€ key,valueì˜ dimensionì„ ë³€í™”ì‹œì¼œì£¼ì—ˆë‹¤. ë„ˆë¬´ ë§ì€ headë¥¼ ì‚¬ìš©í•´ë„ ì•ˆë˜ê³  í•˜ë‚˜ë§Œ ì‚¬ìš©í•´ë„ ì•ˆë¨ ì´ê±´ ë„ˆë¬´ ë‹¹ì—°í•œê±°ë‹¤ ë­ë“ ì§€ ì ë‹¹í•œê²Œ ì¢‹ë‹¤ 6.3 English Constituency Parsingì´ Transformerë¥¼ í™œìš©í•œ modelì€ í†µì—­ì—ì„œ ë‚˜ì•„ê°€ì„œ ì˜ì–´ êµ¬ë¬¸ì„ ë¶„ì„í•´ì£¼ëŠ” ë°©ë²•ìœ¼ë¡œ ë°œì „ì‹œì¼œë‚˜ê°€ì•¼ í•œë‹¤. ì´ê±´ ë³„ë¡œ ì¤‘ìš”í•˜ì§€ ì•Šì€ê²ƒ ê°™ë‹¤ í•´ë³´ë‹ˆê¹Œ RNNë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚´ì—ˆë‹¤ ë 7. Conclusionê²°ë¡  ê¸°ì¡´ê³¼ ë‹¤ë¥´ê²Œ attentionì—ë§Œ ê¸°ë°˜ì„ ë‘” multi-headed self attentionì„ ì‚¬ìš©í•œ ì´ transformerì€ ë‹¤ë¥¸ RNNì´ë‚˜ CNNë³´ë‹¤ ì„±ëŠ¥ì´ ë¹ ë¥´ë©° ì´ Transfomerë¥¼ ë”ìš±í° inputê³¼ outputì„ ê°€ì§€ëŠ” imageë‚˜ ë¹„ë””ì˜¤ ì˜¤ë””ì˜¤ ë“±ì—ì ìš©ì‹œí‚¤ëŠ” ê²ƒì„ ê¸°ëŒ€í•˜ê³  ìˆë‹¤. Transformer ì§±ì§±ë§¨ ê·¸ë¦¬ê³  ì¡°êµë‹˜ê»˜ì„œ ê´€ì‹¬ ìˆìœ¼ì‹  Neuroscienceì™€ Attentionì‚¬ì´ì˜ ê´€ë ¨ ë³´ë©´ ìš°ë¦¬ ì¸ê°„ì˜ í™©ë°˜ì—ì„œë„ ì´ attentionì˜ ê°œë…ì„ ì ìš©í•´ì„œ ì‚¬ë¬¼ì„ ì¸ì§€í•˜ê³  ìˆìœ¼ë‹ˆ, ì˜ë˜ëŠ”ê²Œ ì–´ì°Œë³´ë©´ ë‹¹ì—°í•˜ë‹¤ ì¶œì²˜ : https://arxiv.org/pdf/1706.03762.pdf ê·¸ë¦¬ê³  naver boostcamp","link":"/2021/02/05/2021-02-05-Attention/"},{"title":"Image Classification","text":"K Nearest Neighbors (k-NN) ê¸°ì¡´ì˜ dataê°€ ê°€ì§€ê³ ìˆëŠ” labelì„ í™œìš©í•´ì„œ ìƒˆë¡œìš´ dataì˜ labelì„ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œê°€ ëœë‹¤. ì´ë ‡ê²Œ ëœë‹¤ë©´ ë¯¸ë¦¬ ìœ ì‚¬ë„ë¥¼ ì •ì˜í•´ì•¼ í•œë‹¤. ê·¸ë¦¬ê³  system ë³µì¡ë„ê°€ ë„ˆë¬´ ë†’ë‹¤. ë”°ë¼ì„œ dataë¥¼ NNì˜ parameterì— ë…¹ì—¬ë„£ëŠ” ê²ƒì´ë‹¤. Yann Lecunì˜ CNN ê°œë°œ : ìš°í¸ë²ˆí˜¸ì¸ì‹ì— í˜ì‹ ì„ ì´ë£¨ì–´ëƒ„ Using better activation function annotation dataì˜ íš¨ìœ¨ì ì¸ í•™ìŠµ ê¸°ë²• data ë¶€ì¡±ë¬¸ì œì˜ ì™„í™” : ëŒ€í‘œì ì¸ ë°©ë²•ë“¤ Data augmentation Leveraging pre-trained information Leveraging unlabeled dataset for training Data augmentation Dataë¥¼ í†µí•œ patternì˜ ë¶„ì„ Dataset is almost biased != real dataê²°êµ­ ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” dataë“¤ì€ ì‚¬ëŒì´ biasí•´ì„œ ì°ì€ ì‚¬ì§„ë“¤ì´ ëŒ€ë¶€ë¶„ì´ê¸° ë•Œë¬¸ì— ìš°ë¦¬ê°€ ì–»ì–´ë†“ì€ training dataê¹Œì§€ ëª¨ë‘ í‘œí˜„í•˜ì§€ ëª»í•˜ëŠ” dataë“¤ì´ë‹¤. ex) crop, rotate, Brightness, â€¦ Affine transformation ë³€í™˜ì „í›„ì— ì„ ìœ¼ë¡œ ìœ ì§€ê°€ ë˜ê³ , ê¸¸ì´ì˜ ë¹„ìœ¨ê³¼ í‰í–‰ê´€ê³„ê°€ ìœ ì§€ê°€ ë˜ì§€ë§Œ ê°ë„ê°€ ë‹¬ë¼ì§€ëŠ”. ê¸°ë³¸ì ì¸ í‹€ì„ ë§ì¶˜ attine transofrmation mixing both images and labels RandAugment randomí•˜ê²Œ augmentation ë°©ë²•ì„ ìˆ˜í–‰í›„ ì˜ë‚˜ì˜¨ê²ƒì„ ê°€ì ¸ë‹¤ ì“°ì. ì–´ë–¤ê±¸ ì ìš©í• ê¹Œ, ì–´ë–¤ ê°•ë„ë¡œ augmentationì„ í• ê¹Œ? ì´ê±¸ policyë¦¬ê³  í•œë‹¤. Random samplingì‹œ datasetì„ ë§Œë“¤ì–´ì•¼ í•˜ëŠ”ë° ì´ëŸ¬í•œ dataë¥¼ ëª¨ì„ë•Œ labelì´ í•„ìš”í•˜ê¸° ë–„ë¬¸ì— ì´ëŸ¬í•œ dataë¥¼ ë‹¨ê¸°ê°„ì— ìˆ˜ì§‘í•˜ê¸°ê°€ ì‰½ì§€ê°€ ì•Šë‹¤. Transfer learning ê¸°ì¡´ì— í•™ìŠµì‹œí‚¨ modelì— ì¡°ê¸ˆ ë°”ê¿”ì„œ ì ìš©. í•œë°ì´í„°setì—ì„œ ë°°ìš´ ì§€ì‹ì„ ë‹¤ë¥¸ taskì— ì ìš© í•œ datasetì— ì ìš©ëœ ê²½ìš°ì— ë‹¤ë¥¸ê³³ì—ë„ ì ìš©í•  ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?Freeze ê¸°ì¡´ì˜ CNN layerâ€™s parameterì ì€ dataë¡œ ë¶€í„° Pseudo-labelingì´ ì¢€ ì‹ ê¸°í•˜ë‹¤. Knowledge distillation ë” ê¹Šì€ network -&gt; ë” ë†’ì€ ì„±ëŠ¥ ê¹Šê²Œ ìŒ“ì„ìˆ˜ë¡ gradient explosionì´ë‚˜ vanishing gradientê°€ ë°œìƒí•˜ì˜€ë‹¤, ê³„ì‚°ë³µì¡ë„ê°€ ì˜¬ë¼ê°€ì„œ ì†ë„ì˜ ì €í•˜, overfittingë¬¸ì œê°€ ì•„ë‹ˆë¼ degradation problemì´ë¼ëŠ”ê²Œ ë°í˜€ì¡Œë‹¤. ë„¤íŠ¸ì›Œí¬ë¥¼ ê¹Šê²Œ ìŒ“ê¸°ìœ„í•œ network GoogLeNet í•˜ë‚˜ì˜ layerì—ì„œ ë‹¤ì–‘í•œ í¬ê¸°ì˜ cnn filterë¥¼ ì‚¬ìš©í•˜ì„œ ì—¬ëŸ¬ì¸¡ë©´ìœ¼ë¡œ imageë¥¼ ê´€ì°°í•˜ê² ë‹¤. í•œì¸µì— ì´ë ‡ê²Œ ì—¬ëŸ¬ filterë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ê³„ì‚°ë³µì¡ë„ê°€ ì˜¬ë¼ê°€ê³ , parameterìˆ«ìê°€ ëŠ˜ì–´ë‚˜ê¸° ë•Œë¬¸ì—, 1x1 filterë¥¼ ì¶”ê°€í•´ ì£¼ì—ˆë‹¤. 1x1 layer as bottle neck architecture ê³µê°„í¬ê¸°ëŠ” ë³€í•˜ì§€ ì•Šê³ , channel ìˆ˜ë§Œ ë³€í™”ì‹œì¼œì¤€ë‹¤. Overall architecture inception moduleì„ ê¹Šê²Œ ìŒ“ì•„ì„œ ì „ì²´ network í˜•ì„± Auxiliary classifiers : gradient vanising ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì¶”ê°€í•´ì¤€ classifiers. ì¤‘ê°„ì¤‘ê°„ì— gradientë¥¼ ê¼½ìì£¼ëŠ” ì—­í• ì„ í•œë‹¤. lossê°€ ì¤‘ê°„ì—ì„œ ë¶€í„° í˜ëŸ¬ë“¤ì–´ê°€ê¸° ë•Œë¬¸ì— ë©€ë¦¬ìˆëŠ” ë‹¨ê¹Œì§€ gradient ì „ë‹¬ì´ ê°€ëŠ¥í•˜ë‹¤. Auxiliary classifier ResNetì•„ì§ë„ í° ì˜í–¥ë ¥ì„ ë°œíœ˜í•˜ê³  ìˆëŠ” networkì´ë‹¤. ìµœì´ˆë¡œ 100ê°œ ì´ìƒì˜ layerë¥¼ ìŒ“ì•˜ë‹¤. ìµœì´ˆë¡œ ì¸ê°„ levelì˜ ì„±ëŠ¥ì„ ë›°ì–´ë„˜ì—ˆë‹¤. ì´ëŸ¬í•œ ì„±ê³¼ë¡œ cvpr best paperë¥¼ ë°›ì•˜ë‹¤. ê¸°ì¡´ì—°êµ¬ìë“¤ì˜ layerë¥¼ ê¹Šê²Œ ìŒ“ëŠ”ë° ë¬¸ì œì  ì›ë˜ëŠ” model parameterê°€ ë§ìœ¼ë©´ errorê°€ ì¤„ì–´ë“¤ ê²ƒì´ë¼ê³  ìƒê°í–ˆëŠ”ë°, 56 layerì˜ errorê°€ ë” í¬ë‹¤ëŠ” ê²°ê³¼ê°€ ë‚˜ì™”ê¸° ë•Œë¬¸ì—, over fittingë•Œë¬¸ì´ ì•„ë‹ˆë¼ëŠ” ê²°ë¡ ì´ ë‚˜ì˜´. ëŒ€ì‹ ì— ìµœì í™” ë¬¸ì œì— ëŒ€í•´ì„œ 56 layerì´ ìµœì í™” ë˜ì§€ ì•Šì€ ê²°ê³¼ì´ë‹¤. Ã ì´ë ‡ê²Œ ë§Œë“¤ì–´ ë²„ë¦¬ë©´ í•™ìŠµì˜ ë¶€ë‹´ê°ì´ ëœì–´ì§€ê³  ë¶„í• ì •ë³µì´ ê°€ëŠ¥í•œ ë¬¸ì œê°€ ë˜ì§€ ì•Šì•˜ëŠ”ê°€? ì´ë¥¼ í•´ê²°í•´ ì£¼ê¸° ìœ„í•´ã… shortcut connectionì„ í†µí•´ back propê³¼ì •ì—ì„œ ê¸¸ì´ í•˜ë‚˜ê°€ ë”ìƒê¸°ëŠ” ê²ƒì´ë‹¤.gradient. vanishing ë¬¸ì œê°€ í•´ê²°ì´ ë˜ì—ˆë‹¤. ì™œì„±ëŠ¥ì´ ì˜ë‚˜ì˜¬ê¹Œ? residual connectionì„ í•˜ë‚˜ ì¶”ê°€í• ë•Œë§ˆë‹¤ 2ë°°ì”© pathê°€ ëŠ˜ì–´ë‚œë‹¤. ë‹¤ì–‘í•œ ê²½ë¡œë¥¼ í†µí•´ì„œ êµ‰ì¥íˆ ë³µì¡í•œ mappingì˜ í•™ìŠµì´ ê°€ëŠ¥í–ˆë‹¤. initializationìœ¼ë¡œ He initializationì„ ì‚¬ìš©í–ˆë‹¤. Reason ? -&gt; initializeë¥¼ ì‘ê²Œ í•´ì£¼ì–´ì•¼ ì´í›„ì— ë”í•´ì¤„ë•Œ ê· í˜•ì´ ë§ëŠ”ë‹¤. 3x3 conv layerë¡œ ëª¨ë‘ ì´ë£¨ì–´ì ¸ ìˆë‹¤. Only a single FC layer at final output DenseNetchannel ì¶•ìœ¼ë¡œ concatnateí•œë‹¤. í›¨ì”¬ì´ì „ì˜ layerì— ëŒ€í•œ ì •ë³´ë“¤ë„ ëª¨ë‘ ì´ì–´ì¤€ë‹¤. ìƒìœ„ layerì—ì„œë„ ëª¨ë“  í•˜ìœ„ layerì˜ íŠ¹ì§•ì„ ì°¸ì¡°í•  ìˆ˜ ìˆë„ë¡ í•´ì£¼ì—ˆë‹¤. ë”í•˜ê¸° ë‘ ì‹ í˜¸ë¥¼ í•©ì³ë²„ë¦°ë‹¤ concatnate chanelì€ ëŠ˜ì–´ë‚˜ì§€ë§Œ featureë¥¼ ë”ìš± ì˜ ë³´ì¡´ fixëœ 3x3 ë§Œí¼ì˜ weight paramterê°€ ì´ë¯¸ ì¡´ì¬ë¥¼ í•˜ê³  2d offsetì„ ìœ„í•œ branchê°€ ë”°ë¡œ ì¡´ì¬ í•œë‹¤. ê°ê°ì˜ weightë“¤ì„ ë²Œë ¤ì¤€ë‹¤? Semantic segmentationí”½ì…€ë‹¨ìœ„ë¡œ ë¶„ë¥˜í•´ë³´ì ì˜ìƒì†ì˜ maskë¥¼ ìƒì„±í•˜ê²Œ ë˜ëŠ”ë° ê°™ì€ classì´ì§€ë§Œ ì„œë¡œë‹¤ë¥¸ ë¬¼ì²´ë¥¼ êµ¬ë¶„í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤. ì˜ìƒì†ì— ìë™ì°¨ê°€ ì—¬ëŸ¬ëŒ€ ìˆì–´ë„ë‹¤ ê°™ì€ class (ìƒ‰) ìœ¼ë¡œ êµ¬ë¶„í•œë‹¤. ì˜ìƒë‚´ì˜ ì¥ë©´ contentë¥¼ ì´í•´í•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” í•„ìˆ˜ì ì¸ ê¸°ìˆ ì´ë‹¤. objectë“¤ì´ êµ¬ë¶„ë˜ëŠ” íŠ¹ì§•ì„ ì´í•´ë¥¼ í•˜ì—¬ Fully Convolutional Networksì…ë ¥ì—ì„œ ë¶€í„° ëê¹Œì§€ NNìœ¼ë¡œ êµ¬ì„±í•œë‹¤.ì…ë ¥ìœ¼ë¡œ ì„ì˜ì˜ í•´ìƒë„ ì¶œë ¥ë„ ì…ë ¥ì— ë§ì¶˜ í•´ìƒë„, ì¤‘ê°„ì˜ layerë“¤ë„ ëª¨ë‘ ë¯¸ë¶„ê°€ëŠ¥í•œ layerë“¤ì´ë‹¤. ê°ìœ„ì¹˜ë‹¤ channelì¶•ìœ¼ë¡œ flatteningì´í›„ ê°ê°ì˜ vectorë¥¼ ìŒ“ì•„ì„œ ê° ìœ„ì¹˜ë§ˆë‹¤ vectorê°€ í•˜ë‚˜ì”© ë‚˜ì˜¤ê²Œ ëœë‹¤. Upsampling receptive fieldê°€ ì‘ê¸° ë•Œë¬¸ì— upsamplingì„ í†µí•´ì„œ ê°•ì œë¡œ resolutionì„ ë§ì¶”ì–´ì¤€ë‹¤. ì¼ë‹¨ì€ ì‘ê²Œ ë§Œë“¤ì–´ì„œ receptive fieldë¥¼ ìµœëŒ€í•œ í‚¤ìš´ë‹¤ìŒì— upsamplingí•œë‹¤. Transpose Convolution ê²°ê³¼ë¥¼ ì´ë ‡ê²Œ ê·¸ëƒ¥ ë”í•´ë„ ë˜ëŠ”ê±´ê°€?cnnê³¼ stride ì‚¬ì´ì¦ˆë¥¼ ì¡°ì ˆí•´ì„œ ê²¹ì¹˜ëŠ”ë¶€ë¶„ì´ ì—†ê²Œë” ì¡°ì ˆí•´ì£¼ì–´ì•¼ í•œë‹¤. (overlap problem) Upsampling Convolution í•™ìŠµê°€ëŠ¥í•œ upsamplingì„ í•™ìŠµê°€ëŠ¥í•œ í•˜ë‚˜ì˜ layerë¡œ ë§Œë“¤ì–´ì£¼ì—ˆë‹¤. í•´ìƒë„ê°€ ë‚®ì•„ì§€ì§€ë§Œ semanticí•˜ê³  Holistic ì¤‘ê°„ì¸µì˜ mapì„ upsamplingí•œ ì´í›„ì— ë†’ì€ layerì— ìˆëŠ” feature mapì„ upsamplingì„ í†µí•´ í•´ìƒë„ë¥¼ ì˜¬ë¦¬ê³  ì´ì— ë§ì¶°ì„œ ì¤‘ê°„ì¸µì˜ mapë“¤ë˜í•œ upsamplingí•œë‹¤. ì´ë“¤ì„ concatnateí•˜ì—¬ì„œ ê°í”½ì…€ë§ˆë‹¤ classì˜ scoreë¥¼ ë±‰ì–´ì£¼ê²Œ ëœë‹¤. ìµœëŒ€í•œ ë§ì€ layerë“¤ì„ í•©ì¹œê²ƒì´ í° ë„ì›€ì´ ëœë‹¤. FCNì€ end to endë¡œ ì†ìœ¼ë¡œ ë§Œë“ ê²Œ ì•„ë‹ˆë¼ ëª¨ë‘ NNì´ë¼ ë³‘ë ¬ì²˜ë¦¬ë„ ê°€ëŠ¥í•˜ê³  ì„±ëŠ¥ë„ ì¢‹ìœ¼ë©°, low high featureëª¨ë‘ ì˜ í¬í•¨í•œë‹¤. U-Net built upon fully convolutional networks with skip connections channel sizeê°€ ì¤„ê³  í•´ìƒë„ê°€ ëŠëŠ” expanding path fusion - concatnationì„ ì‚¬ìš©í•œë‹¤. DeepLabpixelê³¼ pixelì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì´ì–´ì¤€í›„ pixelê°„ì˜ ê±°ë¦¬ë¥¼ ëª¨ë¸ë§í•˜ì˜€ë‹¤.í™•ì‚°ì˜ ë°˜ë³µìœ¼ë¡œ ë¬¼ì²´ì˜ ê²½ê³„ì— ì˜ë§ëŠ” segmetationì„ Dilated convolution parameterìˆ˜ëŠ” ëŠ˜ì–´ë‚˜ì§€ë§Œ depthwise convolution channelë³„ë¡œ convì—°ì‚°ì„ í•´ì„œ ê°’ì„ ê°ê° ë½‘ì€í›„, ê° channelë³„ë¡œ pointwise convolutionì„ í†µí•˜ì—¬ í•˜ë‚˜ë¡œ í•©ì³ì¤€ë‹¤. Instance segmentationìœ¼ë¡œ ë¹ ë¥´ê²Œ ë°œì „ì„ í•˜ê³ ìˆë‹¤. Instance segmantation : ê°™ì€ ì‚¬ëŒì´ì—¬ë„ ê°™ì€ìƒ‰ì´ ì•„ë‹Œ ë”°ë¡œë”°ë¡œ segmentationì´ ê°€ëŠ¥í•œ ê¸°ëŠ¥ panoptic segmentation Instance segmentationì„ í¬í•¨í•˜ëŠ” ê¸°ìˆ  ê°ì²´ë“¤ì„ êµ¬ë¶„í•˜ëŠ” ê¸°ìˆ  : object detection scene understandingì„ ìœ„í•œ ê¸°ìˆ  bounding obì™€ classificationì„ ë™ì‹œì— ì¶”ì •í•˜ëŠ” ê¸°ìˆ ì´ë‹¤. í•´ë‹¹í•˜ëŠ” boxì˜ ë¬¼ì²´ì˜ categoryê¹Œì§€ ì¶”ì •í•œë‹¤.2ê°œì˜ ì¢Œí‘œë¡œ bounging boxë¥¼ ê²°ì •í•œë‹¤. ë‚˜ë¨¸ì§€ëŠ” classì—ëŒ€í•œ probabilityë¥¼ ê²°ì •í•´ ì¤€ë‹¤. Bounding box localization selective search oversegmentation ì´í›„ ë¹„ìŠ·í•œ ìƒ‰ê¹”ë¼ë¦¬ í•©ì³ì¤€ë‹¤. Two-stage detector R-CNN ê¸°ì¡´ì˜ image classificationì„ í™œìš© selective serch ë¡œ region proposalì„ êµ¬í•˜ê³ ì ì ˆí•œ í¬ê¸°ë¡œ warpingì„ í•´ì„œ CNN (pretrained)ì— ë„£ì–´ì¤€í›„ categoryë¥¼ êµ¬í•´ì¤€ë‹¤.ë§ˆì§€ë§‰ classifierì€ SVMì„ ì¼ë‹¤.ë‹¨ì  : model í•˜ë‚˜í•˜ë‚˜ë§ˆë‹¤ ëª¨ë‘ cnnì„ ëŒë ¤ì•¼í•˜ê³  selective searchë¥¼ ì‚¬ìš©í•´ì„œ í•™ìŠµì„ í†µí•œ ì„±ëŠ¥í–¥ìƒì— ì œí•œì´ìˆë‹¤. Fast R-CNN recycle a pre-computed feature for multiple object detection ì˜ìƒì „ì²´ì— ëŒ€í•œ featureì„ ì¶”ì¶œí›„ ì´ë¥¼ ì¬í™œìš© CNNì—ì„œ Convolutional feature mapì„ ë½‘ì•„ì£¼ê³ (warping x) ROI pooling layerë¡œ feature mapìœ¼ë¡œ ë¶€í„° ROI featureë¥¼ ë½‘ì•„ë‚¸ë‹¤ feature poolingì´í›„ classì™€ bbox regressionì„ ì‚¬ìš©í•œë‹¤. ì—¬ì „íˆ roië¥¼ ì°¾ê¸° ìœ„í•´ selective searchë¥¼ ì“°ê³ ìˆë‹¤ Faster R-CNN ìµœì´ˆì˜ endtoend object detection IoU = Area overlap/Area of Union, ë†’ì„ìˆ˜ë¡ ë‘ì˜ì—­ì´ ë§ì´ ê²¹ì¹œë‹¤ Anchor boxes- 9ê°œì˜ actor boxë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤. ë¯¸ë¦¬ ì •í•´ë†“ì€ bboxì˜ í¬ê¸° Selective searchë¥¼ ëŒ€ì²´í•˜ëŠ” RPNì„ ì œì•ˆí•˜ì˜€ë‹¤. ê·¸ëŸ´ë“¯í•œ bboxë§Œ ë‚¨ê¸°ê¸° ìœ„í•´ non maximum suppressionì„ ì‚¬ìš©í•˜ì˜€ë‹¤. Single stage detectorì •í™•ë„ ë³´ë‹¤ ì†ë„ë¥¼ ì„ íƒí•œ ê²ƒì´ë‹¤ imageë¥¼ girdë¡œ ë‚˜ëˆ„ì–´ì„œ 4ê°œì˜ ì¢Œí‘œì™€ confidence scoreë¥¼ ì˜ˆì¸¡í•œë‹¤. ê°ê°ì˜ taskë³´ë‹¤ Instance segmentationê³¼ Panoptic segmentation Instance SegmentationInstance segmentation = Sementic segmentation + distuguishing instances Mask R-CNN RPN ê¸°ì¡´ì˜ ROI í’€ë§ì€ ì •ìˆ˜ì¢Œí‘œë§Œ ì§€ì›ì„ í–ˆì—ˆëŠ”ë°, interpolationì„ ìœ„í•´ì„œ ì†Œìˆ˜ì  pixel levelì„ ì§€ì›í•˜ì˜€ë‹¤. Panoptic SegmentationUPSNetFPNêµ¬ì¡°ë¡œ ê³ í•´ìƒë„ featureë¥¼ ë½‘ì€ ì´í›„ Semantic Head ì™€ Instance Headë¡œ ë‚˜ëˆ„ì–´ predictë¥¼ í•˜ê²Œ ëœë‹¤. Landmark localizationFacial landmark localizaiton Human pose estimation ë‹¤ì–‘í•œ dataë¥¼ ì‚¬ìš©í•œ í•™ìŠµ Multi-model learningChallenges ê°ê°ì˜ ê°ê°ì˜ ë°ì´í„°ê°€ ëª¨ë‘ ë‹¤ë¥¸ representationì„ ëˆë‹¤ Feature spaceì— ëŒ€í•˜ì—¬ balanceê°€ ë§ì§€ ì•ŠëŠ”ë‹¤. ì—¬ëŸ¬ modelityë¥¼ ì‚¬ìš©í•  ê²½ìš° íŠ¹ì • modelì— biasë ìˆ˜ ìˆë‹¤. ëŒ€í‘œì ì¸ êµ¬ì¡° Matching Translating Referencing Visual data &amp; TextJoint embedding Image tagging íƒœê·¸ -&gt; ì´ë¯¸ì§€, ì´ë¯¸ì§€ -&gt; íƒœê·¸ ê° featureë“¤ì€ ì°¨ì›ì„ ë§ì¶°ì£¼ê³  ì´ë‘˜ì˜ Joint embeddingì„ ë§Œë“¤ì–´ì¤€ë‹¤. ê°™ì€ spaceì— ì´ë¯¸ì§€ì™€ textë¥¼ embeddingí•´ì£¼ê³  matchingë˜ëŠ” imageì™€ text ë¼ë¦¬ ê±°ë¦¬ê°€ ê°€ê¹Œì›Œ ì§€ê²Œë” í•™ìŠµì„ ì§„í–‰í•œë‹¤. Metric Learning ì°½ - í”„ë¡œì„¸ìŠ¤ (í˜„ì¬ ì§„í–‰ì¤‘)íƒ­ - ì“°ë ˆë“œ (ê·¸ëƒ¥ ë„ì›Œì§„ ì°½) ì“°ë ˆë“œë§ˆë‹¤ ê°–ëŠ” ë©”ëª¨ë¦¬ ê³µê°„ / í”„ë¡œì„¸ìŠ¤ê°€ ê³µìœ í•˜ëŠ” ë©”ëª¨ë¦¬ ê³µê°„ì´ ìˆë‹¤. í”„ë¡œì„¸ìŠ¤ê°€ ëŠ˜ì–´ë‚˜ë©´ ì“°ë ˆë“œ ê³µìœ  ê³µê°„ì´ ëŠ˜ì–´ë‚˜ê²Œ ëœë‹¤. process: ì½”ì–´ìˆ˜ì— ë”°ë¼ ë³‘ë ¬ì²˜ë¦¬ ê°€ëŠ¥thread: í”„ë¡œì„¸ìŠ¤ ìœ„ì— ì˜¬ë¼ê°€ìˆëŠ” taskë³´í†µ 1ê°œì˜ processë¡œ concurrentë¡œ ì²˜ë¦¬í•˜ëŠ” ê²ƒ ë³´ë‹¤, max coreì˜ 5070%ì •ë„ë¡œ processë¥¼ ë‚˜ëˆ ì„œ ì²˜ë¦¬í•´ì£¼ëŠ” ê²Œ í›¨ì”¬ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤. â€”- ps. íŒŒì´ì¬ì€ ë©€í‹°í”„ë¡œì„¸ì‹± &gt;&gt; ë©€í‹°ì“°ë ˆë”©search keyword: multi threading/processing, python global interpreter lock(GIL) mini task) [ë©€í‹° í”„ë¡œì„¸ì‹±/ì“°ë ˆë”©] ìœ¼ë¡œ 10ë§Œê¹Œì§€ì˜ ì†Œìˆ˜ ì°¾ê³  ì„±ëŠ¥ ë¹„êµ í›„ githubì— ì˜¬ë¦¬ê¸° (~3.15 ì›”) â€”- point â€”- í”¼ì–´ì„¸ì…˜ ë¿ë§Œ ì•„ë‹ˆë¼, ì•ìœ¼ë¡œ ê³µë¶€ë°©í–¥ì— ìˆì–´ ìˆ˜ì—… ë‚´ìš© ì™¸ì ìœ¼ë¡œ ì „ì²´ì ì¸ ê·¸ë¦¼ì„ ê·¸ë¦¬ë©° ê³µë¶€ë¥¼ ì´ì–´ë‚˜ê°ˆ ê²ƒ! (cs, ml pipeline ë“±â€¦) Q) ml ì—”ì§€ë‹ˆì–´ë¼ë©´, ê²€ìƒ‰ì„ í–ˆì„ ë•Œ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚´ê¸° ìœ„í•´ì„œëŠ” ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”? A) ì–´ë–¤ ê²ƒì´ë‘ ì–´ë–¤ ê²ƒì„ ì—°ê²°ì‹œí‚¬ ê±´ì§€â€¦ë“±ë“± ì˜ ìƒê°í•´ë³´ì! ^_^","link":"/2021/03/08/2021-03-08-Boostcamp31.1/"},{"title":"Pstage2_KLUE","text":"ë¬¸ì¥ë‚´ ê°œì²´ê´€ ê´€ê³„ ì¶”ì¶œë­”ê°€ ì•„ì‰¬ì› ë˜ P-stage 2 KLUEê°€ ëì´ ë‚¬ë‹¤. ì´ë²ˆ stageì—ì„œëŠ” ë¦¬ë”ë³´ë“œ ìˆœìœ„ë¥¼ ì˜¬ë¦¬ëŠ”ë°ì—ë§Œ ì§‘ì¤‘í•˜ê¸° ë³´ë‹¤ëŠ” ë‹¤ì–‘í•œ taskë¥¼ ì¨ë³´ê³  ì›ë¦¬ë¥¼ ì´í•´í•˜ê³  ê²°ê³¼ë¥¼ í† ë¡ ê³„ì‹œíŒì— ê¼­ ì¡°ê¸ˆì´ë¼ë„ ê³µìœ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•˜ê¸°ë¡œ ë§ˆìŒë¨¹ì—ˆì—ˆë‹¤. Overview ë¬¸ì œì •ì˜ : ë¬¸ì¥ì˜ ë‹¨ì–´(Entity)ì— ëŒ€í•œ ì†ì„±ê³¼ ê´€ê³„ë¥¼ ì˜ˆì¸¡í•˜ë¼. Input data : 9000ê°œì˜ train data, 1000ê°œì˜ test data ìš°ë¦¬ê°€ ë¹¼ë‚´ì•¼ í•  column : sentence, entities, place of entities Output ì´ 42ê°œì˜ classë¥¼ ì˜ˆì¸¡ 1{'ê´€ê³„_ì—†ìŒ': 0, 'ì¸ë¬¼:ë°°ìš°ì': 1, 'ì¸ë¬¼:ì§ì—…/ì§í•¨': 2, 'ë‹¨ì²´:ëª¨íšŒì‚¬': 3, 'ì¸ë¬¼:ì†Œì†ë‹¨ì²´': 4, 'ì¸ë¬¼:ë™ë£Œ': 5, 'ë‹¨ì²´:ë³„ì¹­': 6, 'ì¸ë¬¼:ì¶œì‹ ì„±ë¶„/êµ­ì ': 7, 'ì¸ë¬¼:ë¶€ëª¨ë‹˜': 8, 'ë‹¨ì²´:ë³¸ì‚¬_êµ­ê°€': 9, 'ë‹¨ì²´:êµ¬ì„±ì›': 10, 'ì¸ë¬¼:ê¸°íƒ€_ì¹œì¡±': 11, 'ë‹¨ì²´:ì°½ë¦½ì': 12, 'ë‹¨ì²´:ì£¼ì£¼': 13, 'ì¸ë¬¼:ì‚¬ë§_ì¼ì‹œ': 14, 'ë‹¨ì²´:ìƒìœ„_ë‹¨ì²´': 15, 'ë‹¨ì²´:ë³¸ì‚¬_ì£¼(ë„)': 16, 'ë‹¨ì²´:ì œì‘': 17, 'ì¸ë¬¼:ì‚¬ë§_ì›ì¸': 18, 'ì¸ë¬¼:ì¶œìƒ_ë„ì‹œ': 19, 'ë‹¨ì²´:ë³¸ì‚¬_ë„ì‹œ': 20, 'ì¸ë¬¼:ìë…€': 21, 'ì¸ë¬¼:ì œì‘': 22, 'ë‹¨ì²´:í•˜ìœ„_ë‹¨ì²´': 23, 'ì¸ë¬¼:ë³„ì¹­': 24, 'ì¸ë¬¼:í˜•ì œ/ìë§¤/ë‚¨ë§¤': 25, 'ì¸ë¬¼:ì¶œìƒ_êµ­ê°€': 26, 'ì¸ë¬¼:ì¶œìƒ_ì¼ì‹œ': 27, 'ë‹¨ì²´:êµ¬ì„±ì›_ìˆ˜': 28, 'ë‹¨ì²´:ìíšŒì‚¬': 29, 'ì¸ë¬¼:ê±°ì£¼_ì£¼(ë„)': 30, 'ë‹¨ì²´:í•´ì‚°ì¼': 31, 'ì¸ë¬¼:ê±°ì£¼_ë„ì‹œ': 32, 'ë‹¨ì²´:ì°½ë¦½ì¼': 33, 'ì¸ë¬¼:ì¢…êµ': 34, 'ì¸ë¬¼:ê±°ì£¼_êµ­ê°€': 35, 'ì¸ë¬¼:ìš©ì˜ì': 36, 'ì¸ë¬¼:ì‚¬ë§_ë„ì‹œ': 37, 'ë‹¨ì²´:ì •ì¹˜/ì¢…êµì„±í–¥': 38, 'ì¸ë¬¼:í•™êµ': 39, 'ì¸ë¬¼:ì‚¬ë§_êµ­ê°€': 40, 'ì¸ë¬¼:ë‚˜ì´': 41} EDAì´ë²ˆ ìì—°ì–´ taskì˜ ê²½ìš°ì—ëŠ” ë°ì´í„°ë„ ì ê³  image taskì— ë¹„í•´ ë³µì¡í•œ edaê°€ í•„ìš”í•œê²ƒ ê°™ì§€ëŠ” ì•Šì•˜ë‹¤. ë”°ë¼ì„œ ê°€ì¥ ì¤‘ìš”í•œ labelë“¤ì˜ ê°¯ìˆ˜ë§Œì„ í™•ì¸í•˜ê³  ë¹ ë¥´ê²Œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ë„˜ì–´ê°”ë‹¤. (í† ë¡ ê¸€ì—ë„ í•˜ë‚˜ ì‘ì„±í•˜ê¸´ í–ˆì§€ë§Œ, ê±°ê¸°ì„œ ì‘ì„±í–ˆë˜ labelë“¤ê°„ì˜ ìœ ì‚¬ì„±ìœ¼ë¡œ class imbalnaceë¥¼ í•´ê²°í•˜ê¸° ë³´ë‹¤ëŠ” focal lossë¥¼ ì‚¬ìš©í•˜ëŠ”ê²Œ ì§ê´€ì ìœ¼ë¡œ ë” ì‰¬ì›Œ ë³´ì—¬ì„œ focal lossë¥¼ ì‚¬ìš©í•˜ê¸°ë¡œ í•˜ì˜€ë‹¤. labelë“¤ì˜ ë¶„í¬ ì£¼í”¼í„° ë…¸íŠ¸ë¶ì„ í†µí•´ ë¹ ë¥´ê²Œ dataë¥¼ ë¶ˆëŸ¬ì™€ì„œ labelë“¤ì˜ ë¶„í¬ë¥¼ í™•ì¸í•˜ì—¬ ë³´ë‹ˆ. labelë“¤ê°„ì˜ ë¶ˆê· í˜•ì´ ë§¤ìš° ë§¤ìš° ì‹¬í–ˆë‹¤. ì‹¬ì§€ì–´ ì¸ë¬¼ : ì‚¬ë§êµ­ê°€ì˜ labelì„ ê°€ì§€ëŠ” dataëŠ” 1ê°œ ë¿ ì´ì˜€ë‹¤. ì´ 1ê°œë¡œ ê³¼ì—° test dataì˜ í•´ë‹¹ labelì„ ì˜ ë§ì¶œìˆ˜ ìˆì„ê¹Œ? ì•„ë‹ê²ƒ ê°™ë‹¤. ì´ëŸ¬í•œ imbalnaceë¬¸ì œì— íš¨ê³¼ì ìœ¼ë¡œ ëŒ€ì²˜í•˜ëŠ” ë°©ë²•ì€ ì´ì „ p-stageì—ì„œë„ ë°°ì› ì—ˆë‹¤. ë°”ì•„ì•„ì•„ë¡œ focal loss focal lossì— ëŒ€í•´ì„œ ê°„ëµí•˜ê²Œ ì•Œì•„ë³´ì 1234567891011121314151617class FocalLoss(nn.Module): def __init__(self, weight=None, gamma=2., reduction='mean'): nn.Module.__init__(self) self.weight = weight self.gamma = gamma self.reduction = reduction def forward(self, input_tensor, target_tensor): log_prob = F.log_softmax(input_tensor, dim=-1) prob = torch.exp(log_prob) return F.nll_loss( ((1 - prob) ** self.gamma) * log_prob, target_tensor, weight=self.weight, reduction=self.reduction ) Focal lossëŠ” í˜ì´ìŠ¤ë¶ì˜ Lin et alì´ ì œì•ˆí•œ loss functionì´ë‹¤ ê°„ë‹¨í•˜ê²Œ ë§í•˜ë©´ ë¶„ë¥˜ ì—ëŸ¬ì— ê·¼ê±°í•˜ì—¬ ë§ì¶˜ í™•ë¥ ì´ ë†’ì€ ClassëŠ” ì¡°ê¸ˆì˜ lossë¥¼, ë§ì¶˜ í™•ë¥ ì´ ë‚®ì€ ClassëŠ” Lossë¥¼ í›¨ì”¬ ë†’ê²Œ ë¶€ì—¬í•´ì£¼ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ì£¼ì–´ì„œ class imbalanceì— ë”ìš± íš¨ìœ¨ì ìœ¼ë¡œ ëŒ€ì²˜í•˜ëŠ” loss funtionì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤. loss í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê²Œ ë˜ë©´ input tensorë“¤ì— ëŒ€í•œ í™•ë¥ ë¡œ í‘œí˜„ëœ tensorë¥¼ ì–»ì€ë’¤ F.nll_lossë¥¼ í˜¸ì¶œí•œë‹¤. F.nll_loss(((1-prob) ** self.gamma) * log_prob,target_tensor,weight=self.weight,reduction=self.reduction) Weight ê°’ì„ ìš°ë¦¬ê°€ ê°€ì§€ê³  ìˆëŠ” label ë¶„í¬ì— ëŒ€í•œ 1-d tensorë¡œ ë„£ì–´ì¤€ë‹¤. Ex) ìš°ë¦¬ê°€ ê°€ì§€ê³  ìˆëŠ” labelë“¤ì€ 42ê°œë‹ˆê¹Œ 42 lengthë¥¼ ê°€ì§€ëŠ” 1-d tensor ê°’ì€ labelì˜ ë¶„í¬ì— ë§ê²Œ ì´ë ‡ê²Œ loss í•¨ìˆ˜ë¥¼ ë¶ˆëŸ¬ë‹¤ê°€ ì§œì£¼ë©´ ë„ì Modelëª¨ë¸ì„ êµ‰ì¥íˆ ì—¬ëŸ¬ê°œ ë¶ˆëŸ¬ë‹¤ê°€ì²˜ìŒì—” ëŒë ¤ë³´ì•˜ë‹¤. ì‚¬ìš©í•´ë³¸ model ëª©ë¡ bert-base-multilingual-cased xlm-roberta-large koelectra-base-v3-discriminator kobert ì´ë ‡ê²Œ 4ê°œì •ë„ ì‹¤í—˜í•´ ë³´ì•˜ë˜ê²ƒ ê°™ì•˜ë‹¤. ê²°êµ­ ê²°ë¡ ë§Œ ë§í•´ë³´ìë©´ RoBERTa-largeë¥¼ ì‚¬ìš©í–ˆë‹¤.hevitz ë‹˜ì˜ í† ë¡ ê³„ì‹œíŒ ê¸€ì„ ë³´ë‹ˆ, bertë„ largeë¥¼ ì‚¬ìš©í•˜ë©´ ì¢‹ê² ì§€ë§Œ?????? ì•„ì§ Huggingfaceì— modelì´ ê³µê°œëœê²ƒ ê°™ì§€ ì•Šë‹¤. ê·¸ë¦¬ê³  This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. ë¼ëŠ” RoBERTaì˜ ë…¼ë¬¸ì„ ë³´ë©´ ì´ significant performance gains ë¼ëŠ” ë¬¸êµ¬ëŠ” ì´ê²Œ ë§ë‹¤ë¼ëŠ” í™•ì‹ ì„ ê°€ì ¸ë‹¤ ì£¼ì—ˆë‹¤. (ë¬¼ë¡  ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ë“¤ ìê¸°ê°€ ì§±ì´ë¼ê³ í•˜ê¸´í•˜ì§€) ë˜í•œ 4ê°œë¥¼ ê°ê° ì ë‹¹í•œ hyperparameterë¡œ ëŒë ¤ë³¸ ê²°ê³¼ í‰ê· ì ì¸ ì •í™•ë„ê°€ roberta-largeë¥¼ ì‚¬ìš©í•˜ë©´ ëŒ€í­ ì¦ê°€í•¨ì„ í™•ì¸í•˜ì˜€ë‹¤.RoBERTaëŠ” bertì™€ ìœ ì‚¬í•˜ì§€ë§Œ BERTì— ë‹¤ì–‘í•œ ë°©ë²•ì„ ì ìš©ì‹œì¼œ ì„±ëŠ¥ì„ í–¥ìƒí•œ modelì´ë‹¤. Modelì˜ êµ¬ì¡°ëŠ” bertì™€ í¡ì‚¬í•˜ë‹ˆ ìƒëµí•˜ê² ë‹¤. ë‹¨ì§€ hyperparameterë¥¼ ìµœì í™”í•˜ê³  NSPë¥¼ ì—†ì• ê³  ìµœëŒ€í•œ max_lengthì— ë§ì¶°ì„œ ë¬¸ì¥ì„ ë„£ì–´ì£¼ê³ , maskingì„ ë” ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ í•´ì£¼ì—ˆë‹¤ê³  í•œë‹¤. train ë°©ë²•? loss optimizer Train-set, validation-set ë‚˜ëˆ„ê¸° ì´ë²ˆ KLUEì—ì„œ Huggingfaceì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì§ˆë¦¬ë„ë¡ ë‹¤ë£¬ê²ƒ ê°™ë‹¤. ë¬¼ë¡  ì•„ì§ ëª¨ìë¥´ì§€ë§Œ ã…ã…ì´ì „ P-stageì—ì„œëŠ” training ê³¼ì •ì„ ìš°ë¦¬ê°€ ë‹¤ pytorch ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ train í•¨ìˆ˜ë¥¼ ë§Œë“¤ê³  ì¼ë¼ì¼ë¼ í•´ì„œ êµ¬í˜„í–ˆì—ˆëŠ”ë°!!!! ì´ëŸ° í¸ë¦¬í•œ trainerë¼ëŠ”ê²Œ ìˆëŠ” hugging face ì•„ì£¼ ì¹­ì°¬í•´ ^^ í•˜ì§€ë§Œ ë‹¨ìˆœíˆ trainerë¥¼ ì‚¬ìš©í•˜ëŠ”ê²ƒì€ ì‹¤ë ¥ ì¦ì§„ì— ë³„ë¡œ ì˜ë¯¸ê°€ ì—†ë‹¤ê³  ìƒê°í–ˆë‹¤. (ë¬¼ë¡  ë§ˆìŠ¤í„°ë‹˜ ë§ì”€ì²˜ëŸ¼ Hugging faceë§Œ ì˜ ì‚¬ìš©í•˜ë”ë¼ë„ ê·¸ë§Œí¼ ì¥ì ì´ ìˆë‹¤ê³  í•œë‹¤!!!) ê·¸ë˜ì„œ huggingface í™ˆí˜ì´ì§€ì— ë“¤ì–´ê°€ì„œ trainerë¥¼ ìì„¸íˆ ì‚´í´ë³´ì•˜ë‹¤. ì²˜ìŒì— ì •í–ˆë˜ focal lossë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” trainer classë¥¼ ìƒì†ë°›ì•„ì„œ ë‚˜ë§Œì˜ trainer classë¥¼ êµ¬í˜„í•´ì£¼ì–´ì•¼ í–ˆë‹¤. í™ˆí˜ì´ì§€ë¥¼ ë³´ë©´ ê´€ë ¨ ì˜ˆì œê°€ ìˆì–´ ì‰½ê²Œ ë°”ê¿”ì¤„ìˆ˜ ìˆì—ˆë‹¤. (í˜„ê·œë‹˜ì˜ ë„ì›€ê³¼ í•¨ê»˜ë¼ë©´ ã…ã…) 12345678class FocalLossTrainer(Trainer) : def compute_loss(self, model, inputs, return_outputs=False) : labels = inputs.pop('labels') outputs = model(**inputs) logits = outputs.logits loss_fn = FocalLoss(weight=weight) loss = loss_fn(logits, labels) return (loss, outputs) if return_outputs else los ì—¬ê¸°ì„œ loss_fnë§Œ ìœ„ì—ì„œ ì •ì˜í•œ FocalLoss()ë¥¼ ë¶ˆëŸ¬ë‹¤ê°€ ì‚¬ìš©í•˜ë©´ ë„ì! easy Optimizerê´€ë ¨í•´ì„œëŠ” trainer ì•ˆì—ì„œ ì‚¬ìš©í•˜ëŠ” AdamWë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ ë ê±° ê°™ì•˜ê³  ìˆ˜ì •í•´ì£¼ì–´ì•¼ í•  ê²ƒì€ lr_scheduler lr ì´ì •ë„? ì¸ê²ƒ ê°™ë‹¤. default ë¡œ ì„¤ì •ëœ lr_schedulerì€ stepì— ë”°ë¼ linear í•˜ê²Œ lr ì´ ê°ì†Œí•˜ëŠ” schedulerë¥¼ ì“´ê±° ê°™ì€ë°ì €ë²ˆì— ì‚¬ìš©í–ˆë˜ CosineAnnealingWarmRestarts ìœ¼ë¡œ ë°”ê¾¸ì–´ì„œ ì‚¬ìš©í•´ë³´ë©´ ì–´ë–¨ê¹Œ? ìƒê°ì„ í•´ë³´ì•˜ë‹¤. ì €ë²ˆ stageì—ì„œ AdamPì™€ CosineAnnealingWarmRestartsì˜ ì¡°í•©ìœ¼ë¡œ ê½¤ë‚˜ ì ì í•œ ì¬ë¯¸ë¥¼ ë³´ì•˜ê¸° ë•Œë¬¸ì— ã…ã… ì„¸ì„¸í•œ parameterì€ seedë¥¼ ê³ ì •ì‹œí‚¨ ì´í›„ validation scoreë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •í•´ì£¼ë©´ ë ë“¯ ì‹¶ë‹¤.validation ê³¼ trainì€ 2:8 ë¡œ ë‚˜ëˆ„ì—ˆê³  ì œì¶œì „ì—ëŠ” dataëª¨ë‘ë¥¼ ì‚¬ìš©í•´ì„œ train í•œ modelë¡œ inference í•˜ì˜€ë‹¤. ì ì´ì œ ê°€ì¥í¬ê²Œ ê³ ë¯¼ì„ í•´ì¤€ ë¶€ë¶„ì´ë‹¤ Input í˜•ì‹ì— ë”°ë¥¸ ì„±ëŠ¥ì´ë²ˆ stageì—ì„œ ì œê³µëœ baseline codeëŠ” ê½¤ë‚˜ simpleí•˜ê³  ê°„ê²°í•˜ì§€ë§Œ ìˆì„ê±°ëŠ” ë‹¤ìˆë‹¤. ê°€ì¥ ì˜ë¬¸ì ì´ ë“¤ì—ˆë˜ ê²ƒì€ tokenizerì— ë„£ì–´ì£¼ëŠ” data ì˜ í˜•ì‹ì´ì˜€ë‹¤. ent01 : ì´ìˆœì‹  ent02 : ë¬´ì‹  sentence : ì´ìˆœì‹ ì€ ì¡°ì„ ì¤‘ê¸°ì˜ ë¬´ì‹ ì´ë‹¤. # RoBERTa tokenizer ê¸°ì¤€ special token 123456789{'bos_token': '&lt;s&gt;', 'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'sep_token': '&lt;/s&gt;', 'pad_token': '&lt;pad&gt;', 'cls_token': '&lt;s&gt;', 'mask_token': '&lt;mask&gt;'} &lt;s&gt; ì´ìˆœì‹  &lt;/s&gt; ë¬´ì‹  &lt;/s&gt;&lt;s&gt; ë¬¸ì¥ &lt;/s&gt; ì´ëŸ¬í•œ í˜•ì‹ìœ¼ë¡œ ë“¤ì–´ê°”ë‹¤.ì˜¤í”¼ìŠ¤ì•„ì›Œì—ì„œ ì—¬ì­ˆì–´ ë³´ë‹ˆê¹Œ, ë³„ë‹¤ë¥¸ ì´ìœ  ì—†ì´ ê°„ë‹¨í•˜ê²Œ ì •í•´ì¤€ í˜•ì‹ì´ë¼ê³  í–ˆë‹¤. ë”°ë¼ì„œ ì´ˆì½”ì†¡ì´ë‹˜ê»˜ì„œ ì˜¬ë ¤ì£¼ì‹  nerì„ entitiyì‚¬ì´ì— ë„£ì–´ì£¼ëŠ” ë…¼ë¬¸ê¸€ì„ ì½ê³  pororo libraryë¥¼ ì´ìš©í•˜ì—¬ nerì„ ì–»ì–´ë‚¸ë’¤ ì´ë¥¼ entityì–‘ì˜†ì— ì‚½ì…í•˜ì—¬ ì£¼ì—ˆë‹¤. ì´ì™€ê°™ì€ í˜•ì‹ì´ë‹¤. ì²˜ìŒì—ëŠ” ì € ê¸°í˜¸ë“¤ì„ special tokenì— ì¶”ê°€í•´ì¤€ë’¤, modelì— ë„£ì–´ì£¼ì—ˆì§€ë§Œ!! í˜œë¦°ë‹˜ì˜ ì§ˆë¬¸ê³¼ í”¼ë“œë°±ìœ¼ë¡œ ë…¼ë¬¸ì—ì„œëŠ” special í† í°ìœ¼ë¡œ ì§€ì •í•˜ì§€ ì•Šê³ , ì›ë˜ vocabì— ìˆëŠ” ê¸°í˜¸ë“¤ì„ ì‚¬ìš©í•˜ì˜€ë‹¤ê³  í•œë‹¤â€¦â€¦ì´ë¯¸ í† ë¡ ê¸€ì— ì˜¬ë ¸ëŠ”ë°â€¦â€¦â€¦ ì˜¬ë¦¬ê¸° ì˜í–ˆë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆë‹¤. ì˜¬ë¦¬ì§€ ì•Šì•˜ë‹¤ë©´ ì´ ë¬¸ì œë¥¼ í‰ìƒ ëª¨ë¥´ê³  ì˜ëª»ëœ ì§€ì‹ì„ ê°€ì§„ì±„ë¡œ ì‹¤í—˜í•˜ì˜€ì„ê²ƒì´ë‹¤. ë§ˆìŠ¤í„°ì´ë‚˜ ì¡°êµë‹˜ë“¤ ë§ì”€ëŒ€ë¡œ ì¼ë‹¨ ë‚˜ëŒ€ëŠ”ê²Œ ì¢‹ì€ê²ƒ ê°™ë‹¤. ë‚¨ë“¤ì—ê²Œ ë°°ìš¸ì ë„ ë§ê³ , ë‚˜ëŒ€ë©´ì„œ ìŠ¤ìŠ¤ë¡œ ì¢€ë” ì°¾ì•„ë³´ê³  í•™ìŠµí•˜ê²Œ ë˜ëŠ”ê²ƒ ê°™ë‹¤. ì´ëŸ¬í•œ input í˜•ì‹ì€ ë™ì¼ seed model hyperparamterìœ¼ë¡œ ì•½ 1.5í¼ ì •ë„ leaderboard accì˜ ìƒìŠ¹ì„ ì´ëŒì–´ëƒˆë‹¤. autotokenizerë¡œ ë¶ˆëŸ¬ì˜¨ tokenizerì€ ëŒ€ë¶€ë¶„ì˜ vocabë“¤ì„ í¬í•¨í•˜ê³  ìˆì–´, unknownìœ¼ë¡œ ë‚˜ì˜¤ëŠ” tokenë“¤ì´ ì ì€ê²ƒì„ í™•ì¸í•˜ì˜€ë‹¤.íŠ¹íˆ entityê°€ unkë¡œ ë‚˜ì˜¤ê²Œë˜ë©´ í° ë¬¸ì œì„ìœ¼ë¡œ ì´ë¥¼ ì²´í¬í•˜ì˜€ëŠ”ë°, ëª¨ë‘ ì˜ tokenizeëœê²ƒì„ í™•ì¸í•˜ì˜€ë‹¤. optimzerì€ trainerì—ì„œ ì‚¬ìš©í–ˆë˜ê±¸ë¡œ ë™ì¼í•˜ê²Œ ì‚¬ìš©í•˜ì˜€ê³ lrê³¼ schedulerë¥¼ ë°”ê¾¸ì–´ ê°€ë©° RoBERTaì— ë§ëŠ” hyperparameterë¥¼ ì°¾ê¸°ìœ„í•´ ë…¸ë ¸í–ˆë‹¤.;) ì´ë²ˆ stageì—ëŠ” ë”°ë¡œ í•˜ê³ ìˆëŠ” ROS ì‹¤ìŠµê³¼ ê²¹ì³ í•˜ê³ ì‹¶ì€ê²Œ 3ê°€ì§€ ìˆì—ˆëŠ”ë° ëª»í•´ë´¤ë‹¤ã… ã…  wandbë¡œ ì‹¤í—˜ ê´€ë¦¬í•˜ê¸°, sweep ì‚¬ìš©í•´ì„œ automlê¹Œì§€ í•´ë³´ê¸° inputì— ë¬´ì‘ìœ„ë¡œ masking ì ìš©í•´ë³´ê¸° augmentationìœ¼ë¡œ data ì¦ê°•í•´ë³´ê¸° í•˜ì§€ë§Œ ì´ë²ˆ stageë¡œ ê´€ì‹¬ì´ ì—†ì—ˆë˜ NLPì— ëŒ€í•´ ë‹¤ì‹œí•œë²ˆ ìƒê°í•´ë³´ê²Œ ë˜ì—ˆë‹¤. ìƒê°ë³´ë‹¤ ì¬ë¯¸ìˆëŠ” ì•„ì´ë””ì–´ë“¤ì´ ë§ì•˜ê³ , íŠ¹íˆ ì˜¤í”¼ìŠ¤ì•„ì›Œë‚˜ ë§ˆìŠ¤í„° í´ë˜ìŠ¤ì—ì„œ ë©˜í† ë‹˜ê³¼ ë§ˆìŠ¤í„°ë‹˜ì˜ ì—´ì •ì„ ë³´ê³  êµ‰ì¥íˆ í° ì˜ê°ê³¼ ìê·¹ì„ ë°›ì•˜ë‹¤. ì•„ì§ í•œêµ­ì–´ ê´€ë ¨ NLP taskë“¤ì´ ë§ì´ ë¶€ì¡±í•˜ë‹¤ëŠ” ê±¸ ë“£ê³  í™•ì‹¤íˆ ê³ ë ¤í•´ë³´ê²Œ ë˜ì—ˆë‹¤. Hugging face ë¥¼ ë§ì´ ë‹¤ë£¨ì–´ ë³¸ì ë„ ë§¤ìš° ë“ì´ë˜ì—ˆë˜ stageì˜€ë‹¤.","link":"/2021/04/22/Pstage2-KLUE/"},{"title":"Pstage3_Image_Segmentation_Detection","text":"Image Segmentationhttps://github.com/bcaitech1/p3-ims-obd-hansarang Overview ë¬¸ì œì •ì˜ : ì“°ë ˆê¸°ê°€ ì°íŒ ì‚¬ì§„ì—ì„œ ì“°ë ˆê¸°ë¥¼ Segmentation Input data : 4109ì¥ì˜ ì“°ë ˆê¸° ì‚¬ì§„ì¤‘, 3287ì¥ (80%)ëŠ” train data, ë‚˜ë¨¸ì§€ 812ì¥(20%)ëŠ” private test data (512,512)ì˜ ì´ë¯¸ì§€ Annotation train_all.json: trainì— ì“°ì¼ ìˆ˜ ìˆëŠ” ëª¨ë“  image, annotation ì •ë³´ (image: 3272, annotation: 26400) train.json: train_all.json ì¤‘ 4/5ì— í•´ë‹¹í•˜ëŠ” ì •ë³´ (image: 2617, annotation: 21116) val.json: train_all.json ì¤‘ 1/5ì— í•´ë‹¹í•˜ëŠ” ì •ë³´ (image: 655, annotation: 5284) test.json: ì˜ˆì¸¡í•´ì•¼í•  ì´ë¯¸ì§€ë“¤ì˜ ì •ë³´ (image: 837) id: íŒŒì¼ ì•ˆì— annotation ê³ ìœ  id, ì´ê±´ í•œ image ì•ˆì— ì—¬ëŸ¬ê°€ì§€ì˜ ê°ì²´ê°€ ìˆê¸° ë–„ë¬¸ì— imageë³„ë¡œ ê°ê°ì˜ ê°ì²´ì˜ annotationë“¤ì´ ìˆë‹¤. segmentation: masking ë˜ì–´ ìˆëŠ” ê³ ìœ ì˜ ì¢Œí‘œ bbox: ê°ì²´ê°€ ì¡´ì¬í•˜ëŠ” ë°•ìŠ¤ì˜ ì¢Œí‘œ (x_min, y_min, w, h) area: ê°ì²´ê°€ ì¡´ì¬í•˜ëŠ” ì˜ì—­ì˜ í¬ê¸° category_id: ê°ì²´ê°€ í•´ë‹¹í•˜ëŠ” classì˜ id image_id: annotationì´ í‘œì‹œëœ ì´ë¯¸ì§€ ê³ ìœ  id images id: íŒŒì¼ ì•ˆì—ì„œ image ê³ ìœ  id, ex) 1 height: 512 width: 512 file_name: ex) batch_01_vt/002.jpg Output data : 11 class = {UNKNOWN, General trash, Paper, Paper pack, Metal, Glass, Plastic, Styrofoam, Plastic bag, Battery, Clothing} í‰ê°€ Metric ë²Œì¨ 2ì£¼ì „ì˜ ê¸°ì–µì´ë¼ ê°€ë¬¼ê°€ë¬¼í•˜ì§€ë§Œ ì¼ë ¬ì˜ ê³¼ì •ë“¤ì„ í•˜ë‚˜í•˜ë‚˜ ë˜ì§šì–´ ë³´ë©° ì´ì–´ë‚˜ê°€ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. deeplab V3+, efficientnet-b5 EDA ë¨¼ì € EDA ë¶€í„° ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. ìˆ˜ì—…ì—ì„œ ì œê³µí•´ì£¼ì‹  edaë“¤ë¡œ ìš°ë¦¬ dataì˜ labelë¶„í¬ë¥¼ í™•ì¸í•  ìˆ˜ ìˆì—ˆê³ , ë”°ë¡œ ì´ë¯¸ì§€ dataë¥¼ ì‹œê°í™”ë¥¼ í•´ë³´ë©° ì´ì „ì˜ stageì™€ ë§ˆì°¬ê°€ì§€ë¡œ ìƒë‹¹íˆ imbalanceí•˜ë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œê²Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì— stage 2ë•Œë„ ì ìš©í•˜ì˜€ë˜ focal lossì™€ ë‹¤ì–‘í•œ augmentationì„ ì‚¬ìš©í•˜ì—¬ í•´ê²°í•´ì•¼ê² ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆìŠµë‹ˆë‹¤. Model ìœ„ì—ì„œ ì–¸ê¸‰í•œê²ƒ ì²˜ëŸ¼ ì²˜ìŒë¶€í„° sotaì— ê°€ê¹Œìš´ deeplab V3+ë¥¼ ì„ ì •í•˜ì˜€ê³ , íš¨ìœ¨ì ì´ê³  ë‹¤ì–‘í•œ ì‹¤í—˜ê³¼ì •ì„ ìœ„í•´ backboneì€ efficientnet b1ìœ¼ë¡œì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ì— ì‘ì€ modelì—ì„œì˜ parameterê°€ ê³¼ì—° í° modelì—ì„œë„ ë˜‘ê°™ì´ ì ìš©ë ê¹Œë¼ëŠ” ì˜ë¬¸ì´ ë“¤ì—ˆì§€ë§Œ, ì´ëŠ” lrì´ë‚˜ schedulerì— í•´ë‹¹í•œë‹¤ê³  ìƒê°ì´ ë˜ì–´, augmentationì‹¤í—˜ì—ì„œë§Œ modelì˜ sizeë¥¼ ë‚®ì¶”ì—ˆìŠµë‹ˆë‹¤. ê°€ì¥ì²˜ìŒ í•œ ì‹¤í—˜ì€ ì œê¸°ì–µì—ëŠ” ë™ì¼ì¡°ê±´ì—ì„œì˜ backboneì— ë”°ë¥¸ ì„±ëŠ¥ì´ì˜€ìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ í¬ê¸°ì˜ Resnextì™€ efficientNetìœ¼ë¡œ ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ê³ , ê²°ë¡ ì ìœ¼ë¡œ efficientNet-b5ë¥¼ backboneìœ¼ë¡œ ì“´ modelì´ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ìŠµë‹ˆë‹¤.ResnextëŠ” efficientNetì— ë¹„í•´ ìˆ˜ë ´ì†ë„ë„ ë¹ ë¥´ê³  epochë‹¹ ì‹œê°„ë„ ì ê²Œ ê±¸ë ¤, ì´í›„ì˜ ì•™ìƒë¸”ì„ ìœ„í•´ best modelì„ ì €ì¥í•´ ë‘ì—ˆìŠµë‹ˆë‹¤. Augmentation ì´ë²ˆ taskì—ì„œ ì“°ê²Œëœ augmentationì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤ HorizontalFlip(p=0.5) Rotate(p=0.5, limit=45) Cutout(num_holes=4, max_h_size=20, max_w_size=20), CLAHE(), RandomBrightnessContrast(p=0.5), Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0) ì´ë ‡ê²Œ ì¡°í•©í•´ì„œ ì¼ì„ë•Œ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë‹¤ëŠ” ê²°ë¡ ì„ ì–»ì—ˆì—ˆìŠµë‹ˆë‹¤.ë‹¤ì–‘í•œ ì¡°í•©ê³¼ í™•ë¥  ê°’ë“¤ì„ ì ìš©í•˜ì—¬ ë¹„êµí•˜ì—¬ ì§„í–‰í•˜ëŠ” ì¼ë ¨ì˜ ê³¼ì •ë“¤ì€ ë§¤ìš° ê³ ë˜ê³  ë§ì€ ì‹œê°„ì„ í•„ìš”ë¡œ í•˜ì˜€ìŠµë‹ˆë‹¤.ì—¬ê¸°ì„œ ì§€ê¸ˆì™€ì„œ ìƒê°í•´ë³´ë©´ Auto Augmentationì„ ì ìš©í•´ ë³´ì•˜ìœ¼ë©´ ì¢‹ì•˜ì„ë“¯ ì‹¶ìŠµë‹ˆë‹¤â€¦ ë˜í•œ ì¶”ê°€ì ìœ¼ë¡œ horizontalflipì„ ì´ìš©í•œ ttaë¥¼ ì ìš©ì‹œì¼œ ë³´ì•˜ì§€ë§Œ, ì„±ëŠ¥ì˜ í•˜ë½ì„ ì•¼ê¸°í–ˆìŠµë‹ˆë‹¤. Loss &amp; Optimizer &amp; Scheduler LossëŠ” Focal lossì™€ soft-crossentropy-lossë¥¼ ê°ê° 0.3,0.7ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë‘ì–´ í•™ìŠµí•˜ì˜€ìŠµë‹ˆë‹¤. ì›ë˜ëŠ” Focalë§Œì„ ì‚¬ìš©í•˜ì˜€ì§€ë§Œ, stage1ì—ì„œ multi lossì—ì„œ ì¬ë¯¸ë¥¼ ë§ì´ ë´¤ì—ˆê¸° ë•Œë¬¸ì—, ë§ˆìŠ¤í„°ë‹˜ì˜ ì˜ê²¬ì„ ë“£ê³  sclì„ ì¶”ê°€í•´ ì£¼ì—ˆìŠµë‹ˆë‹¤. ì™œì¸ì§€ëŠ” ëª¨ë¥´ê² ì§€ë§Œ soft-crossentropy-lossë§Œì„ ì‚¬ìš©í•˜ì˜€ì„ë•Œ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•„, ì•™ìƒë¸”ë•Œì˜ ë‹¤ì–‘ì„±ì„ ìœ„í•´ multilossë¡œë„ í•™ìŠµì„ í•´ë‘ì—ˆìŠµë‹ˆë‹¤. Optimizerë˜í•œ Adamê³„ì—´ì˜ Adampë¥¼ ì‚¬ìš©í•˜ì˜€ê³ , Adamê³„ì—´ê³¼ ì˜ì–´ìš¸ë¦¬ëŠ” Customizedëœ CosineAnnealingWarmRestartsì˜ schedulerë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. í™•ì‹¤ì´ ì¤‘ê°„ì¤‘ê°„ì— lrì„ ë†’í˜€ì£¼ëŠ”ê²Œ local minimumì„ ì˜ë¹ ì ¸ë‚˜ì˜¤ëŠ” ëª¨ìŠµì„ í™•ì¸ í•  ìˆ˜ìˆì—ˆìŠµë‹ˆë‹¤. ë‚´ë¶€ì— ë‚´ì¥ëœ Cosin schedulerì€ gammaê°€ ì—†ê¸°ë•Œë¬¸ì— customizedëœ schedulerë¥¼ ë¶ˆëŸ¬ë‹¤ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. Adamì— ì˜ë§ëŠ” cosine ê³„ì—´ì˜ schedulerë¥¼ ì‚¬ìš©í•œ ê²°ê³¼, steplrì„ ì‚¬ìš©í•œ íƒ€ íŒ€ì›ì˜ model ëŒ€ë¹„ ì œ modelì˜ ì„±ëŠ¥ì´ ì˜ë‚˜ì™”ìŒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤. wandbì˜ ê·¸ë˜í”„ë¥¼ ë³´ì‹œë©´ ë³´í†µ 18 epochì¯¤ì—ì„œ ìµœê³ ì ì„ ì°ê³  ìˆ˜ë ´í•˜ëŠ” ëª¨ìŠµì„ ê´€ì°°í•˜ì˜€ìŠµë‹ˆë‹¤. K-fold &amp; Pseudo Labeled data single modelì˜ ì„±ëŠ¥ì˜ í•œê³„ì— ë¶€ë”›í˜€ 0.63ëŒ€ë¥¼ í—¤ì–´ë‚˜ì˜¤ì§€ ëª»í•˜ê³ ìˆì—ˆë˜ 2ì£¼ì°¨â€¦ê¸°ì¡´ì˜ ìµœê³ ì„±ëŠ¥ parameterë¥¼ ê³ ì •í•˜ê³  Train+allê³¼ pseudo labeledëœ dataë¥¼ í•©ì³ì„œ 2ë°°ì˜ dataë¡œ í•™ìŠµì„ ì§„í–‰í•˜ì˜€ê³  ê²°ê³¼ëŠ” ë§¤ìš° ì„±ê³µì ì´ì˜€ìŠµë‹ˆë‹¤. K-foldë¡œ ì§„í–‰í•˜ê³  ì‹¶ì—ˆì§€ë§Œ, GPUìì›ì˜ ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ì‹œê°„ì˜ í•œê³„ë•Œë¬¸ì— Train-allë¡œ ì§„í–‰í•˜ì—¬ ì œê°€ ê²½í—˜ì ìœ¼ë¡œ ì²´ë“í•œ 18 epochì—ì„œ ëŠëŠ” ë°©ì‹ì„ ì²´íƒí•˜ì˜€ìŠµë‹ˆë‹¤. ê²°ê³¼ëŠ” ë§¤ìš°ì„±ê³µì ìœ¼ë¡œ single model ê¸°ì¤€ 0.6842ë¼ëŠ” í° ì„±ëŠ¥í–¥ìƒì„ ì–»ì–´ë‚´ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ íŒ€ì›ë¶„ë“¤ë„ pseudo labelì„ ì ìš©í•˜ì—¬ ì•™ìƒë¸”ì„ í•˜ì˜€ë‹¤ë©´ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì–´ë‚¼ ìˆ˜ ìˆì—ˆì„í…ë° ë§¤ìš° ì•„ì‰½ìŠµë‹ˆë‹¤. ì•™ìƒë¸” ìµœì¢…ì ìœ¼ë¡œ ì €ì˜ ë‹¤ì–‘í•œ backboneê³¼ lossë¥¼ ê°€ì§€ëŠ” modelë“¤ì„ ì¡°í•©í•˜ì—¬ soft votingì„ í•˜ì˜€ìŠµë‹ˆë‹¤. ê°€ì¤‘ì¹˜ëŠ” LBìƒìœ¼ë¡œ ê°€ì¥ë†’ì€ modelì— 0.4ë¥¼ ì£¼ì—ˆê³  ë‚˜ë¨¸ì§€ì— 0.2ì”©ì„ ì£¼ì–´ ì´ 4ê°œì˜ singel modelì„ ì•™ìƒë¸” í•˜ì—¬ ì œì¶œì„ í•´ë´¤ëŠ”ë°, 0.6961ì´ë¼ëŠ” ì•„ì£¼ ë†’ì€ ì ìˆ˜ê°€ ë‚˜ì™”ìŠµë‹ˆë‹¤. ì´ modelì— ë‹¤ë¥¸ íŒ€ì›ë¶„ë“¤ì˜ model hard voting í•´ë³´ì•˜ì§€ë§Œ ì„±ëŠ¥ì´ ê³„ì† í•˜ë½í•˜ì—¬ ê²°êµ­ì—ëŠ” ì €ì˜ modelë§Œì„ ì‚¬ìš©í•œ ì ìˆ˜ê°€ ìµœì¢…ì ìˆ˜ê°€ ë˜ëŠ” ì•„ì‰¬ìš´ ìƒí™©ì´ ì—°ì¶œë˜ì—ˆìŠµë‹ˆë‹¤â€¦ ì–´ëŠì •ë„ íŒ€ì›ë“¤ê°„ì˜ í‰ê· ì ì¸ ì ìˆ˜ëŒ€ê°€ ë¹„ìŠ·í•´ì•¼ ì•™ìƒë¸” í–ˆì„ë•Œ ì¢‹ì€ ì ìˆ˜ë¥¼ ë‚¼ìˆ˜ìˆì—ˆì§€ë§Œ, psudo labelì„ ì €ë§Œ ëŒë ¸ì—ˆê¸° ë•Œë¬¸ì—â€¦ì‹œê°„ì´ 2ì¼ì •ë„ ë”ìˆì—ˆë‹¤ë©´ ë‹¤ë¥¸ íŒ€ì› ë¶„ë“¤ë„ ìˆ˜ë„ë¼ë²¨ë¡œ ì„±ëŠ¥ì„ ì–´ëŠì •ë„ í–¥ìƒì‹œì¼œ ë¹„ìŠ·í•œ ì ìˆ˜ëŒ€ë¡œ ë§ì¶°ì¤„ìˆ˜ ìˆì—ˆì„ í…ë° í•˜ëŠ” ì•„ì‰¬ì›€ì´ ë‚¨ì•˜ìŠµë‹ˆë‹¤â€¦ Object Detection í‰ê°€ Metric ìœ„ì™€ ê°™ì€ PB curveë¥¼ ê·¸ë¦°ë‹¤. ì´ ë•Œ recallê³¼ precisionì€ confidence scoreë³„ë¡œ ì ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤.ì´í›„ (A+B)ì— í•´ë‹¹í•˜ëŠ” ì˜ì—­ì˜ ë„“ì´ê°€ APê°€ ë˜ê³ , ê°ê°ì˜ classì˜ APì˜ í‰ê· ì´ ì €í¬ê°€ êµ¬í•˜ë ¤ëŠ” mAPì…ë‹ˆë‹¤. ì´ë²ˆ object taskì—ì„œëŠ” mmdetectionì´ë¼ëŠ” ê°•ë ¥í•œ toolì„ ê¸°ë°˜ìœ¼ë¡œ ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.mmdetectionì—ì„œëŠ” ì €í¬ê°€ configíŒŒì¼ë§Œì„ ìˆ˜ì •í•˜ì—¬ ì¤€ë‹¤ë©´ ì†ì‰½ê²Œ ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ë‹¤ì–‘í•œ modelë“¤ì„ ì‹¤í—˜í•´ ë³¼ ìˆ˜ìˆì—ˆìŠµë‹ˆë‹¤. Model ì´ë²ˆ taskì—ì„œë˜í•œ sota modelë¡œ ì•Œë ¤ì§„ swin transformerë¥¼ backboneìœ¼ë¡œ ì“°ê³  detector ë¶€ë¶„ì€ cascade mask rcnnê³¼ htcë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.ë‹¤í–‰ì´ë„ Swin transformerì˜ configíŒŒì¼ì´ gitì— ì „ë¶€ ì˜¬ë¼ì™€ìˆì—ˆê³ , ì €í¬ì˜ ì‹¤í—˜í™˜ê²½ì— ë§ê²Œ ì¡°ê¸ˆ ë³€ê²½í•´ ì£¼ë©´ ë˜ì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ mmdetectionì´ë¼ëŠ” íˆ´ìì²´ì— ì ì‘ì„ í•˜ëŠ”ë° ì‹œê°„ì´ ì¢€ ì†Œìš”ê°€ ë˜ì—ˆê³ , í•œ 3-4ì¼ ì •ë„ê°€ ì§€ë‚˜ì„œì•¼ ì–´ëŠì •ë„ ê°€ë‹¥ì´ ì¡íˆë©´ì„œ ì–´ë–»ê²Œ ì¨ì•¼í• ì§€ ê°ì´ ì¡í˜”ë˜ê²ƒ ê°™ìŠµë‹ˆë‹¤. backboneì„ ê³ ì •í•œ í›„ neckì„ ë³€ê²½ì‹œì¼œì„œ ë‹¤ì–‘í•œ ì‹¤í—˜ì„ í•´ë³´ì•˜ìŠµë‹ˆë‹¤. FPN PAFPN NAS-FPN BiFPN ì´ë ‡ê²Œ 4ê°€ì§€ì˜ ì„ íƒì§€ê°€ ìˆì—ˆëŠ”ë° ì´ì¤‘ ê°€ì¥ ì˜¤ë˜ëœ FPNì„ ì„ íƒí•œ ì´ìœ ëŠ” í•œê°€ì§€ ì…ë‹ˆë‹¤. ì™œëƒí•˜ë©´ pretrainedëœ pthíŒŒì¼ì„ githubì—ì„œ ì œê³µí•´ì£¼ê³  ìˆëŠ”ë°, ì´ modelì—ì„œ FPNì„ ì“°ê³ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì²˜ìŒì—ëŠ” backboneë§Œì„ pretrainedëœê±¸ ê°€ì ¸ì™€ì„œ ì“°ë‹¤ê°€, ì „ì²´ê°€ trainedëœ modelì´ ìˆëŠ”ê²ƒì„ ë°œê²¬í•˜ê³  ì‹¤í—˜í•´ë³´ì•˜ëŠ”ë° ì „ì²´ê°€ pretrainedëœê±¸ ê°€ì ¸ì™€ì„œ ì €í¬ taskì— fine tuning? transfer learningí•˜ëŠ” ë°©ì‹ì´ ë”ìš± ì„±ëŠ¥ì´ ì¢‹ì•˜ìŠµë‹ˆë‹¤. FPNìœ¼ë¡œ trainedëœ ì „ì²´ modelì„ ê°€ì ¸ë‹¤ê°€ NAS-FPNìœ¼ë¡œ ë°”ê¾¸ì–´ì¤€ modelì— ì ìš©ì„ ì‹œì¼œì¤„ì‹œ neckìª½ì˜ weightë“¤ì—ëŠ” ê°’ì´ ë“¤ì–´ê°€ì§€ ì•Šê²Œë©ë‹ˆë‹¤. ì´ê²ƒì´ ì„±ëŠ¥ì´ ë” ì¢‹ì„ ìˆ˜ë„ ìˆê¸°ë•Œë¬¸ì— ì´ëŸ¬í•œ ë°©ë²•ë„ ì‹œë„í•´ ë³´ì•˜ì§€ë§Œ, ë°”ê¾¸ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ FPNì„ ì‚¬ìš©í•˜ëŠ”ê²ƒì´ ë” ì¢‹ì•˜ìŠµë‹ˆë‹¤. í•œê°€ì§€ ì•„ì‰¬ì› ë˜ ê²ƒì€ pretrainedëœ Swin transformerë¥¼ backboneìœ¼ë¡œ ì“°ë©´ì„œ swinì— ëŒ€í•œ ì–´ëŠì •ë„ ì „ë°˜ì ì¸ ì´í•´ë§Œì„ ê°€ì§€ê³  ìˆì—ˆì„ë¿, ì„¸ì„¸í•œ modelì˜ êµ¬ì¡°ëŠ” ì•Œì§€ ëª»í•œì±„ ì‘ì„±ë˜ì–´ì§„ config íŒŒì¼ë§Œì„ ê°€ì§€ê³  ì‹¤í—˜ì—ë§Œ ì§‘ì¤‘í•  ìˆ˜ ë°–ì— ì—†ì—ˆë˜ ìƒí™©ì´ì˜€ìŠµë‹ˆë‹¤. ì•½ê°„ì”© parameterë“¤ì„ ìˆ˜ì •í•´ ì£¼ë©´ì„œ, ê·¼ë³¸ì ì¸ ì´í•´ì—†ì´ ì§ê´€ì— ì˜í•´ ì‹¤í—˜ì„ ë°˜ë³µí•˜ê³  ìˆëŠ” ì œ ìì‹ ì„ ë°œê²¬í•œ í›„ competitionì— ëŒ€í•œ ì•½ê°„ì˜ íšŒì˜ê°ì´ ë“¤ì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë©˜í† ë‹˜ê»˜ì„œ libraryë¥¼ ì˜ë‹¤ë£¨ëŠ” ê²ƒë„ í•˜ë‚˜ì˜ ëŠ¥ë ¥ì´ë¼ê³  ë§ì”€í•´ì£¼ì…”ì„œ ë‹¤ì‹œ í•œë²ˆ ìƒê°í•´ ë³´ì•˜ë˜ê²ƒ ê°™ìŠµë‹ˆë‹¤. Augmentation Augmentationì—ëŠ” Flipê³¼ Autoaugmentation, Normalizeë“±ì„ ì ìš©í•´ë³´ì•˜ìŠµë‹ˆë‹¤.ê°€ì¥ criticalí•˜ê²Œ ì‘ìš©í–ˆë˜ augementationì´ ë°”ë¡œ autoaugë¡œ, ì—¬ê¸°ì„œ resizeì™€ crop sizeë¥¼ ì–´ë–»ê²Œ ì£¼ëŠëƒì— ë”°ë¼ ì„±ëŠ¥ì°¨ì´ê°€ ì¡°ê¸ˆì”© ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì´ì „ stageë“¤ì—ì„œì˜ ê²½í—˜ìœ¼ë¡œ, image taskì—ì„œ high scaleì˜ image trainingì€ ì˜¤ëœ ì‹œê°„ì„ ìš”êµ¬í•˜ì§€ë§Œ ê·¸ë§Œí¼ ì„±ëŠ¥ì´ ì˜ë‚˜ì™”ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ resizeì˜ listì—ëŠ” upscaleëœ ì •ì‚¬ê°í˜•ê³¼ ê°€ë¡œê°€ ê¸´ ì§ì‚¬ê°í˜•, ì„¸ë¡œê°€ ê¸´ ì§ì‚¬ê°í˜•ë“±ì„ ê³ ë£¨ ì„ì–´ autoaugì•ˆì— ì¸ìë¡œ ë„£ì–´ì£¼ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë³€ê²½ì ì€ ì •ì‚¬ê°í˜•ë§Œì„ ë„£ì–´ì¤¬ì„ë•Œ, low scaleing í•´ì£¼ì—ˆì„ë•Œì— ë¹„í•´ì„œ ì ìˆ˜ì˜ í° í–¥ìƒì„ ì•¼ê¸°í–ˆìŠµë‹ˆë‹¤. Loss Optimizer Scheduler Loss ì €í¬ê°€ ë°”ê¾¸ì–´ ì¤„ ìˆ˜ìˆì—ˆë˜ lossëŠ” bbox lossë¡œ 3ê°€ì§€ ì •ë„ì˜ ì„ íƒì§€ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ê·¸ì¤‘ DIoU Lossë¥¼ ì±„íƒí•˜ì˜€ì„ë•Œ ì„±ëŠ¥ì´ ì•½ê°„ ìƒìŠ¹í–ˆê³ , classification lossìª½ì˜ cross entropy lossëŠ” ê±´ë“œë¦¬ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Optimizer Optimizerì€ AdamWë¥¼ ì‚¬ìš©í•˜ì˜€ê³ , ì´ˆê¸° lrê°’ì€ 1e-4ìœ¼ë¡œ ê³ ì •ì‹œì¼œ ì£¼ì—ˆìŠµë‹ˆë‹¤. Scheduler Schedulerì€ modelì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì ìš©ì‹œì¼œ ì£¼ì—ˆìŠµë‹ˆë‹¤. HTCë¥¼ ì´ìš©í•œ modelì—ëŠ” cosineannealingì„ cascade mask rcnnì„ ì ìš©í•œ modelì—ëŠ” steplrì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. StepLRì„ ì‚¬ìš©ì‹œ ì–´ë– í•œ epochì—ì„œ lrê°’ì„ ê°ì†Œì‹œì¼œì¤„ì§€ë¥¼ ì •í•  ìˆ˜ ìˆì—ˆëŠ”ë°, í‰ê· ì ìœ¼ë¡œ 8,11 epochì—ì„œ map50ê°’ì´ ìˆ˜ë ´í•˜ê¸° ì‹œì‘í•˜ëŠ”ê²ƒì„ í™•ì¸í•˜ê³  ì´ì¯¤ì—ì„œ gamma=0.1ì˜ factorë¡œ lrê°’ì„ ê°ì†Œì‹œì¼œì£¼ì—ˆìŠµë‹ˆë‹¤. Pseudo-labeling ì´ì „ segementationì—ì„œ pseudo labelingìœ¼ë¡œ í° ì¬ë¯¸ë¥¼ ë³´ì•˜ì—ˆê¸° ë•Œë¬¸ì— ì´ë²ˆ taskì—ì„œëŠ” ì¢€ ì¼ì° ìµœê³ ì„±ëŠ¥ì˜ modelë¡œ pseudo dataë¥¼ ë§Œë“¤ì–´ ë¹ ë¥´ê²Œ ì‹¤í—˜í•´ ë³´ì•˜ìŠµë‹ˆë‹¤. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import jsonimport pandas as pdimport numpy as npwith open('./train_all.json') as json_file1: train_data = json.load(json_file1)with open('./test.json') as json_file2: test_data = json.load(json_file2)start = len(train_data['images'])train_id = train_data['images']test_id = test_data['images']for idx in test_id: idx['id'] = start start+=1PredictionString = pd.read_csv('./output1.csv')['PredictionString']annotation = []ids = 26402image_id = startfor bboxs in PredictionString: bboxs = bboxs.strip().split(' ') bboxs = [float(i) for i in bboxs] bboxs = [bboxs[i:i + 6] for i in range(0, len(bboxs), 6)] for bbox in bboxs: prob = bbox[1] if prob&lt;0.8: continue temp = dict() temp['id'] = ids temp['image_id'] = image_id temp['category_id'] = int(bbox[0]) temp['segmentation'] = [[1,2,3,4,5]] temp['area'] = 1 bbox = bbox[2:] new = [bbox[0],bbox[3],bbox[2]-bbox[0],bbox[3]-bbox[1]] new = [i for i in list(np.round(new, 1))] temp['bbox'] = new temp['iscrowd'] = 0 annotation.append(temp) ids+=1 image_id+=1train_data['images'] += test_idtrain_data['annotations'] += annotationwith open('./train+pseudo.json', 'w') as outfile: json.dump(json_data1, outfile,indent=4) ìœ„ì˜ ì½”ë“œë¡œ pseudo dataë¥¼ cocoí˜•íƒœë¡œ ë°”ê¾¸ì–´ì¤€ë’¤ í†µí•©ëœ json íŒŒì¼ë¡œ trainì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ thresholdê°’ì„ ì„¤ì •í•´ì£¼ëŠ”ê²Œ ë§¤ìš° ì• ë§¤í–ˆë‹¤.ì´ì— ë”°ë¼ ì„±ëŠ¥ì´ ë„ˆë¬´ í•˜ë½í•˜ëŠ” í˜„ìƒì´ ë°œìƒí•˜ì˜€ê³  ê²°êµ­ pseudo labelëœ dataëŠ” ì“°ì§€ ëª»í•˜ì˜€ìŠµë‹ˆë‹¤â€¦ segementationì—ì„œ ì˜ë¨¹íˆë˜ pseudo labelì´ detectionì—ì„œëŠ” ë¶€ì •í™•í•œ labelê°’ë“¤ë¡œ í•™ìŠµì´ ì–´ë ¤ìš´ê°€ ë´…ë‹ˆë‹¤. WBF ë§ˆì§€ë§‰ìœ¼ë¡œ ìµœê³  single model ê¸°ì¤€ htcì™€ cascade ëª¨ë‘ 0.5572ì˜ ì ìˆ˜ë¥¼ ì–»ì–´ëƒˆê³  ì´ 4ê°œì˜ modelì„ ì•™ìƒë¸”í•œ ê²°ê³¼ 0.5824ì˜ ê²°ê³¼ë¥¼ ì–»ì–´ëƒˆìŠµë‹ˆë‹¤.ì´í›„ ëª¨ë“  íŒ€ì›ë“¤ì˜ csv íŒŒì¼ì„ WBFí•˜ì—¬ ìµœì¢…ì ì¸ score 0.5884ë¥¼ ì–»ì–´ë‚´ì—ˆìŠµë‹ˆë‹¤. Conclusion í•œê°€ì§€ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ëŠë‚€ì ì€ ì´ë ‡ê²Œ competitionì„ ë§ˆì¹œì´í›„ì— ê´€ë ¨ ë…¼ë¬¸ë“¤ê³¼ kaggle notebookë“¤ì„ ìì„¸íˆ ì •ë…í•˜ë©° ì“°ì˜€ë˜ ë°©ë²•ë¡ ë“¤ê³¼ modelë“¤ì„ ìƒì„¸í•˜ê²Œ ê³µë¶€í•´ì•¼ ê² ë‹¤ëŠ” í•„ìš”ì„±ì…ë‹ˆë‹¤. competition ì§„í–‰ì¤‘ì— ê°œì„ í•´ì•¼ í•  ì‚¬í•­ì€ ì¤‘ê°„ì¤‘ê°„ íŒ€ì›ë“¤ê°„ì˜ í‰ê· ì ì¸ ì ìˆ˜ëŒ€ë¥¼ ë§ì¶”ì–´ ë†“ì•„ì•¼ ìµœì¢… ì•™ìƒë¸” ê³¼ì •ì—ì„œ í° ì„±ëŠ¥ í–¥ìƒì„ ì´ë£°ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤. êµ‰ì¥íˆ ì—´ì •ì ì¸ 4ì£¼ë¥¼ ë³´ëƒˆìŠµë‹ˆë‹¤â€¦ ì•„ì‰¬ì›€ë„ ë‚¨ê³  í›„ë ¨í•˜ê¸°ë„ í•©ë‹ˆë‹¤â€¦ì§„í–‰í–ˆë˜ ë§ì€ ì‹¤í—˜ ë‚´ìš©ë“¤ì„ ëª¨ë‘ ë©ì—…ë ˆí¬íŠ¸ì— ë‹´ì§€ ëª»í–ˆë‹¤â€¦. ì¶”í›„ì— ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•´ì„œ gitì— ì˜¬ë ¤ë‘¬ì•¼ê² ìŠµë‹ˆë‹¤.","link":"/2021/04/25/Pstage3-Image-Segmentation-Detection/"},{"title":"VQA (Visual Question Answering)","text":"Boostcampì—ì„œ ë§Œë‚œ ë™ë£Œë“¤ê³¼ í•¨ê»˜ 2021 ì¸ê³µì§€ëŠ¥ ì˜¨ë¼ì¸ ê²½ì§„ëŒ€íšŒì— ì°¸ì—¬í–ˆìŠµë‹ˆë‹¤.ì´ 10ê°œì˜ ê³¼ì œì¤‘ ì‹œê°ì¥ì• ì¸ ì‹œìŠ¤í…œ ê°œë°œì„ ìœ„í•œ VQA ëª¨ë¸ì´ë¼ëŠ” Competitionì— ì°¸ì—¬í•˜ì˜€ìŠµë‹ˆë‹¤. ê°œìš”ì´ë¯¸ì§€ë¥¼ ë³´ê³  ì£¼ì–´ì§„ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” Visual Question Answering ëª¨ë¸ ê°œë°œVQAë€ ì‹œê°ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.ì‹¤ë‚´ ë° ì‹¤ì™¸ ìƒí™œ ê±°ì£¼ í™˜ê²½ì—ì„œ ì´¬ì˜ëœ ì´ë¯¸ì§€ì™€ ê·¸ì— ê´€ë ¨ëœ ì§ˆë¬¸, ëŒ€ë‹µì´ ì„¸íŠ¸ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.ì´ 224,464ê°œì˜ ì´ë¯¸ì§€ íŒŒì¼ê³¼ 702,135ê±´ì˜ ì§ˆë¬¸-ë‹µë³€ ìŒì´ train dataë¡œ ì£¼ì–´ì¡ŒìŠµë‹ˆë‹¤. ê´€ë ¨ ë…¼ë¬¸ review VQA: Visual Question Answering Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks UNITER: UNiversal Image-TExt Representation Learning","link":"/2021/07/12/VQA/"},{"title":"VQA: Visual Question Answering vs Competition Baseline","text":"VQA taskì˜ ì‹œì´ˆê²©ì¸ ë…¼ë¬¸ì´ë‹¤. VQA challengeì˜ ì „ë°˜ì ì¸ ê°œìš”ì™€ dataset, Base modelë“±ì„ ë‹¤ë£¨ê³  ìˆë‹¤. 1. Introduction VQAë€ Vision, NLP, knowledge representationì„ ëª¨ë‘ ì ‘ëª©ì‹œí‚¨ multi-discipline taskì´ë‹¤ ìœ„ì™€ ê°™ì´ ì–´ë– í•œ imageê°€ ì£¼ì–´ì§€ê³ , imageê°€ ì—†ìœ¼ë©´ ë§ì¶”ê¸° í˜ë“  ì§ˆë¬¸ë“¤ë¡œ êµ¬ì„±ë˜ì–´ìˆë‹¤. Answerì˜ ìœ í˜•ì— ë”°ë¼ open-ended questions ê³¼ multiple-choice taskë¡œ ë‚˜ëˆ„ì–´ ì§„ë‹¤. Open-ended questionì€ answerê°€ ë‹¤ì–‘í•´ ì§ˆ ìˆ˜ ìˆì§€ë§Œ, multiple-choice taskì˜ ê²½ìš°ì—ëŠ” ë¯¸ë¦¬ ì •í•´ì§„ answer listì¤‘ í•˜ë‚˜ê°€ ë‹µì•ˆì´ì—¬ì•¼ í•œë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” ë‘ê°€ì§€ì˜ answerìœ í˜• ì „ë¶€ ë‹¤ë£¨ê³  ìˆë‹¤. (ìš°ë¦¬ì˜ VQA taskëŠ” multiple-choice task ë§Œì„ ë‹¤ë£¨ê³  ìˆê¸° ë•Œë¬¸ì— open-ended ëŠ”ìƒëµ) 2. DatasetImage data MS COCO dataset Object Detectionê³¼ image captioningì—ì„œ ì‚¬ìš©ë˜ëŠ” dataset containing multiple objects and rich contextual information 123,287 training and validation images and 81,434 test images Abstract Scene dataset The dataset contains 20 â€œpaperdollâ€ human models [2] spanning genders, races, and ages with 8 different expressions. Paperdollë¡œ realí•œ ìƒí™©ì„ í‘œí˜„í•¨ we create a new abstract scenes dataset containing 50K scenes Question dataê°„ë‹¨í•œ questionì´ ì•„ë‹Œ ë³µì¡í•˜ê³  ì–´ë ¤ìš´ questionì„ ë§Œë“¤ì–´ë‚´ê¸° ìœ„í•´ â€œWe have built a smart robot. It understands a lot about images. It can recognize and name all the objects, it knows where the objects are, it can recognize the scene (e.g., kitchen, beach), peopleâ€™s expressions and poses, and properties of objects (e.g., color of objects, their texture). Your task is to stump this smart robot! Ask a question about this scene that this smart robot probably can not answer, but any human can easily answer while looking at the scene in the image.â€ ì´ëŸ¬í•œ ìš”ì²­ì„ ì£¼ì–´ questionì„ ë§Œë“¤ì–´ ë‚´ë„ë¡ í•˜ì˜€ë‹¤. Answer data18ê°œì˜ ì„ íƒì§€ë¥¼ êµ¬ì„± Correct : The most common (out of ten) correct answer Plausible : To generate incorrect, but still plausible answers we ask three subjects to answer the questions without seeing the image Popular : These are the 10 most popular answers. For instance, these are â€œyesâ€, â€œnoâ€, â€œ2â€, â€œ1â€, â€œwhiteâ€, â€œ3â€, â€œredâ€, â€œblueâ€, â€œ4â€, â€œgreenâ€ for real images Random : Correct answers from random questions in the dataset 3. Model VQAë¥¼ ìœ„í•œ Base modelê³¼ Competitionì—ì„œ ì œê³µí•´ì¤€ Baselineì˜ êµ¬ì¡°ë„ì´ë‹¤. Image channel Imageë¡œ ë¶€í„° embedding vectorë¥¼ ë½‘ì•„ë‚´ëŠ” ì—­í• ì„ í•œë‹¤.Pretrainedëœ VGGNetì„ ì‚¬ìš©í•˜ì˜€ìœ¼ë©´ ê¸°ì¡´ VGGNetì˜ ë§ˆì§€ë§‰ Fully-Connected MLPì˜ layerì—ì„œ 4096 dimìœ¼ë¡œ ë½‘ì•„ë‚´ì—ˆë‹¤. ì¸ê³µì§€ëŠ¥ ê²½ì§„ëŒ€íšŒì¸¡ì—ì„œ ì œê³µí•œ baselineì—ì„œëŠ” pretrainedëœ Resnet-50ì„ ì‚¬ìš©í•˜ì—¬ ë§ˆì§€ë§‰ Fully-Connected MLPì˜ layerì—ì„œ 768 dimìœ¼ë¡œ ë½‘ì•„ë‚´ê³  ìˆë‹¤. ì´í›„ ë…¼ë¬¸ê³¼ ë‹¤ë¥´ê²Œ ë³„ë„ì˜ Fully-Connected layerë¥¼ í†µê³¼ì‹œì¼œì£¼ì§€ ì•ŠëŠ”ë‹¤. ì• ì´ˆì— question channelê³¼ dimensionì„ ë§ì¶”ì–´ì£¼ì—ˆë‹¤. ë§ˆì§€ë§‰ target dimensionì€ 83ìœ¼ë¡œ train dataì˜ label ê°œìˆ˜ì´ë‹¤. Question channel Questionìœ¼ë¡œ ë¶€í„° embedding vectorë¥¼ ë½‘ì•„ë‚´ëŠ” ì—­í• ì„ í•œë‹¤.Each question word is encoded with 300-dim embedding by a fully-connected layer + tanh non-linearity which is then fed to the LSTM. Cell state, hidden state dim = 512Concate last cell state and last hidden state representations ì„ í•˜ì—¬ 1024ì˜ ì°¨ì›ì„ ë§Œë“¤ì–´ì£¼ì—ˆë‹¤. ì¸ê³µì§€ëŠ¥ ê²½ì§„ëŒ€íšŒì¸¡ì—ì„œ ì œê³µí•œ baselineì—ì„œëŠ” huggingfaceë¥¼ ì‚¬ìš©í•˜ì—¬ pretrainedëœ RoBERTa-base modelì„ í†µí•´ embedding vectorë¥¼ ë½‘ì•„ ë‚´ì—ˆë‹¤. ë½‘ì•„ë‚¸ embedding vectorì˜ dimì´ 768ì´ë¼ Image channelì˜ last dimensionë˜í•œ 768ë¡œ ë§ì¶°ì¤€ê²ƒì´ë‹¤. Multi-Layer Perceptron ì´ë¶€ë¶„ì€ ë‘˜ì˜ êµ¬ì¡°ê°€ ë™ì¼í•˜ë‹¤. ê° channelì—ì„œ ë‚˜ì˜¨ embedding vectorë“¤ì„ element wise multiplicationí•´ì¤€ë‹¤. ì´í›„ layerì„ ì¶”ê°€í•´ì£¼ì–´ ì°¨ì›ì„ ëŠ˜ë ¤ì¤€ë’¤, ë§ˆì§€ë§‰ layerì—ì„œ target labelì˜ ê°œìˆ˜ë§Œí¼ì˜ dimensionì„ ë½‘ì•„ë‚¸ë‹¤. ì´ë¶€ë¶„ì—ì„œ ê¶ê¸ˆí–ˆë˜ì ì€ ê° channelì˜ embedding vectorë¥¼ í•©ì¹˜ëŠ” ë°©ë²•ì— ë”°ë¥¸ ì„±ëŠ¥ì˜ ì°¨ì´ì˜€ë‹¤. Concat Element-wise Multiplication Element-wise Add ì´ì™€ ê´€ë ¨ëœ ë…¼ë¬¸ì„ ì°¾ì•„ë³´ì•˜ë‹¤. Component Analysis for Visual Question Answering Architectures ì´ë¼ëŠ” ë…¼ë¬¸ì—ì„œ ê° fusion ë°©ì‹ì— ë”°ë¥¸ ì„±ëŠ¥ì„ ì‹¤í—˜í•´ë³´ì•˜ë‹¤. ìœ„ ë…¼ë¬¸ì˜ ê²°ê³¼ì— ë”°ë¼ 3ê°€ì§€ fusion ë°©ì‹ì¤‘ Multiplication ë°©ì‹ì„ ê³„ì†í•´ì„œ ê³ ìˆ˜í–ˆë‹¤. ë˜í•œ ìœ„ë…¼ë¬¸ì—ì„œëŠ” BERTë¥¼ ì‚¬ìš©í•˜ì—¬ questionë¬¸ì¥ì„ embeddingë§Œ í•˜ê³  GRUë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… vectorë¥¼ ë½‘ì•„ë‚´ê³  ìˆë‹¤. ì´ëŸ¬í•œ ë°©ì‹ë„ ì‹œë„í•´ ë³¼ë§Œ í• ê²ƒ ê°™ë‹¤. 4. Result","link":"/2021/07/12/vqa_paper1/"},{"title":"fpn","text":"","link":"/2021/07/12/fpn/"},{"title":"swin_transformer","text":"","link":"/2021/07/12/swin-transformer/"},{"title":"satrn","text":"","link":"/2021/07/12/satrn/"},{"title":"Pstage4_ìˆ˜ì‹ì¸ì‹","text":"Pstage4_ìˆ˜ì‹ì¸ì‹ê¸°ìˆ ìŠ¤íƒ: AWS, Git, OpenCV, Pytorch, Streamlit, wandbë°œí‘œì‹œê°„: 15:00 - 15:20ë°œí‘œíŠ¸ë™: TRACK 4ìº í¼ ID: T1076, T1104, T1116, T1200, T1220, T1224ìº í¼ ì´ë¦„: ì› ì¡°, ê´‘ì› ì†¡, ì°¬ì—½ ì‹ , jaesub huh, Geumji Takí”„ë¡œì íŠ¸(ëŒ€íšŒ): OCR Github Repository íŒ€ ì†Œê°œ ğŸ¸ ì¡°ì› SATRN (Locality Feedforward, Shallow CNN) êµ¬í˜„ Augmentation (Resize &amp; Noramlization&amp; pixel í‰ê· ì— ë”°ë¥¸ ì´ì§„í™”â€¦) ì œì•ˆ &amp; ì‹¤í—˜ Ensemble (soft voting) êµ¬í˜„ &amp; ì‹¤í—˜ teacherforcing ratio scheduling ì‹¤í—˜ Beam search êµ¬í˜„ ë¶€ì¡±í•œ tokenì— ëŒ€í•œì¶”ê°€ Data ìƒì„± BERTë¥¼ ì‚¬ìš©í•œ misspelledëœ ìˆ˜ì‹ ì¡ì•„ë‚´ê¸° ì œì•ˆ CSTR ë…¼ë¬¸ reading &amp; ê³µìœ  í˜‘ì—… Gitì˜ Discussion, Pull &amp; Request, Wikië¥¼ í™œìš©í•˜ì—¬ í† ë¡ , ìë£Œ, ê²°ê³¼ê³µìœ  Wandbë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜ê³µìœ  ìˆ˜ì‹ì¸ì‹ Competition Overall ìƒì„¸ê°œìš” ìˆ˜ì‹ ì´ë¯¸ì§€ë¥¼ latex í¬ë§·ì˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤. ìˆ˜ì‹ì€ ì—¬ëŸ¬ ìì—°ê³¼í•™ ë¶„ì•¼ì—ì„œ ì–´ë ¤ìš´ ê°œë…ë“¤ì€ ê°„ë‹¨í•˜ê³  ê°„ê²°í•˜ê²Œ í‘œí˜„í•˜ëŠ” ë°©ë²•ìœ¼ë¡œì„œ ë„ë¦¬ ì‚¬ìš©ë˜ì–´ ì™”ìŠµë‹ˆë‹¤. Latex ë˜í•œ ì—¬ëŸ¬ ê³¼í•™ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë…¼ë¬¸ ë° ê¸°ìˆ  ë¬¸ì„œ ì‘ì„± í¬ë§·ìœ¼ë¡œì„œ í˜„ì¬ê¹Œì§€ë„ ë„ë¦¬ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ OCRê³¼ ë‹¬ë¦¬ ë¶„ìˆ˜, ì‹œê·¸ë§ˆ, ê·¹í•œê³¼ ê°™ì€ í‘œí˜„ì„ ì¸ì‹í•˜ê¸° ìœ„í•´ multi line recognitionì„ íŠ¹ì§•ìœ¼ë¡œ ê°€ì§‘ë‹ˆë‹¤. Dataset Scale : ê°ê°ì˜ image scaleì€ ì œê°ê° Label : Latex í˜•ì‹ì˜ ìˆ˜ì‹ Train Data Hand Written Data : 50000 Printed Data : 50000 Evaluation Data Public : 6000 Private : 6000 Prediction 241ê°œì˜ token classì¤‘ í•˜ë‚˜ë¥¼ ìƒì„±í•œ ë’¤ ì´ë“¤ì˜ sequence í‰ê°€ Metric 0.9 x â€œSentence Accuracyâ€ + 0.1 x (1-â€œWERâ€) Sentence Accuracy ì „ì²´ ì¶”ë¡  ê²°ê³¼(ìˆ˜ì‹) ì¤‘ì—ì„œ ëª‡ ê°œì˜ ìˆ˜ì‹ì´ ì •ë‹µ(ground truth)ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì§€ WER (Word Error Rate) ë‹¨ì–´ ë‹¨ìœ„ë¡œ ì‚½ì…(insertion), ì‚­ì œ(deletion), ëŒ€ì²´(substitution)ëœ ê¸€ì ê°œìˆ˜ë¥¼ ê³„ì‚° Problem &amp; Solving EDA ë¶€ì¡±í•œ token data Train dataì— ë“±ì¥í•˜ëŠ” tokenë“¤ì¤‘ ë¹ˆë„ìˆ˜ê°€ ê°€ì¥ì ì€ 50ê°œë¥¼ ë½‘ì•„ë³´ì•˜ë‹¤. 10íšŒ ë¯¸ë§Œìœ¼ë¡œ ë“±ì¥í•œ tokenë“¤ì— ëŒ€í•œ ì¶”ê°€ì ì¸ dataê°€ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨ë˜ì—ˆë‹¤. ì‚¬ì´íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ë¶€ì¡±í•œ tokenë“¤ì„ í¬í•¨í•˜ëŠ” ìˆ˜ì‹ì„ trainíŒŒì¼ì— 50ê°œ ì •ë„ì˜ dataë¥¼ ì¶”ê°€í•´ì£¼ì—ˆë‹¤. train_100003.jpg \\ominus \\vdots \\nexists \\rightleftarrows A \\supsetneq B ì„¸ë¡œí˜•íƒœì˜ ìˆ˜ì‹ ì´ë¯¸ì§€ ë‹¤ì–‘í•œ Aspect Ratio ì˜ëª»ëœ labeling P = 1ì´ ì•„ë‹Œ P = ìœ¼ë¡œ labeling ë˜ì–´ìˆìŒ ì„ ì´ ê·¸ì–´ì ¸ ìˆê±°ë‚˜ í˜•ê´‘íŒ¬ì´ ì¹ í•´ì ¸ ìˆëŠ” case Augmentation Resize Rotate OCR taskì—ì„œ ì„¸ë¡œë¡œ ë˜ì–´ì§„ imageë“¤ì€ ëª¨ë‘ noiseë¡œ ì‘ìš© EDAì—ì„œ ì ë‹¹í•œ Aspect ratioì˜ thresholdë¥¼ ì°¾ì€ ë’¤, threshold ë¯¸ë§Œì¸ dataë“¤ì— í•œí•˜ì—¬ Rotate Normalization ì´ë¯¸ì§€ì˜ ê° pixel ê°’ë“¤ì„ 0-1ì˜ ê°’ìœ¼ë¡œ normalize ì‹œì¼œì¤€ë‹¤ ì´ì§„í™” &amp; ê°€ë¡œì„  ì œê±° Model (SATRN) Locality-aware feedforward layer Base-line modelì— êµ¬í˜„ëœ Fully-connected feed forwardì—ì„œ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ê³  ìˆëŠ” Convolutuon feed forwardë¡œ êµì²´ Adaptive 2D Positional Encoding Baselineì—ì„œ êµ¬í˜„ëœ ì¼ë°˜ì ì¸ 2D positional encodingì—ì„œ ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ í•™ìŠµê°€ëŠ¥í•œ adaptive 2D positional encodingìœ¼ë¡œ ë³€ê²½ Backbone EfficientNet DenseNet ShallowCNN Mini SATRN for fast experiment ë‹¤ì–‘í•œ ì‹¤í—˜ì„ ë¹ ë¥´ê²Œ ì§„í–‰í•˜ê¸° ìœ„í•´ SATRNì˜ sizeë¥¼ ì¤„ì—¬ì„œ Mini SATRNìœ¼ë¡œ ë‹¤ì–‘í•œ ì‹¤í—˜ì„ ì§„í–‰ SATRNì˜ layer parameter ìˆ˜ì • Increase number of Decoder Layer mini model ê¸°ì¤€, Decoder layerë¥¼ ì¶”ê°€í• ìˆ˜ë¡ ì„±ëŠ¥ì´ í–¥ìƒ Change Activation Function ShallowCNNì˜ activation functionì„ ReLUëŒ€ì‹  mishë¥¼ ì‚¬ìš© Post - Processing Beam search Tokenì„ ë½‘ì„ ë•Œ argmaxë¥¼ ì‚¬ìš©í•˜ì—¬ í•˜ë‚˜ì˜ tokenë§Œì„ ë½‘ëŠ”ê²Œ ì•„ë‹Œ ê° stepë§ˆë‹¤ beam size k ë§Œí¼ì˜ tokenì„ ë½‘ì•„ ìµœëŒ€í•œ ì í•©í•œ sequenceë¥¼ ì„ íƒí•˜ë ¤ ì‹œë„í•¨ Ensemble ë‹¤ì–‘í•œ augementationì„ ê±°ì¹œ modelë“¤ì„ í•œë²ˆì— ë¶ˆëŸ¬ì™€ì„œ soft votingì„ ì‚¬ìš©í•˜ì—¬ ensemble Baselineì—ëŠ” ë¹ ì ¸ìˆëŠ” torch.nogradë¡œ memory ì ˆì•½ Use Language Model (x) ê´€ë ¨ í† ë¡  link OCR Demoë°ëª¨í˜ì´ì§€ ì£¼ì†Œ : http://35.74.99.158:8501/ Paper Review í”„ë¡œì íŠ¸ ì§„í–‰ì„ ìœ„í•´ ì½ì€ ë…¼ë¬¸ì˜ ëª©ë¡ì€ ë‹¤ìŒê³¼ ê°™ìŒ CSTR.pdf Misspelling Correction with Pre-trained Contextual Language Model.pdf CRNN.pdf An_Attentional_Scene_Text_Recognizer_with_Flexible_Rectification.pdf CBAM.pdf Towards End-to-end Text Spotting with Convolutional Recurrent Neural Networks.pdf TextBoxes_A Fast Text Detector with a Single Deep Neural Network.pdf","link":"/2021/06/24/%5BOCR-04%5D%202021%20%E1%84%8E%E1%85%AE%E1%86%AB%E1%84%92%E1%85%A1%E1%84%80%E1%85%A8%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%80%E1%85%B5%E1%84%80%E1%85%A2%E1%84%87%E1%85%A1%E1%86%AF%20%E1%84%8E%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A7%E1%86%AF%E1%84%89%E1%85%A1%E1%86%AB%20f82316ab87ef4617ab2e8ce26be53669/"},{"title":"cstr","text":"","link":"/2021/07/12/cstr/"}],"tags":[{"name":"boj","slug":"boj","link":"/tags/boj/"},{"name":"nlp","slug":"nlp","link":"/tags/nlp/"},{"name":"hugging_face","slug":"hugging-face","link":"/tags/hugging-face/"},{"name":"ner","slug":"ner","link":"/tags/ner/"},{"name":"Basic","slug":"Basic","link":"/tags/Basic/"},{"name":"Math","slug":"Math","link":"/tags/Math/"},{"name":"Books","slug":"Books","link":"/tags/Books/"},{"name":"Book","slug":"Book","link":"/tags/Book/"},{"name":"Summary","slug":"Summary","link":"/tags/Summary/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"Vision","slug":"Vision","link":"/tags/Vision/"},{"name":"RNN","slug":"RNN","link":"/tags/RNN/"},{"name":"Transformer","slug":"Transformer","link":"/tags/Transformer/"},{"name":"GAN","slug":"GAN","link":"/tags/GAN/"},{"name":"GPT","slug":"GPT","link":"/tags/GPT/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"Graph","slug":"Graph","link":"/tags/Graph/"},{"name":"Multimodal","slug":"Multimodal","link":"/tags/Multimodal/"},{"name":"Competition","slug":"Competition","link":"/tags/Competition/"},{"name":"VQA","slug":"VQA","link":"/tags/VQA/"},{"name":"CV","slug":"CV","link":"/tags/CV/"},{"name":"OCR","slug":"OCR","link":"/tags/OCR/"}],"categories":[{"name":"Algorithm","slug":"Algorithm","link":"/categories/Algorithm/"},{"name":"blog","slug":"blog","link":"/categories/blog/"},{"name":"Mathmatics_for_ML","slug":"Mathmatics-for-ML","link":"/categories/Mathmatics-for-ML/"},{"name":"Boostcamp","slug":"Boostcamp","link":"/categories/Boostcamp/"},{"name":"PaperReview","slug":"PaperReview","link":"/categories/PaperReview/"},{"name":"Competition","slug":"Competition","link":"/categories/Competition/"}]}