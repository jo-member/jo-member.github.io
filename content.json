{"pages":[{"title":"About Kinetic","text":"Intro.안녕하세요.","link":"/about/%E1%84%86%E1%85%AE%E1%84%8C%E1%85%A6.html"}],"posts":[{"title":"[백준 1157] 단어공부","text":"알파벳 대소문자로 된 단어가 주어지면, 이 단어에서 가장 많이 사용된 알파벳이 무엇인지 알아내는 프로그램을 작성하시오. 단, 대문자와 소문자를 구분하지 않는다. 첫째 줄에 알파벳 대소문자로 이루어진 단어가 주어진다. 주어지는 단어의 길이는 1,000,000을 넘지 않는다. 첫째 줄에 이 단어에서 가장 많이 사용된 알파벳을 대문자로 출력한다. 단, 가장 많이 사용된 알파벳이 여러 개 존재하는 경우에는 ?를 출력한다. 123456789101112x =input().upper()used = {}for i in x: if i not in used: used[i] = 1 else: used[i]+=1lis = sorted(used.items(),key = (lambda x:x[1]),reverse=True)if len(lis)!=1 and lis[0][1]==lis[1][1]: print(&quot;?&quot;)else: print(lis[0][0]) counter 모듈을 사용하면 더 쉽겠지만 구현을 연습하기 위해 그냥 dict를 써서 구현해주었다. easy했다.","link":"/2021/01/02/2020-01-02-boj1157/"},{"title":"[백준 1316] 그룹 단어 체커","text":"1. 접근 &amp; 구현간단한 문제이다. count 함수를 활용하였다. 알파벳의 갯수를 새준다음 이들이 연속된 갯수인지를 check해주기 위해 count()의 start,end를 정해주어 이들이 같으면 연속된것이고, 만약 다르다면 뒤에 같은 알파벳이 더 나온다는 뜻이므로 반복문을 break해주고 bool 값을 false로 바꾸어주어 그룹단어가 아님을 나타내준다. 만약 조건 check를 통과하였다면 index를 연속된 알파벳 이후로 바꾸어주어 반복문을 계속한다. 2. 코드(Python)12345678910111213141516N = int(input())l = [input() for _ in range(N)]cnt = 0for w in l: i = 0 b = True while i&lt;len(w): x = w.count(w[i]) if x!=w.count(w[i],i,i+x): b = False break else: i+=x if b: cnt+=1print(cnt)","link":"/2021/01/02/2020-01-02-boj1316/"},{"title":"[백준 2941] 크로아티아 알파벳","text":"간단한 문자열 문제이다. 입력으로 주어진 단어의 크로아티아 알파벳 갯수를 출력한다. 일단 크로아티아 알파벳의 정보를 담고있는 dictionary를 만들어 줘보자. 이후 특별히 긴 알파벳의 시작을 보면 c,d,l,n,s,z일때만 1자리 뒤에까지 check해주고 dz일때는 3번째까지 check해준다. 1234567891011121314151617181920x = input()d = [&quot;c=&quot;,&quot;c-&quot;,&quot;dz=&quot;,'d-','lj','nj','s=','z=']special = ['c','d','l','n','s','z']cnt = 0i = 0while i&lt;len(x): if x[i] in special: if i&lt;len(x)-2 and x[i]+x[i+1]+x[i+2] == 'dz=': i+=3 cnt+=1 elif i&lt;len(x)-1 and x[i]+x[i+1] in d: i+=2 cnt+=1 else: cnt+=1 i+=1 else: cnt+=1 i+=1print(cnt) index의 조작이 필요해 for문이 아닌 while문을 사용했고, 딱히 어려운 문제는 아니였다.","link":"/2021/01/02/2020-01-02-boj2941/"},{"title":"[백준 5430] AC","text":"구현을 완료했지만 시간 복잡도가 높아 시간초과가 나온다. reverse와 pop의 시간복잡도가 O(n)이므로 pop때문에 이러한 문제가 발생하는것 같다. 따라서 pop을 deque를 사용하여 popleft로 시간복잡도를 O(1)로 낮추지만 시간초과는 해결되지 않았다. 12345678910111213141516171819202122from sys import stdinfrom collections import dequedef solution(command,l): if len(l)&lt;command.count('D'): return 'error' for c in command: if c==&quot;R&quot;: l.reverse() elif c==&quot;D&quot;: l.popleft() return '['+','.join(l)+']'T = int(input())ans = []for _ in range(T): command = stdin.readline() n = int(stdin.readline().rstrip()) l = deque(stdin.readline().rstrip()[1:-1].split(',')) if n==0: l = [] print(solution(command,l)) 따라서 R일때마다 뒤집지 않고, point 변수를 만들어주어 reverse일때마다 point변수를 바꾸어주고 pop은 reverse 마다 방향을 바꾸어 주어 pop을 해주는 방식으로 구현하였다. 예를 들어 reverse 할때마다 point값을 바꾸어주며 D일때 point값을 확인해 popleft할지 pop할지를 결정한다. 최종적으로 point의 값을 확인하여 reverse를 할지 말지 결정해준다. 123456789101112131415161718192021222324252627282930313233from sys import stdinfrom collections import dequedef solution(command,l): point = 'left' if len(l)&lt;command.count('D'): return 'error' for c in command: if c==&quot;R&quot;: if point =='left': point ='right' else: point = 'left' elif c==&quot;D&quot;: if point =='left': l.popleft() else: l.pop() if point =='left': return '['+','.join(l)+']' else: l.reverse() return '['+','.join(l)+']'T = int(input())ans = []for _ in range(T): command = stdin.readline() n = int(stdin.readline().rstrip()) l = deque(stdin.readline().rstrip()[1:-1].split(',')) if n==0: l = [] print(solution(command,l)) 해결이 되었다!! 무조건 문제의 조건을 구현하기 보다는 보다 효율적인 방법을 찾는것이 바람직 하다는것을 느꼈다.","link":"/2021/01/02/2020-01-02-boj5430/"},{"title":"[백준 15686] 치킨배달","text":"접근문제의 입력조건이 작아 모든걸 탐색하는 brute force 알고리즘을 사용할수있음을 추측할수있다. 일단 M개의 치킨집에 대한 모든 조합을 구해 list에 담은뒤에 해당리스트를 순회하며 해당 치킨집들이 선택되었을때 도시의 치킨거리를 구하여 answer배열에 담아준다 이후 answer배열중 min값을 택하여 출력해준다. from itertools import combinations for i, value in enumerate() 위두가지를 잘 사용 코드1234567891011121314151617181920212223242526from itertools import combinationsN,M = map(int,input().split())m = [list(map(int,input().split())) for _ in range(N)]answer = []cnt = 0chicken = []for i in range(N): if 2 in m[i]: cnt += m[i].count(2) l = [i for i, value in enumerate(m[i]) if value == 2] for o in l: chicken.append([i,o])chicken = list(combinations(chicken,M))for x in chicken: d = [] for i in range(N): for j in range(N): if m[i][j] == 1: dist = [] for y in x: a, b = y dist.append(abs(i - a) + abs(j - b)) d.append(min(dist)) answer.append(sum(d))print(min(answer))","link":"/2021/01/04/2020-01-04-boj15686/"},{"title":"Blog 시작","text":"신년을 맞이하여 새롭게 무언가를 시작해본다정리하고 증명하기 위해 공부의 흔적을 남겨보자 네이버 Connect에서 진행하는 boostcamp AI tech의 BAT와 1차 코딩테스트를 통과하였다. 1차는 그리 어려운 알고리즘 없이 그저 구현하는 문제들이 출제 되었다.2차까지 잘보아서 꼭 붙었으면 좋겠다. 6개월동안 열정적으로 할 준비는 되어있다. 분명 대학원 입시에 도움이 되겠지?","link":"/2020/12/28/2020-12-28-first-post/"},{"title":"Introduction and Motivation","text":"Introduction and Motivation Machine learning의 목적은 data로 부터 valuable patterns을 뽑아내는 것이다Dataset에 맞게 Model을 설계, Model은 input과 output의 function을 describe 할 수 있어야 한다 A model is said to learn from data if its performance on a given task improves after the data is taken into account. Learning이란 data의 pattern이나 struct를 model의 parameter을 자동으로 optimize하며 찾아낸다. 머신러닝 시스템의 근본적인 이해를 위해 수학적인 기반은 매우 중요하다 Finding Words for Intuition Predictor : Input data에 맞는 prediction을 해내는 Algorithm Data as vectors vector as array (computer science view) vector as arrow with a direction and magnitude (physics view) vector as an object that obeys addition and scaling (mathmatical view) Model : input data-set과 비슷한 data를 만들어 내기위해 사용한다. 좋은 모델은 data의 hidden pattern을 뽑아낼 수 있고, 어떠한 일이 다음에 일어날지 예측또한 할 수 있다. Learning : 우리가 dataset과 model이 주어졌다고 가정하자. Model을 training하는 작업은 training data에 맞게 model의 parameter를 optimize하는 과정이다대부분의 training 방법은 산을 오르는 과정과 비슷하다. 이 산의 정상은 maximum score를 의미한다. 우리는 model이 UNSEEN DATA 에서 잘 동작하기를 원한다. 따라서 우리는 model이 이전에 마주친 상황과 다른 상황에 자주 노출시켜 주어야 한다. Represent data as vector 확률과 optimization view를 사용해 적절한 모델을 선정 다양한 numerical optimization 방법을 사용하여 model이 training에 사용된 data가 아닌 다른 data에서 잘 동작하도록 함에 초점을 맞추어 학습 {: .align-center} 출처 : Marc Peter Deisenroth, Mathmatics for Machine Learning,(Cambridge University Press)","link":"/2020/12/30/2020-12-30-ML&Mathmatics/"},{"title":"Linear Algebra","text":"Vector Geometric vectors우리가 흔히 중고등학교때 배운 vector의 개념 Polynomials다항식 또한 vectors이다. 두 다항식은 더할수있고 scalar배또한 할 수 있다.따라서 Polynomials are Instance of Vecors Elements of $\\Bbb{R}^n$$a,b\\in\\Bbb{R}^3$ 일때 $a+b = c \\in\\Bbb{R}^3$스칼라로 곱해도 속한다 Linear algebra는 이러한 비슷함에 초점을 둔다. {: .align-center} Vector space의 개념과 성질이 매우 ML에서 매우 중요하다.Vector space란 작은 Vector의 집합들끼리 서로 더하고 Scailing하여서 나온 집합이다. 2.1 Systems of linear equationsExample 2.1 {: .align-center}Product Nn을 위해 Resource Rm 이 필요한경우 제한량 : bj Product i 생산을 위해 필요한 Resource Rj의 양: aij Product i 의 생산량 : xi 위와 같은 선형 방정식을 간단하게 만들기 위해 matrix를 사용 {: .align-center}{: width=”50%” height=”50%”} {: .align-center}{: width=”50%” height=”50%”} 2.2 Matrix행렬의 기본법칙은 이미 선형대수학 시간에 학습하였으니 넘어가겠다. 2.3 Solving Systems of Linear Equation2.3.1 Particular and General Solution 해를 구하는 방법을 알기에 앞서 Linear Equation의 해에대하여 알아보자. 위의 행렬식을 보면 2개의 방정식과 4개의 미지수가 있다. 따라서 무수히 많은 근이 존재할 것이다. Ax = b 라는 compact한 식으로 나타낸뒤 해를 첫번째 column C1의 42 두번째 column C2의 8배로 나타낼 수 있다 따라서 이의 해는 $[42,8,0,0]^T$ 로 나타낼수 있다. 이러한 식은 무수히 많은 해중 단 하나의 해이기 때문에 이를==Particular solution==이라고 부른다. 이제 모든 해를 일반적으로 표현하기 위하여 수학적 techinque을 사용한다. 0을 더함으로서 양변의 equality는 보존됨으로, 0을 만들어 내보자. 세번째 column을 1,2번째 column의 합으로 표현해 보면 따라서 0 = 8C1+ 2C2- 1C3 +0C4 로 나타낼수 있고 이들의 계수인 [8,2,-1,0]^T^ 가 바로 0을 만들어 내는 Vector이다. 또한 이 해(Vector)의 scalar배수들 또한 같은 결과를 가진다. 따라서 이후 작성….. 출처 : Marc Peter Deisenroth, Mathmatics for Machine Learning,(Cambridge University Press)","link":"/2020/12/30/2020-12-30-ML&Mathmatics2/"},{"title":"Python의 자료형 &amp; Pythonic code","text":"오늘배운 내용들은 굉장히 방대하다 1. 자료구조특징이 있는 정보를 어떻게 저장하면 좋을까? 기본데이터 구조 스택과 큐 튜플과 집합 사전 (dictionary) Collection 모듈 1-1) Stack스택이란?나중에 들어온 데이터를 먼저 반환하도록 설계된 메모리 구조 LIFO (Last In First Out) Data의 입력 : Push Data의 출력 : Pop Stack with list object append()와 pop()을 사용하여 리스트로 스택을 구현가능 a.pop() 이라는 함수는 return이 존재하고 a 도 변화시켜줌 1-2) QueueQueue란?먼저들어온 데이터를 먼저 반환하도록 설계된 메모리 구조 FIFO(First in First out) Queue with list object append()와 pop(0)를 사용하여 리스트로 큐를 구현가능 1-3) TupleTuple이란? 값의 변경이 불가능한 리스트 선언시 [] 가 아닌 ()를 사용 tuple의 요소에 인덱스로 접근하여 변경시도시 오류 발생(할당이 안된다) 왜쓸까? 프로그램 작동중 변경되면 안되는 데이터를 저장할때 사용 ex) 학번 우편번호 이름 …. 사용자의 실수에 의한 에러를 방지함 (1)의 타입은 int tuple로 만들고 싶다면 (1,)라고 선언해주어야 함 1-4) SetSet이란? 값을 순서없이 저장, 중복이 안됨 set 객체의 선언으로 구현가능 123s = set([1,2,3,4,1,2,3,4])s = {1,2,3,1,2,3,4}#이런식으로 선언을 하면 중복이 사라짐 remove(), add(), update(), discard()로 값 수정가능 Set의 연산1234567891011s1 = set([1,2,3,4,5])s2 = set([3,4,5,6,7])s1.union(s2) s1 | s2 # s1과 s2의 합집합s1.intersection(s2)s1 &amp; s2 # s1과 s2의 교집합s1.difference(s2)s1 - s2 #s1과 s2의 차집합 1-5) Dict사전 (dictionary) 란? 데이터를 저장할 때 데이터를 구분지을 수 있는 값을 함께 저장 구분을 위한 데이터 값 : key key값을 활용하여, value를 관리함 123456789101112131415c = {&quot;a&quot; : 1, &quot;b&quot; :2}c.items() # tuple의 형태로 key와 value값들이 나옴c.values() # dict의 value값만이 list로 나옴c.keys() #dict의 key값만이 list로 나옴c[&quot;c&quot;] = 3 # dict에 추가&quot;k&quot; in c#false&quot;a&quot; in c#truefor k,v in c.items(): print(k) print(v) 1-6) Collections List, Tuple, Dict를 파이썬에서 모듈로 지원해줌 1-6-1) Deque queue와 stack을 동시에 지원함 List에 비해 시간복잡도가 낮아 빠름 (pop이 시간복잡도가 낮음) append() appendleft() Linked list의 특성을 지원함 기존의 list함수들도 모두 지원함 123456789101112131415from collections import dequed = deque()for i in range(5): d.append(i)#d = deque([0,1,2,3,4])d.appendleft(10)# d = deque([10,0,1,2,3,4])d.rotate(1)# d = deque([4,10,0,1,2,3])d.extend([1,2,3])# d = deque([4,10,0,1,2,3,1,2,3]) 1-6-2) Ordered dictDict와 달리 데이터를 입력한 순서대로 dict를 반환함 그러나 지금은 별로 의미가 없음 1-6-3) Default Dict12345678910d = dict()print(d[&quot;first&quot;])#error 발생#이럴때 default를 주면 됨from collections import defaultdictd = defaultdict(lambda : 0)d[&quot;first&quot;]# 0 ex) 하나의 지문에 몇개의 단어가 나오는지 세고 싶은 경우 1234from collections import defaultdictd = defaultdict(lambda : 0)for word in text: d[word]+=1 1-6-4) Counter Sequence type의 data element들의 갯수를 dict의 형태로 반환 1234567891011121314151617181920212223242526from collections import Counterc = Counter()c = Counter('gallahad')print(c)# Counter('a':3, 'l':2, 'g' : 1, 'd' : 1, 'h' : 1)c = Counter({'red':4,'blue' : 2})print(c)# Counter('red': 4, 'blue' : 2)print(list(c.elements()))# ['blue','blue','red','red','red','red']c = Counter(dogs = 10, cats = 8)print(c)# Counter('dogs':10,'cats':8)print(list(c.elements()))'# ['dogs','dogs','dogs','dogs','dogs','dogs','dogs','dogs','dogs','dogs',...,'cats']#Set의 연산들을 지원함c1 = Counter('allis')c2 = Counter('paul')print(c1-c2)print(c1.subtract(c2))# Counter('a' : 0, 'l' : 1, 'i':1,'s' :1, 'p' : -1, 'u' : -1)print((c1-c2)['a'])#0 1-6-5) Namedtuple tuple의 형태로 Data 구조체를 저장하는 방법 저장되는 data의 variable을 사전에 지정해서 저장함 12345from collections import namedtuplePoint = namedtuple('Point', ['x','y'])p = Point(x = 11, y = 22)p[0], p[1] 2. Pythonic code 파이썬 스타일의 코딩 기법 파이썬 특유의 문법을 사용하여 효율적으로 코드를 표현함 많은 코드들의 장점을 채용함 고급코드 작성시 많이 필요함 ex) 12colors = ['red','blue','green']result = ''.join(colors) split, join list comprehension enumerate, zip lambda, map, reduce generateor asterisk 왜 쓸까? 남들의 코드를 이해하기 위해 효율 : 단순 for loop보다 list가 더 빠르다 쓰면 간지난다 2-1) Split &amp; Join**string type값을 기준값으로 나눠서 List 로 return ** ex) split 12ex = &quot;python, java, javascript&quot;p, j, js = ex.split(,) #unpacking ex) join 123ex = ['python', 'java', 'javascript']s = '-'.join(ex)#s = 'python-java-javascript' 2-2) List Comprehension 기존 리스트를 사용하여 새로운 리스트를 만든다 파이썬에서 가장많이 사용된다 for + append 보다 빠르다 ex) 12result = [i for i in range(5) if i%2==0]#result = [0,2,4] ex) 12345word1 = &quot;hello&quot;word2 = &quot;world&quot;result = [i+j for i in word1 for j in word2]# result = ['hw','ho','hr'....., 'od'] 1234567891011121314151617case_1 = [&quot;A&quot;,&quot;B&quot;,&quot;C&quot;]case_2 = [&quot;D&quot;,&quot;E&quot;,&quot;A&quot;]result = [i+j for i in case_1 for j in case_2 if not(i==j)]# Filter: i랑 j과 같다면 List에 추가하지 않음# [i+j if not(i==j) else i for i in case_1 for j in case_2]words = 'The quick brown fox jumps over the lazy dog'.split()# 문장을 빈칸 기준으로 나눠 list로 변환#print (words)'''['The', 'quick', 'brown', 'fox', 'jumps','over', 'the', 'lazy', 'dog']'''&gt;&gt;&gt; stuff = [[w.upper(), w.lower(), len(w)]for w in words]# list의 각 elemente들을 대문자, 소문자 Two dimensional array가 필요 123456789101112131415case_1 = [&quot;A&quot;,&quot;B&quot;,&quot;C&quot;]case_2 = [&quot;D&quot;,&quot;E&quot;,&quot;A&quot;]result = [[i+j for i in case1] for j in case2]#뒤에께 먼저 작동하고 '''for j in case2: line = [] for i in case1: line.append(i+j) result = [['AD','BD','CD'], ['AE','BE','CE'],['AA','BA','CA']]''' 2-3) Enumerate &amp; ZipEnumerate1234567891011my_str = 'ABCD'd = {v:i for i,v in enumerate(my_str)}mylist = ['a', 'b', 'c', 'd']l = list(enumerate(mylist))#[(0, 'a'), (1, 'b'), (2, 'c'), (3, 'd')]{i:j for i,j in enumerate('Artificial intelligence (AI), is intelligence demonstrated by machines,unlike the natural intelligence displayed by humans and animals.'.split())}#문장을 공백을 기준으로 나누어 단어들의 list로 만들고 list의 index와 값을 unpacking하여 dict로 저장 Zip두개의 list의 값을 병렬적으로 ==tuple== 타입으로 추출하여 저장 123456math = (100,80,90)kor = (90,90,80)eng = (90, 80, 70)result = [sum(value)/3 for value in zip(math,kor,eng)]# [세학생의 평균점수가 저장됨] 같은 위치에 있는 값들!! 2-4) Lambda &amp; Map &amp; ReduceLambda 함수 이름없이 쓸수있는 익명함수 수학의 람다 대수에서 유래됨 ex) 1234567f = (lambda x,y : x+y)f(10,50)#60 값이 return 이됨up_low = lambda x: x.upper()+ x.lower()# s = up_low(&quot;my happy&quot;)# s = &quot;MY HAPPYmy happy&quot; 파이썬 3부터 권장되지 않음 def를 걍써라 문법이 어려움 함수 작동의 test 가어려움 docstring이 없음 다른사람들의 코드 해석이 어려움 이름이 존재하지 않는 함수가 생김 Map sequnce형 data가있을때 함수에 각각 적용을 해주고 그결과를 받는 기능 두개이상의 list에 적용가능, if filter도 사용가능 ex) 12345678ex = [1,2,3,4]f = lambda x : x**2l = list(map(f,ex))l = [f(value) for value in ex]# l = [1,4,9,16]f = lambda x,y : x*yl = list(map(f,ex,ex))# l = [1,4,9,16] Reduce map function과 달리 list에 똑같은 함수를 적용해서 통합 1234567891011from functools import reducex = reduce(lambda x,y : x+y, [1,2,3,4,5])'''1+2 = 33+3 = 66+4 = 1010+5 = 15따라서 x = 15의 값이 나온다''' 머신러닝에서는 여전히 Lambda, Map, Reduce를 많이 사용함 2-5) Iterable Object Sequnce형 자료형(list,tuple,문자열 등등)에서 데이터를 순서대로 추출하는 object 12for city in ['s','t','n']: print(city) 위와 같이 data를 순서대로 추출하는 내부적으로는 iter 과 __next___가사용됨 iter(), next() 함수로 한개씩 넘어감 123456789101112cities = ['Seoul',&quot;Busan&quot;,&quot;Jeju&quot;]# 이자체가 모두 memory에 올라가게 됨 그런데memory_iter_object = iter(cities)# 이렇게 해버리면 memory 다음위치들에 대한 주소값을 가져오게됨print(next(memory_iter_object))#Seoulprint(next(memory_iter_object))#Busanprint(next(memory_iter_object))#'Jeju' {: .align-center} 이런식으로 iterable object가 저장이 됨 list는 사실 cities라는게 포인터고 배열안에 주소값이 각각저장되어있어 각 index마다 주소값이 저장되어있음 이걸 iterable 한 object로 해주면 node가 다음객체의 주소값을 가지고있음 Generator(몰랐던거라 매우 중요)Generator iterable object를 특수한 형태로 사용하는 함수 element가 사용되는 시점에 값을 메모리에 반환 : yeild를 사용하여 한번에 하나의 element만을 반환 12345678910111213141516171819202122232425import sysdef general_list1(value): result= [] for i in range(value): l.append(i) return resultprint(general_list1(50))#[1,2,3,...,49]result = general_list1(50)sys.getsizeof(result)#520 def general_list2(value): result = [] for i in range(value): yeild i print(general_list2(50))# &lt;generator object&gt;for a in general_list2(50): print(a)#평소에는 memory에 실제로 값이 안들어가 있고 주소값만을 가지고 있음#print 호출시 주소값을 활용하여 yeild가 값을 줌# 따라서 generator를 사용하면 memory 사이즈를 줄일수있음, 대용량 data Generator Comprehension1234g = (n*n for n in range(500))print(type(g))#&lt;class generator&gt; 2-6) Function Passing Argument Keyword argument Default argument Variable-length arguments Keyword argument 함수에 입력되는 parameter의 변수명을 사용, arguments를 넘김 이름대로 함수에 parameter가 들어감 (순서가 X) 1234def print_somthing(my_name, your_name):print(&quot;Hello {0}, My name is {1}&quot;.format(your_name, my_name))print_somthing(&quot;Sungchul&quot;, &quot;TEAMLAB&quot;)print_somthing(your_name=&quot;TEAMLAB&quot;, my_name=&quot;Sungchul&quot;) Default argument굳이 넣어주지 않아도 default로 값을 설정해줄시 그값으로 생성이됨 1234def print_somthing_2(my_name, your_name=&quot;TEAMLAB&quot;):print(&quot;Hello {0}, My name is {1}&quot;.format(your_name, my_name))print_somthing_2(&quot;Sungchul&quot;, &quot;TEAMLAB&quot;)print_somthing_2(&quot;Sungchul&quot;) Variable Length arguments함수의 parameter가 정해져있지 않았다면 ? -&gt; asterisk(가변인자)를 사용 *args를 변수명으로 사용 {: .align-center} 위와 같이 사용가능 *args의 type은 tuple형태로 여러개의 값이 묶임 Keyword variable - length Parameter 이름을 따로 지정하지 않고 입력하는 방법 asterisk 2개를 사용하여 함수의 parameter를 표시 입력된 값들은 dict로 사용가능 가변인자는 오직 한개만 기존 가변인자 다음에 사용 12345678910111213141516171819202122def kwargs_test_1(**kwargs): print(kwargs) print(type(kwargs)) kwargs_test_1(first = 3, second = 4, third = 5)'''{'first' : 3, 'second' : 4, 'third' = 5}&lt;class dict&gt;'''def kwargs_test_3(a,b,c,*args,**kwargs): print(a+b+c+sum(args)) print(kwargs) kwargs_test_3(1,2,3,5,2,1, first = 3,second = 4,third = 5)'''13{'first' : 3, 'second' : 4, 'third' : 5}''' 2-7) AsteriskUnpacking a container123456789101112131415161718192021222324252627282930313233343536373839404142434445def astersk_test(a, *args): print(a,args) print(type(args)) astersk_test(1,*(2,3,4,5))#이때 tuple 1개의 변수가 들어간다고 생각하지만 *가 들어가서 tuple이 풀리게 됨# 따라서 astersk_test(1,2,3,4,5)라고 생각함astersk_test(1,(2,3,4,5))# 1 ((2, 3, 4, 5),)# &lt;class 'tuple'&gt;print(*[1,2,3,4])#1 2 3 4print([1,2,3,4])#[1, 2, 3, 4]a,b,c = ([1,2],[2,3],[3,4])print(a,b,c)data = ([1,2],[2,3],[3,4])print(data)print(*data)'''[1, 2] [2, 3] [3, 4]([1, 2], [2, 3], [3, 4])[1, 2] [2, 3] [3, 4]'''# zip을 활용data = ([1,2],[3,4],[5,6])for d in zip(*data): print(d)#(1, 3, 5)#(2, 4, 6)#만약 위에게 없으면data = ([1,2],[3,4],[5,6])a,b,c = datafor d in zip(a,b,c): print(d)# keyword unpackingdef ast(a,b,c,d): print(a,b,c,d)data = {'a' : 1, 'b' :2, 'c' : 4}ast(10,**data)#10 1 2 4 ==asterisk가 함수의 argument로 들어갔을때 unpacking이 일어난다== asterisk를 2개 사용시 keyword unpacking이라고 바꿔준다 새롭게 배운것 ==default dict== ==named tuple== ==reduce== ==generator== ==Variable Length arguments== ==Keyword variable - length== ==Asterisk==","link":"/2021/01/20/2021-01-20-Boostcamp3/"},{"title":"Day2","text":"VariableVariable &amp; Memory변수란? 변수 = 값 변수는 메모리 주소를 가지고 있고 변수에 들어가는 값은 주소에 해당됨 ex) A = 8 : A라는 이름을 가진 메모리주소에 8을 저장해라 ※ 폰노이만 구조 {: .align-center} Dynamic Typing코드의 실행시점에 타입을 결정 - 컴파일러 언어가 아닌 인터프리터 언어 연산자와 피연산자문자간에도 연산이 가능함 (concatnate) 데이터 형변환 float(), int()와 같은 함수로 데이터의 형변환 가능 List인덱싱 list 에 있는 값들은 주소값을 가짐 -&gt; 주소를 사용하여 값을 호출 슬라이싱 [시작 index : 끝 index : 간격] 리스트의 연산두개의 리스트 사이에도 concatnation이 가능 append : 리스트에 요소 추가 extend : 새로운 리스트 추가 insert : index에 요소 추가 remove : 리스트에 특정 요소 삭제 del : index를 주고 해당 index요소 삭제 12345a = [1,2,3,4,5]b = [5,4,3,2,1]b = a 이때 a,b는 앝은 복사를 하게되어 주소를 공유하게됨 따라서 값만 복사하고 싶을시 b = a[:] 123t = [1,2,3]a,b,c = t 이차원 리스트 이차원 리스트를 복사하는 법? 123import copycopy_score = copy.deepcopy(midterm_score) Function &amp; ConsoleFunction함수란? 어떤 일을 수행하는 코드의 덩어리 반복적인 수행을 1회 작성후 지속적인 호출로 대체 가능 코드를 논리적인 단위로 구분하여 정리해놓을수 있음 캡슐화 : 코드의 세부적인 내용을 모르고 인터페이스만 알아도 타인의 코드를 사용가능 12345678def calculate_rectangle_area(x,y): result = x * y return resultrectangle_x = 10rectangle_y = 20 print(calculate_rectangle_area(rectangle_x,rectangle_y)) 메인 프로그램 수행 함수호출 함수수행 &amp; return 다시 메인 프로그램 수행 return 값과 parameter에 대한 개념을 다시 상기 Console in/out어떻게 프로그램과 데이터를 주고 받을 것인가?? Print Formatting %string format 함수 fstring 1234print('%s %s' %('one','two'))print('{} {}'.format('one','two'))print('%d %d' %(1,2))print('{} {}'.format(1,2)) 1. %-format 1print(&quot;product : %s, price : %f&quot; %('apple',5.243)) %10s : 10칸 %10.1f : 10칸에 소숫점 1자리까지 출력 Conditional &amp; Loop조건문,반복문 - 대부분 생략is 연산은 memory의 주소를 비교!!! 값의 비교인 ==와 주소의 비교인 is는 다른 연산임 -5부터 256까지는 파이썬안에 정적 memory로 저장되어있음 12345a = -1b = -1print(a is b) 결과는 True but when 12345a = -6b = -6print(a is b) 결과는 False all() - 모두 true면 true 아니면 false any() - 하나라도 true면 true 아니면 false 삼항 연산자 12value = 12is_even = True if value%2 == 0 else False 1if __name__ == &quot;__main__&quot;: String and advanced function conceptStringstring 특징 string은 1byte의 크기로 한글자씩 memory에 할당됨 문자열 또한 2진수로 컴퓨터가 규칙에 의해 변환해서 저장 문자열은 list와 같은 형태로 data를 처리하기 때문에 slicing, indexing과 같은 특징들을 똑같이 가짐 문자열 함수 len(a) : 문자열 길이를 return a.upper() : 대문자로 변환 a.lower() : 소문자로 변환 a.capitalize() : 첫문자를 대문자로 변환 a.titile() : 제목형태로 (첫번째 띄어쓰기, 첫문자 대문자)로 변환 a.count(‘abc’) : ‘abc’가 들어간 횟수 return a.find(‘abc’) : ‘abc’가 들어간 offset return a.rfind(‘abc’) : same as find a.startswith(‘abc’) : ‘abc’로 시작하는지 여부 return a.endswith(‘abc’) : ‘abc’로 끝나는지 여부 return 1234print(&quot;It\\'s OK&quot;)a = &quot;&quot;&quot;Happy new year&quot;&quot;&quot; {: .align-center} Function 2Call by object reference Call by value 함수의 인자를 넘길때 값을 넘김 함수 내부에서 인자값 변경시 호출자에 영향을 안줌 Call by reference like C에서의 pointer 사용 swap구현할때 call by value가 아닌 call by refernce로 Call by object reference 파이썬은 객체의 주소가 함수로 전달되는 방식 이건 좀 특이한 경우 함수에 객체가 넘어갔을때 주소가 넘어가고 그 함수 안에서 새로운 객체 생성또한 가능 123456def spam(eggs): eggs.append(1)# 기존객체의주소값에[1] 추가 eggs =[2,3]# 새로운객체생성 ham =[0] spam(ham) print(ham)# [0, 1] Scoping Rule 변수가 사용되는 범위 지역변수 (local variable) 전역변수 (global variable) 123456789def test(t): print(x) t =20 print(&quot;In Function :&quot;,t)x =10test(x)print(t)# t는 함수안에서 사용하는 지역변수이기 때문에 함수 밖에서는 사용불가 함수 내부에서 전역변수 사용시 global 키워드를 사용하면 함수내부에서도 global variable 수정, 접근 가능 Recursive Function 자기자신을 호출하는 함수 피보나치 수열같은 점화식을 표현할때 사용한다. 재귀함수는 종료조건이 있어 종료조건 전까지 반복 수행 Function Type Hint 파이썬의 가장 큰 특징 : dynamic typing -&gt; 처음 사용하는 사용자가 interface를 알기힘든 단점이 존재한다 12def type_hint_example(name:str)-&gt;str: return f&quot;Hello, {name}&quot; Docstring파이썬 함수에 대한 상세 정보를 기입 1234def add(a,b): ''' 여기에 상세 정보를 기입 ''' vscode에서 python docstring generator를 깔면 쉽고 명확하게 docstring 작성 가능 Coding Convention명확한 규칙은 없지만 팀마다 코드를 잘 이해하기 위해서 서로서로 가독성이 좋게 작성해야한다 들여쓰기는 4space 나 tap중 1개로 통합하여 사용하자 또한 1줄은 79자를 넘어가면 안된다 불필요한 공백은 피하자 연산자는 1칸씩만 띄우자 소문자 l, 대문자 O, 대문자 I 금지 flack8이라는 모듈로 체크 가능","link":"/2021/01/19/2021-01-19-Boostcamp2/"},{"title":"Day1","text":"부스트 캠프의 첫날이 시작되었다. 첫날이라 앞으로의 계획이나 개발환경 setting이 주를 이룬 수업이였다. 오늘이 아마 일정이 널럴한 마지막날이 될것같다ㅎㅎ😵😵😵 Hidden class파일시스템OS에서 파일을 저장하는 트리구조의 저장 체계 Directory 폴더 또는 디렉토리로 불림 파일과 다른디렉토리 포함 가능 파일 컴퓨터가 논리구조를 저장하는 단위 읽기, 쓰기, 실행등의 작업 가능 절대경로 vs 상대경로경로 - 컴퓨터 파일의 고유한 위치, 트리구조상 노드 절대경로 : 루트 디렉토리부터 파일위치까지의 경로 상대경로 : 현재있는 디렉토리부터 타겟파일까지의 경로 터미널마우스가 아닌 키보드로 명령을 입력 프로그램 실행 마우스로 입력 : GUI 환경 키보드로 입력 : CUI 환경 Windows : CMD window, Windows terminal Mac, Linux : Terminal 기본 명령어 cls : 화면을 clear cd : change directory mkdir : make directory dir : 하위 디렉토리 목록 출력 del : 하나이상의 파일을 지움 copy : copy a b : a를 b에 복사해라 파이썬 개요플랫폼 독립적인 인터프리터 언어이다 운영체제 상관없이 작동되는 언어이다 모든 프로그램은 OS에 의존적인데 소스코드를 바로 실행할수있게 지원하는 프로그램 실행방법 인터프리터 vs 컴파일러컴파일러 소스코드 실행전에 컴파일러가 기계어로 먼저번역 (C,자바,C++) 인터프리터 별도의 번역과정없이 소스코드를 실행시점에 해석 (파이썬,스칼라) 객체지향의 동적 타이핑 언어객체지향 실행순서가 아닌 모듈중심으로 프로그램을 작성 동적타이핑언어 실행시점에 데이터에대한 타입을 결정함 이게 매우 편한것 같다. 자료형이 실수인지 문자열인지 따로 지정을 하지 않아줘도 되는 파이썬의 특징이다 이게 디버깅 과정에서 다른 정적타이핑언어에서보다 효율적이다. 첫날이라 정리할게 많지 않았다. 내일부터 다음주에있는 정기세션 발표준비와 병행해야 해서 좀 바빠질것 같다 :fire:","link":"/2021/01/18/2021-01-18-Boostcamp1/"},{"title":"Object-Oriented Programming","text":"Python Object-Oriented Programming만들어 놓은 코드를 재사용 하고싶다 생각해보기 주체들을 만들고 주체들의 행동들 데이터의 구조를 나누어서 코딩 개요객체 : 속성과 행동을 가지는 일종의 물건 속성은 변수로, 행동은 함수로 표현 oop는 설계도 (틀)에 해당되는 Class와 실제 구현체인 Instance로 나뉨 Class 구현알아두면 좋은 Python naming rule 파이썬의 함수와 변수명에서는 _로 띄어쓰기를 구분하는 snake case 를 사용 파이썬의 Class 명에는 띄어쓰기 부분에 대문자를 사용하는 camel case 를 사용 Attribute 추가하기 Attribute 추가는 __ init __과 함께 init은 객체 초기화 함수 파이썬에서 __는 특수한 예약함수나 변수, 함수명 변경(맹글링)으로 사용 ex) __ main __ , __ str __ , __ init __ Method 구현하기 반드시 self를 추가해야지만 class 함수로 인정이 됨 self : Instance 자신을 의미함 인스턴스 생성시 그 인스턴스 내부에서는 self로 불림 ex)123456789101112131415161718192021222324class SoccerPlayer(object): def __init__(self, name, position, back_number): self.name = name self.position = position self.back_number = back_number def __str__(self): return &quot;Hello, My name is %s. I play in %s in center &quot; % \\ (self.name, self.position) def __add__(self, other): return self.name + other.name def change_backnumber(self, new_number): print(&quot;선수의 등번호를 변경합니다 From : %d To : %d&quot; %(self.back_number,new_number)) a = SoccerPlayer('son','FW',7)b = SoccerPlayer('kain','FW',14)print(a)print(a+b)a.change_backnumber(9)# Hello, My name is son. I play in FW in center # sonkain# 선수의 등번호를 변경합니다 From : 7 To : 9 상속(Inheritance) 부모 클래스로 부터 속성과 Method를 물려받은 자식 클래스를 생성 하는 것 다른 클래스의 object에 부모클래스의 이름을 넣어주면 상속이됨 super() : 그것의 부모클래스를 그대로 받아와 부모객체를 사용할때 사용 super().about_me() : 부모클래스의 method 사용 Polymorphism 같은 이름의 method의 내부 로직을 다르게 작성 Dynamic 123456789101112131415161718192021class Animal: def __init__(self, name): # Constructor of the class self.name = name def talk(self): # Abstract method, defined by convention only raise NotImplementedError(&quot;Subclass must implement abstract method&quot;) class Cat(Animal): def talk(self): return 'Meow!' class Dog(Animal): def talk(self): return 'Woof! Woof!' animals = [Cat('Missy'),Cat('Mr. Mistoffelees'),Dog('Lassie')]for animal in animals:print(animal.name + ': ' + animal.talk())#굳이 cat과 dog의 함수이름을 다르게 할 필요없이 내용만 다르게 함 가시성 객체의 정보를 볼 수 있는 레벨을 조정하는 것 누구나 객체 안의 모든 변수를 볼 필요가 없음 @ 캡슐화 클래스를 설계시 정보은닉, 클래스간 간섭/공유 최소화 Product 객체를 Inventory 객체에 추가 Inventory에는 오직 Product 객체만 들어감 Inventory에 Product가 몇 개인지 확인이 필요 Inventory에 Product items 접근 허용 private 변수로 선언하는법 : self.__items = [] private 변수로 선언시 외부에서 접근이 불가능함 1234567891011121314151617181920class Product(object): passclass Inventory(object): def __init__(self): self.__items = [] def add_new_item(self, product): if type(product) == Product: self.__items.append(product) print(&quot;new item added&quot;) else: raise ValueError(&quot;Invalid Item&quot;) def get_number_of_items(self): return len(self.__items)my_inventory = Inventory()my_inventory.add_new_item(Product())print(my_inventory.__items)#접근이 안되고 에러가 뜬다#만약 써야된다면? 만약 외부에서 Private 변수를 써야하는 상황이 존재한다면? -&gt; property decorator를 사용 내부에서 접근해서 반환해주는 기능을 해줌 123456789101112131415161718192021222324class Product(object): passclass Inventory(object): def __init__(self): self.__items = [] def add_new_item(self, product): if type(product) == Product: self.__items.append(product) print(&quot;new item added&quot;) else: raise ValueError(&quot;Invalid Item&quot;) def get_number_of_items(self): return len(self.__items) @property def items(self): # 보통 아래처럼 외부에서 수정이 가능하게 반환하지 않고 copy본을 return해줌 return self.__items my_inventory = Inventory()my_inventory.add_new_item(Product())print(my_inventory.items)#이럼 접근이 됨#decorator은 함수명을 변수명처럼 쓸수있게 해주는!! Decorate이해하기 위한 개념들 First-class objects Inner function decorator First-Class objects일등 함수, 일급 객체 변수나 데이터 구조에 할당이 가능한 객체 파이썬의 모든 함수는 1급함수이다 파이썬의 모든함수는 파라메터로 전달가능 12345678def square(x): return x*xf = squaref(5)#이건 매우 특이하고 평소에 생각하지 않았던 것이다def formula(method, arg_list): return([method(value) for value in arg_list]) 이렇게 함수도 변수처럼 지정해서 사용할수도 있고 함수의 파라메터로 넣을수도 있구만 :scream: Inner Function 함수내에 또다른 함수의 존재 123456789101112def print_msg(msg): def printer(): print(msg) printer() #closuredef print_msg(msg): def printer(): print(msg) return printer 매우 흔한 구조임 Closure : inner function을 return 값으로 반환 1234567891011def star(func): def inner(*args, **kwargs): print(&quot;*&quot; * 30) func(*args, **kwargs) print(&quot;*&quot; * 30) return inner@stardef printer(msg): print(msg)printer(&quot;Hello&quot;) DecoratorModule and Project 파이썬은 대부분의 라이브러리가 이미 다른사용자가 다 구현해 놓음 이걸 불러와서 쓰는게 굉장히 큰 장점 남이 만든 프로그램 쓰는 법 객체 &lt; 모듈 ModuleOverview 어떤 대상의 부분 혹은 조각 ex)레고 블록 같은것들 프로그램을 나눈 작은 프로그램 조각들 Package : module의 모음 파이썬에서의 module == py파일 import ~~~ 해주면 됨 그럼 그안에 있는 모든 코드가 메모리 로딩이 일어남 같은 directory안에있어야됨 코드를 쉽게 로딩할수 있게 pycache라는 폴더가 생김 name space 모듈호출시 범위를 지정하는 방법 모듈안에는 함수와 클래스등이 존재가능 일부만 호출해주기 위해 From 과 import를 씀 모듈에서 특정함수나 클래스를 호출하기 되도록이면 특정 함수가 어디서 왔는지를 보여주기 위해 모듈명을 별칭으로 쓰는 방법을 쓰자 라고 생각 Package 다양한 모듈들의 합 다양한 오픈소스들이 패키지로 관리됨 폴더별로 __init.py 구성하기 현재 폴더가 패키지임을 알려주는 초기화 파일 없으면 package가 아님 하위촐더와 py파일을 모두포함함 import 와 all keyword사용 참고 : package namespace package내에서 다른폴더에있는 모듈을 부를떄 상대 참조로 호출 from ..sound.echo [.. 점이 2개 : 부모디렉토리기준] from game.graphic.render -&gt; 이건 절대참조 from . render -&gt; 현재 디렉토리 기준 오늘 배웠던게 정말 중요했던것 같다. 다른 문법들보다 이 package의 구성같은게 평소에 굉장히 모호했는데 이번에 확실하게 잡고 넘어 가야지.","link":"/2021/01/21/2021-01-21-Boostcamp4/"},{"title":"Python handling","text":"Exception &amp; File &amp;Log HandlingException Handling 프로그램 사용시에는 예상치못한 예외가 생김 예상이 가능한 예외 발생여부를 사전에 인지하여 개발자가 명시적으로 처리해주어야 함 예상이 불가능한 예외 인터프리터 과정에서 일어나는 예외 수행불가시 인터프리터가 자동호출 예외처리—&gt; Exception Handling Try except 문법ex) 0으로 나눌떄 exception 발생 123456789101112131415161718192021222324252627282930try: 예외발생 코드except &lt;type&gt;: 예외발생시 대처하는 코드 a = [1,2,3,4,5]for i in range(10): try: print(10 / i) print(a[i]) except ZeroDivisionError: print(&quot;Not divided by 0&quot;) except IndexError as e: print(e) else: print('hey') finally: print('wow')''' Not divided by 010.05.03.33333333333333352.52.01.66666666666666671.42857142857142861.251.1111111111111112''' 되도록이면 exception을 명확하게 잡아 가독성을 높혀라 else 구문을 사용할시 예외가 발생하지 않을시 실행 finally 예외 발생여부와 상관없게 항상 실행되는 구문을 추가 Raise 구문 필요에 따라 강제로 exception을 발생 123456while True: value = input(&quot;변환할 정수 값을 입력해주세요&quot;) for digit in value: if digit not in &quot;0123456789&quot;: raise ValueError(&quot;숫자값을 입력하지 않으셨습니다&quot;) print(&quot;정수값으로 변환된 숫자 -&quot;, int(value)) Assert 구문사전에 사용자에게 특정조건이 맞는지 조건을 넣어주고 False일때 error를 발생시켜줌 assert isinstance(decimal_number, int) File Handling파일의 종류 기본적인 파일종류로 text file, binary file로 나뉨 컴퓨터는 text file을 처리하기 위해 binary로 변환시킴 Binary file 이진법을로 나타내져 있음 메모장으로 열면 깨짐 엑셀파일 워드파일 등등 Text file 인간도 이해가능한 문자열로 이루어짐 메모장으로 열면 확인 가능 Python File I/O123f = open(&quot;같은 폴더의 파일&quot;, &quot;r&quot;)contents = f.read() r : 읽기 모드 w : 쓰기 모드 a : 추가 모드 with 구문과 함께 사용하기 인덴테이션이 일어나는 동안에는 with줄의 코드가 다 적용이됨 인덴테이션이 종료가 되면 close가 됨 로그를 남겨야할 필요가 있음 1234f = open(&quot;i_have_a_dream.txt&quot;,&quot;r&quot;)contents = f.read()print(contents)f.close() Logging 모듈print문이랑 비슷 Python data handling CSV- 웹(html) xml json CSV(Comma Separate Value) 엑셀 양식의 데이터를 프로그램에 상관없이 쓰기 위한 데이터 형식이라고 생각하면 쉬움 TSV,SSV등으로 구분해서 만들기도 함 notepad로도 열 수 있고, 쉼표로 구분이 되어있다. 위에서 data_header라는게 있다. 여기에는 데이터의 필드가 담겨있ㅇ며 데이터 저장시 ,로 분리를 하는 코드이다 첫번째 data는 무조건 data의 필드이다 ex) data,index이런 느낌으로 카테고리이다 따라서 첫줄이라면 ,로 나누어서 data_header라는 리스트에 저장해준다 이후 줄부터는 ,로 나누어서 한줄씩 리스트에 저장해준다 text파일 형태로 데이터 처리시 문장내에 들어가있는 “,” 등에 대해 전처리 과정이 필요하다 파이썬에서는 간단히 CSV파일을 처리하기 위해 csv객체를 제공함 12import csvreader = csv.reader(f, delimiter = ',', quotechar = '&quot;',quoting = csv.QUOTE_ALL) delimiter : 글자를 나누는 기준 (default : ‘,’ ) lineterminator : 중바꿈기준 (default : \\r\\n) quotechar : 문자열을 둘러싸는 신호 문자 (default : “ ) quoting : 데이터를 나누는 기준이 quotechar에 의해 둘러싸인 라벨 123456789101112131415161718192021222324import csvseoung_nam_data = []header = []rownum = 0with open(&quot;korea_floating_population_data.csv&quot;,&quot;r&quot;, encoding=&quot;cp949&quot;) as p_file: csv_data = csv.reader(p_file) #csv 객체를 이용해서 csv_data 읽기 for row in csv_data: #읽어온 데이터를 한 줄씩 처리 if rownum == 0: header = row #첫 번째 줄은 데이터 필드로 따로 저장 location = row[7] #“행정구역”필드 데이터 추출, 한글 처리로 유니코드 데이터를 cp949로 변환 if location.find(u&quot;성남시&quot;) != -1: seoung_nam_data.append(row) #”행정구역” 데이터에 성남시가 들어가 있으면 seoung_nam_data List에 추가 rownum +=1with open(&quot;seoung_nam_floating_population_data.csv&quot;,&quot;w&quot;, encoding=&quot;utf8&quot;) as s_p_file: writer = csv.writer(s_p_file, delimiter='\\t', quotechar=&quot;'&quot;, quoting=csv.QUOTE_ALL) # csv.writer를 사용해서 csv 파일 만들기 delimiter 필드 구분자 # quotechar는 필드 각 데이터는 묶는 문자, quoting는 묶는 범위 writer.writerow(header) #제목 필드 파일에 쓰기 for row in seoung_nam_data: writer.writerow(row) #seoung_nam_data에 있는 정보 list에 쓰기 위는 유동인구 데이터중 성남의 데이터만을 수집하는 코드이다 window에서 관리되는 코드는 cp949이다 vscode는 utf8이기 떄문에 encoding을 바꾸어 주어햐 한다 따라서 읽을 때 cp949로 읽는다고 별도로 지정한다 encoding은 cp949, utf8로 왠만하면 저장하기 왠만하면 작은 ‘ 이걸로 나누고 보통 csv는 다른도구를 사용하기 때문에 지금은 이거에 집착할 필요가 없다 왠만하면 pandas를 사용하기 때문에 Web 데이터 송수신을 위한 HTTP 프로토콜 사용 데이터 표시를 위한 HTML 형식 사용 HTML HTML의 모든 요소들은 꺾쇠 괄호안에 둘러 쌓여 있음 모든 HTML은 트리모양의 포함관계를 가짐 왜 HTML을 알아야 하는가? HTML도 일종의 프로그램으로, 규칙을 분석하여 데이터의 추출이 가능하다 추출된 데이터를 바탕으로 하여 다양한 분석이 가능 Regular Expression 정규 표현식 복잡한 문자열 패턴을 정의하는 문자 표현 공식 특정한 규칙을 가진 문자열의 집합 HTML역시 tag를 사용한 일정한 형식이 존재하기 때문에 정규식으로 추출하기가 편함 문법이 방대하여 그떄 그때 검색이 필요 기본적인것만 일단 숙지하자 XML 데이터의 구조와 의미를 설명하는 tag를 사용하여 표시하는 언어(like html) XML은 컴퓨터(PC &lt;-&gt; 스마트폰)간의 정보를 주고받기 매우 유용한 저장방식으로 쓰이고 있음 정규 표현식으로 Parsing이 가능함 그러나 가장 많이 쓰이는 parser인 beautifulsoup BeautifulSoup 언어 Scraping을 위한 도구 느리지만 간편하다 JSON javaScript object Notation 웹언어인 Java Script의 데이터 객체 표현 방식 간결하다 데이터 용량이 적고 Code로 전환이 쉽다 따라서 XML을 대체하고 있다 Python의 DIct Type과 유사, Key:Value값으로 접근가능 Json in Python대부분의 사이트에서 정보교환시 JSON을 사용한다 데이터 저장과 읽기는 dict type과 상호호환 가능","link":"/2021/01/22/2021-01-22-Boostcamp5/"},{"title":"Gradient","text":"Gradient관련 경사하강법경사 상습/경사하강법은 극값에 도달시 움직임을 멈춘다 Algorithmvar = init grad = gradient(var) while (abs(grad)&gt;eps): ​ var = var - lr *grad ​ grad = gradiwnt(var) 학습률 lr 을 조절하여 속도를 조절 eps : 종료조건 lr은 다룰 때 조심히 다루자 grad함수가 미분값을 구해주고 var값에서 미분값을 빼주어 그값에서 미분값을 또구한다 이렇게 계속 업데이트 이번 강의 부터 수학 강의에서는 수식이 많으므로 PDF 파일에 수식을 필기한것으로 정리를 대체하겠다","link":"/2021/01/26/2021-01-26-Boostcamp7/"},{"title":"Pandas &amp; 딥러닝 학습방법","text":"pandas 구조화된 데이터의 처리를 지원하는 파이썬 라이브러리 panel data -&gt; pandas 가로줄은 instance, row data table은 모든 data가 담겨 있는 sample column은 각각의 feature이름들, data.columns하면 이름들을 각각 지정해 줄 수 있음 feature vector, 각 feature에 해당하는 vector data는 각각의 값 head는 앞에 5개 출력 data_url지정 후 , pd.readcsv(data_url, sep=’\\s+’, header = None) header설정시 초기화시 column을 같이 초기화 해줄수 있음 이렇게 read_csv를로 읽어오면 type은 numpy임 SeriesColumn에 해당하는 데이터의 모음 index값이 있다 1234567891011121314151617181920212223import pandaslist_data = [1, 2, 3, 4, 5]list_name = ['a','b','c','d','e']#기존 데이터를 접근하는 인덱싱이 숫자나 문자로 지정을 할 수 있다example_obj = Series(data=list_data)example_obj'''0 11 22 33 44 5dtype: int64'''example_obj = Series(data=list_data,index =list_name )'''a 1b 2c 3d 4e 5dtype: int64''' subclass of ndarray dict타입을 series에 넣어주면, key값들이 index value는 value로 matching되어 생성됨 Series에 inedx로 접근할때 dict와 비슷하게 접근한다 astype으로 type변환 가능 Series.name = ‘number’라고 해서 series의 이름을 지정 가능 index.name도 지정가능 123456789101112131415dict_data_1 = {&quot;a&quot;: 1, &quot;b&quot;: 2, &quot;c&quot;: 3, &quot;d&quot;: 4, &quot;e&quot;: 5}indexes = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;, &quot;g&quot;, &quot;h&quot;]series_obj_1 = Series(dict_data_1, index=indexes)series_obj_1'''a 1.0b 2.0c 3.0d 4.0e 5.0f NaNg NaNh NaNdtype: float64''' Data FrameSeries data가 모여서 하나의 Data Frame을 이룸 여기서는 Column도 가능 접근 자체를 index와 columns로 함(행렬처럼 접근가능) 각 Colunm들은 type이 달라도 됨 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from pandas import Series, DataFrameimport pandas as pdimport numpy as np#Example from - https://chrisalbon.com/python/pandas_map_values_to_values.htmlraw_data = { &quot;first_name&quot;: [&quot;Jason&quot;, &quot;Molly&quot;, &quot;Tina&quot;, &quot;Jake&quot;, &quot;Amy&quot;], &quot;last_name&quot;: [&quot;Miller&quot;, &quot;Jacobson&quot;, &quot;Ali&quot;, &quot;Milner&quot;, &quot;Cooze&quot;], &quot;age&quot;: [42, 52, 36, 24, 73], &quot;city&quot;: [&quot;San Francisco&quot;, &quot;Baltimore&quot;, &quot;Miami&quot;, &quot;Douglas&quot;, &quot;Boston&quot;],}df = pd.DataFrame(raw_data, columns=[&quot;first_name&quot;, &quot;last_name&quot;, &quot;age&quot;, &quot;city&quot;])DataFrame(raw_data, columns=[&quot;age&quot;, &quot;city&quot;])df.first_namedf['first_name']'''0 Jason1 Molly2 Tina3 Jake4 AmyName: first_name, dtype: object'''#이렇게 dataframe내의 series에 접근 가능df.loc[1]# index의 값을 넣어줘서 행을 불러옴(index의 이름)df.iloc[:3]#index의 numbers = pd.Series(np.nan, index=[49, 48, 47, 46, 45, 1, 2, 3, 4, 5])s.loc[:3]'''49 NaN48 NaN47 NaN46 NaN45 NaN1 NaN2 NaN3 NaNdtype: float64'''#3이라는 이름을 가진 index까지만 뽑음df.debt = df.age&gt;40#boolean 값을 가진 series 주가 가능df.T#Transposedf.values#array 형태로 출력해줌df.to_csv()# csv파일로 저장가능 column을 삭제함 axis = 1 -&gt; column기준으로 debt를 삭제 del 을 사용하면 삭제할수 있음 거의 csv파일이나 json data Selection with column names1개 이상의 columns 추출 list꼴로 넣어주면 dataframe형태로 뽑힌다 fancy index, boolen index Map apply Lambda map과 lambda를 사용하여 series data에 lambda함수를 적용해준다 여기서 map을 dict type으로 데이터를 교체하기도 함 index는 그대로 s1을 따르고 dict에 key값에 해당하는 index에 value 값을 넣어준다 나머지는 Nan처리 Series + dataframe이것도 연산을 할 수 있다 덧셈시 add함수를 사용하여 axis를 잡아주어야 한다 그러면 broadcating이 일어난다 이렇게 처리해주면 숫자로 학습을 해야하는 딥러닝 과정에서 문자열 데이터를 숫자값으로 바꾸어 주어 계산이 가능해진다. 위와같이 함수를 하나 정의해서 map으로 처리해주면 된다 또한 apply라는 함수로 각 column에 대해서 함수연산을 해 줄 수도 있다 pandas2 Groupby SQL groupby명령어와 같은 split -&gt; apply -&gt; combine 과정을 거쳐 연산함 기존의 data에서 먼저 같은 종류의 index가 같은 것 끼리 묶어서 계속 연산을 진행하는것 df.groupby(&quot;Team&quot;)['Points'].sum() 위연산 실행시 Team이라는 column기준으로 Pointsfmf 모두 더해서 모두 같은 team이 같은값을 가지는애들끼리 point를 모두 더한다 한개 이상의 column을 묶을 수 있음 groupby의 결과(type은 series이다)로 묶인 series에 unstack을 하게 되면 dataframe으로 만들어줌 swaplevel()로 index의 level을 변경할 수 있음 ( 결과물만 바뀌어서 출력되는거지 , 원본은 바뀌지 X ) sort_index(level = 0) sort_values() df.groupby([&quot;Team&quot;,&quot;Year&quot;])['Points'].sum() 이결과로 여러 team과 year로 나누어진 index를 가진 series가 생성되는데 여기서 .sum(level) level값으로 index를 정해 연산을 취해 줄 수 있다. Grouped12345grouped = df.groupby(&quot;Team&quot;)list(grouped)#grouped는 generator의 형태로 list를 취해주어야 값을 확인 가능#해당하는 index를 기준으로 key:value 형태로 반환해서 저장해줌# key는 해당하는 index value는 dataframe의 형태로 만들어 준다 추출된 group정보에는 세가지 유형의 apply가 가능함 Aggregation : 요약된 통계정보를 추출해 줌 Transformation : 해당 정보를 변환해줌 FIltration : 특정 정보를 제거 하여 보여주는 필터링 기능 개별데이터에 이걸 적용할 수 있음 데이터의 변환과정에서 많이 쓴다 groped된 상태에서 group별로 연산을 가능하게 해준다 df.groupby(“Team”).filter(lambda x : len(x) &gt;=3) 딥러닝 학습방법신경망 각 행벡터인 o는 데이터 X와 가중치 W 그리고 biad항인 b의 합으로 표현된다 입력벡터인 X의 차원 = n x d 출력벡터인 O의 차원 = n x p d개의 변수로 p개의 선형모델을 만들어 p개의 잠재변수를 설명 X를 O에 Mapping한다고 생각하면 편하다 행렬을 생각하는 방식에서 행렬을 하나의 data를 다른 domain으로 변환하는 operator라고 생각했을때의 개념을 사용한 것이다 Softmax 출력 벡터 o에 exp를 취해주어 각 출력벡터에 대한 확률으로 나타내 준다 분류문제를 풀때 선형모델과 Softmax함수를 결합하여 예측한다 신경망은 선형모델과 활성함수를 합성한 함수이다 X라는 입력 벡터들에 가중치를 곱해주고 bias를 더해주어 activation functiond을 적용 위의 식은 2 layer의 가장 간단한 NN이다 활성함수란? 흔히들 요즘에는 ReLU함수를 사용한다 활성함수를 쓰지 않으면 선형모델과 딥러닝은 차이가 없기 때문에 중요한 요소이다 ex) sigmoid, softmax, ReLU, tanh, 다층 perceptron은 위의 H를 다시 다른 가중치에 곱하고 …. 이후의 과정을 반복한다 이론적으로는 2-layer 신경망으로도 함수를 근사할 수 있지만 층이 깊을수록 목적함수를 근사하는데 필요한 node의 숫자가 훨씬 빨리 줄어들어 효율적이다 층이 얇으면 wide한 신경망이 되어야 한다 역전파 수식전개","link":"/2021/01/27/2021-01-27-Boostcamp8/"},{"title":"확률론2","text":"Day10 : 확률론2 모수란? 통계적 모델링은 적절한 가정위에서 확률분포를 추정하는것이 목표! 데이터는 유한하기 때문에 근사적으로 확률분포를 추정할 수 밖에 없다 데이터가 특정확률분포를 따른다고 선험적으로 가정한 후 그분포를 결정하는 모수(parameter)를 추정하는 방법을 모수적 방법론이라고 한다 특정확률분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀌면 비모수방법론이다 데이터를 생성하는 원리를 먼저 고려하는 것이 원칙 !!! 데이터로 모수를 추정해보자! 표본평균 : 주어진 데이터의 산술평균 ​ N-1로 나누는게 조금 신기하고 다른 점 -&gt; 불편추정량 중요한 개념 통계량(표본분산, 표본평균)의 확률분포를 표집분포라부르며(sampling distribution) 이게 좀 신기한게 데이터들의 확률분포는 표본분포(sample distribution)이다 모집단의 확률분포가 정규분포를 따르지 않아도 sample의 갯수를 늘린다면 표본평균의 모집분포는 정규분포를 따른다 가능도의 직관적인 정의 : 확률분포함수의 y값 셀 수 있는 사건: 가능도 = 확률 연속 사건: 가능도 ≠≠ 확률, 가능도 = PDF값 수식은 같지만 변수가 다름 최대가능도 추정법 (MLE) 이론적으로 가장 가능성이 높은 모수를 추정하는 방법 중 하나가 바로 MLE다 가능도 함수 : 데이터가 주어진 상황에서 $\\theta$를 변형 시킴에 따라 변하는 함수로 이해 즉 조건부 함수와 비슷 그러나 $\\theta$에 대한 확률이 아닌 대소비교가 가능한 그냥 함수라고 생각을 하자 데이터 집합 X가 독립적으로 추출되었을 경우 로그가능도를 최적화 합니다 P(xi | $\\theta$)의 곱이 풀어서 쓰면 로그들의 곱이라 로그의 합으로 나타낼 수 있다 왜 로그가능도를 사용하나요?데이터의 숫자가 졸라 많아지면 컴퓨터의 정확도로는 Likelyhood를 계산하는 것이 불가능하다 따라서 데이터가 독립일 경우 가능도의 곱셈을 가능도의 덧셈으로 바뀌면 컴퓨터로 연산해서 최적화 가능 경사하강법으로 가능도를 최적화할 때 미분 연산을 사용하게 되는데, 로그 가능도를 사용하면 연산량을 O(n^2^)에서 O(n)으로 줄어든다 대개의 손실함수의 경우 gradient descent를 사용하므로, 음의 로그가능도를 최적화하게 된다. WHY 음의 로그가능도? -&gt; 손실함수를 최소화 해야하기 때문에 음의 로그가능도 사용 Ex1 : 정규분포독립적인 표본을 얻었을 때 최대가능도 추정법을 이용하여 모수를 추정하면 $\\theta$($\\mu,\\sigma$)에 대해 오른쪽 수식을 미분 유도과정을 손으로 도출해보기 Ex2 : 카테고리 분포 카테고리 분포이기 때문에 제약식이 생김 여기서의 모수는 1-d 차원 까지 값이 1또는0이될 확률 모두 더했을때 1이 되어야 하는 제약식이 생긴것 근데 이보다 먼저 가테고리 분포에 대한 이해를 해보자 베르누이 독립시행 동전을 100번 던졌다. 그 중 60번 나왔다. 앞면이 나올 확률이 p(모수)라고 하면 가장 이러한 사건이 일어날 가능도가 높은 p를 구해보면.. 일단 100번 던져서 60번 앞면이 나오는 확률 P = 100C60 p^60 (1-p)^40 이러한 확률을 p로 미분했을 때 0이 되는 p값이 P가 최대가 될 떄일 것이며 이것이 MLE로 추정되는 모수 p이다. 로그함수의 성질을 생각했을때 P의 증감은 f(p) =log(p^60 (1-p)^40)과 증감이 같다. 즉 f(p) 가 최대가 될때 P도 최대가 된다. df/dp = 60/p - 40/(1-p) = 0 , 즉 p = 0.6 일때 f’(p) = 0 이 되며 이것이 우리의 직관과 일치한다. (MLE 끝) 이게 이제 이항분포 전추정 : 0.5값을 가지고 싶어서 던졌는데 구분추정 : 신뢰구간 카테고리 확률분포에서 x데이터가 one hot encoding된 벡터구만 그래서 이렇게 표현하는것이였다 오른쪽 제약식을 만족하면서 왼쪽 목적식을 최대화 -&gt; 최대가능도 추정(MLE) 여기서도 라그랑주가….. 이거 수식전개 해보기 딥러닝에서의 최대가능도 추정법딥러닝 모델의 가중치를 $\\theta$(W1,W2,W3…,WL)이라 표기했을때 마지막 softmax vector은 카테고리분포의 모수(p1,p2…,pk)를 모델링한다 원핫벡터로 표현된 정답 레이블 y = (y1,y2….,yk)를 관찰데이터로 이용해 확률분포인 소프트멕스 벡터의 로그가능도를 최적화 할 수 있다. 확률분포에서의 거리 기계학습에서 유도되는 Loss function들은 모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포의 거리를 통해 유도된다 두확률분포사이의 거리 총변동거리 (Total Variation Distance, TV) 쿨백-라이블러 발산 바슈타인 거리 쿨백-라이블러 발산 (Kullback - Leibler Divergence)정의 졸라리 어렵다 뭔소리인가 도대체 이게 결국은 정답레이블을 P, 모델 예측을 Q라 두면 ? Log likelihood를 최대화 하는것과 P와 Q사이의 쿨백하이블러 발산의 최소화는 밀접하다?? 두개의 확률분포의 거리를 최소화 한다? 결국 결론은 !!!!!!!!!!!!! 딥러닝 기계학습에서 통계학적인 지식(MLE)들을 사용하여 Loss function을 최소화 시킬수 있다???? 피어세션에서 이번 강의에 대한 심도깊은? 토론을 하였다 Further Question 확률과 가능도의 차이는 무엇일까요? (개념적인 차이, 수식에서의 차이, 확률밀도함수에서의 차이) 확률 대신 가능도를 사용하였을 때의 이점은 어떤 것이 있을까요? 다음의 code snippet은 어떤 확률분포를 나타내는 것일까요? 해당 확률분포에서 변수 theta가 의미할 수 있는 것은 무엇이 있을까요? 123456import numpy as npimport matplotlib.pyplot as plttheta = np.arange(0, 1, 0.001)p = theta ** 3 * (1 - theta) ** 7plt.plot(theta, p)plt.show() 위의 코드를 보자 theta는 0-1 사이에서 0.001의 간격으로 값을 가진다 모든 theta의 합은 1로 이는 결국 확률이다 이번주말에 정리해야 될 것들 다양한 확률밀도 함수에대한고찰 확률과 가능도 가능도에서의 최대가능도 추정법 MLE의 적용 - 정규분포, 이항분포 gradient descent 수식전개 월요일날 발표 (기본적인 kaggle dataset) back propagation 수식전개 pandas 정리 시각화 도구","link":"/2021/01/29/2021-01-28-Boostcamp10/"},{"title":"확률론1","text":"Day9 : 확률론 맛보기 확률론 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고 있다. 손실함수의 작동원리가 데이터 공간을 확률, 통계적으로 해석 회귀분석에서 Loss function : L2 norm 예측오차의 분산을 최소화 Classification에서 Loss function : CrossEntropyLoss : 모델예측의 불확실성을 최소화 하는 방향 이들의 Loss를 최소화 하기위해서는 측정하는법을 알아야함 : 이게 바로 확률론 확률분포는 데이터의 초상화 데이터공간을 x*y라 표기하고 D는 이 데이터 공간에서 데이터를 추출하는 분포입니다 이산확률변수 vs 연속확률변수확률변수는 확률분포 D에 따라 이산형(discrete)와 연속형 (continous)확률변수로 구분하게 됩니다 데이터 공간이 아니라 확률변수의 종류에 초점 이산형 확률변수 확률변수가 가질수있는 모든 경우의수를 모두 고려하여 확률을 모두 더해서 모델링한다 연속형 확률변수 연속형 확률변수는 데이터 공간에 정의된 확률변수의 밀도(density)위에서의 적분을 통해 모델링한다 확률 분포에 따라 모델링 방법에 차이가있다 그래서 접근법이 달라야 한다 data를 기반으로 모델링 할때 전체 data X,Y 분포에서 P(x,y) -&gt; joint distribution 각각의 빨간 칸들에 대해서 파란점들을 COUNT한다 전체를 보면 연속확률변수 같지만 나누어 주면 이산확률분포로 생각할수 있게됨 확률 분포 D는 모르기 때문에 주어진 data의 결합분포로 원래 확률분포 D를 modeling 할 수 있다. 원래 확률분포의 D에 따라 P(x,y)의 분포가 결정되는게 아니라!!!! 원래 확률분포에 상관없이 결합분포는 무엇이든 될 수 있다. 이건 즉 modeling 방식에 따라 결정이 되는 것이다 이렇게 하는이유는 원래의 확률분포와 다르더라도 컴퓨터는 이에 근사를 시킬 수 있기때문에 P(x)는 입력 x에 대한 주변확률분포로 y에 대한 정보를 주지는 않는다 x에 대해서 덧셈이나 적분하면 y에대한 주변확률분포를 구할 수 있다. 지금다루는 확률분포에 관한 이야기는 작년 1학기에 들었던 확률변수론에서 자세하게 다루었다 물론 focus가 전전은 조금 다르지만 학문의 기초적인 base는 비슷하기 때문에 한번 봐야겠다 조건분 y = 1일때만 counting 해주어 각각의 x값에 대한 확률분포 이게 바로 조건부 확률분포 이건 입력x에 대한 y의 관계를 나타낼 수 있다. 조건부확률과 기계학습조건부 화률 P(y|x)는 입력변수 x에 대한 정답이 y일 확률 연속 확률분포일때는 확률이 아니라 밀도임 logistic regression에서 사용했던 선형모델과 softmax의 결합은 데이터에서 추출된 패턴을 기반으로 확률을 해석 분류문제에서 softmax(W$\\phi$ + b)는 데이터 x로부터 추출된 특징패턴 $\\phi$ (x)와 가중치 W를 통해 조건부 확률 P(y|x)를 계산한다 회귀문제의 경우 밀도함수라 조건부기대값 E[y|x]를 추정합니다 조건부 확률밀도 함수에 연속이라 적분 이산확률분포면 sum의 형태로 조건부 기댓값은 L2-norm을 최소화 하는 함수와 일치한다!!!! -&gt; 이건 이미 증명이 되어있는것 (증명과정이 필요할까?) 예측의 오차의 분산을 최소화하는 -&gt; 조건부 기댓값 항상은 아니지만 사용함 Expectation데이터를 대표하는 통계량 다른 통계적 함수를 사용하는데 이용하는 기댓값 학교에서 배웠던 수식과 여기서의 수식을 비교해보자 연속확률변수 : 밀도함수 이산확률 : 질량함수 딥러닝은 다층신경망을 사용하여 데이터로부터 특징패턴을 추출합니다 이특징 패턴을 학습하기 위해 어떤 손실함수를 사용할지는 기계학습의 문제와 모델에 의해 결정된다 Montecarlo Sampling이건 강화학습에서 매우 많이 써보았다 확률분포를 명시적으로 모를때에는 데이터를 이용하여 기대값을 계산 몬테카를로 샘플링 방법을 사용해야한다 몬테카를로 예제 구간의 길이가 2이므로 적분값을 2로나누면 결국 기댓값을 계산하는것이므로 몬테카를로 사용 문제는 샘플사이즈가 많아야 오차범위가 줄어든다","link":"/2021/01/28/2021-01-28-Boostcamp9/"},{"title":"Week2","text":"week2 간단 정리 가보자 가보자~ 선형회귀분석 여기서 중요한건 데이터들을 하나의 직선으로 근사하는것이다 정답 직선 -&gt; y 데이터들의 - &gt; x 결국 이거거등 x를 y에 근사시키는거 베타를 구하는게 선형회귀분석이지 만약 n&gt;=m 이건 결국 행이더 크다 -&gt; 식의 갯수가 변수의 갯수보다 많다 이럼 연방 푸는게 불가능 하지 따라서 y에 근접하는 y’을 찾는거다 따라서 !!!!! 정답 y와 y’의 차이가최소화 되게 -&gt; L2 norm을 이용해서 L2 norm을 최소화하는 B를 찾는것 이때 무어 펜로즈 역행렬 사용 해서 B를 구한다 하하하 이선형 회귀분석이랑 경사하강법이랑 무슨상관??????? 무어펜로즈 사용하기 시러서 경사하강법 쓰는거지요 왜 시를까? Reason : 지금은 선형만 하고있어서 무어랑 경사랑 상관없는데 이제 많은 실제 모델들은 비선형이다. 따라서 비선형에서도 사용 할 수 있는 경사하강법을 사용하는것!!!!!!!! 경사하강법 결국 가장중요한건 gradient vector이다 변수가 vector인 다변수 함수의 경우 편미분을 사용하여 진행 gradient 여기서 norm은 절대값을 대체하는 것 eps : 종료조건 왜냐? 완전이 grad가 0이되는건 컴이라 불가능함 var = init grad = gradient(var) while (norm(grad)&gt;eps): ​ var = var-lr*grad ​ grad = gradient(var) 이제좀 gradient descent의 큰그림이 보인다 결국은 어느방향으로 움직여야 함수값이 증가하는지 감소하는지를 알려주는게 gradient로 gradient를 계속해서 update하여 norm이 0에 가까운 값이면 이제 최솟값을 보장해주는 이런 느낌적인 느낌 ㅇㅋ 근데 이제 문제는 이를 행렬을 이용한 수식으로 증명하고 전개하는것이다 요게 이제 어려운 Point 이건 손으로 작성하겠다. Further Question 강의영상 03:47부터 소개되는 내용인, d-차원 벡터(베타)에 대한 그레디언트 벡터를 구하는 계산을 각자 직접 손으로 해보기 바랍니다! 그럼 이것도 손으로 계산해서 update (with ipad) 딥러닝 학습방법 이해하기딥러닝은 비선형 모델을 해석하는 것이다 그 전에 먼저 선형모델을 다루어 보자 신경망을 수식으로 분해 이게 가장 중요한 수식 다시 행렬 review를 해보면 X는 데이터들의 집합 xi는 i번째 data xij는 i번쨰 data의 j번째 변수값 다시말해 nxd 인 X행렬은 n개의 data를 가지고 각데이터는 d개의 변수를가짐. W는 data를 다른 차원에 mapping해주는 역할을 한다. 데이터에 가중치를 곱하고 bias를 더해주는 이런느낌 Softmax 분류문제를 풀때 선형모델의 출력과 softmax의 결합으로 특정벡터가 어느 클래스에 속하는지 알 수 있음 벡터를 확률로 변환하는 함수이다 ​ ​ ​ ​ 위와 같이 순차적으로 가중치와 bias를 계산한 값을 다음노드에 또 집어넣고 계속해서 순방향으로 전파 -&gt; forward propagation 학습할때는 back propagation을 사용 ​ ​ Back Propagation​ 선형회귀분석에서 B에 해당하는 gradient벡터를 계산해서 업데이트 했던것처럼 각층에 존재하는 parameter의 미분값을 계산해서 업데이트 각층의 가중치에 대한 gradient벡터를 계산하는 법 행렬들의 원소의 모든 개수만큼 경사하강법이 적용이 된다 한층일때는 목적식에 대한 gradient 벡터를 동시에 계산할 수 있지만, 딥러닝의 경우 gradient 벡터를 순차적(back propagation)으로 계산하게된다 ​ ​ 손실함수 –&gt; L일떄 우측을 계산 위층에 있는 gradient를 계산한 다음에 chain rule을 이용해 아래층의 gradient를 계산 ​ 이러하게 계속해서 각 가중치에 대한 gradient vector를 구한뒤 이들을 SGD를 사용해서 Loss function을 update시킨다 확률론 맛보기 회귀분석에서 손실함수로 사용되는 L2 norm은 예측오차의 분산을 최소화하는 방향으로 학습하도록 유도 Cross-Entropy는 모델예측의 불확실성을 최소화 하는 방향으로 학습하도록 유도 분산 및 불확실성을 최소화 하는 방법을 알아야 함 이산확률변수 VS 연속확률변수위의 개념이 결국은 가장 중요하다 이산은 경우의수를 다 고려해서 확률을 구함 연속은 밀도함수를 적분해서 확률을 구함 조건부 확룰 변수","link":"/2021/01/29/2021-01-29-week2/"},{"title":"Numpy &amp; Basic Math1","text":"Start Numpy큰 matrix를 파이썬으로 표현하기 위해 사용 그리고 memory를 효율적으로 사용하기 위해 반복문 없이 데이터 배열처리를 지원함 ndarrayimport numpy as np test_array = np.array([1,2,3,4],float) ndarry 와 list 의 차이점 numpy는 하나의 데이터 type만 배열에 들어갈수 있다 dynamic typing이 안됨 numpy는 차례대로 데이터가 바로 메모리에 할당되지만 python의 list는 주소값을 메모리에 저장하게 되어 2번을 거쳐서 값을 불러옴 따라서 값이 ndarray의 값이 같더라도 is 를 써보면 false가 나온다 list는 아님 array의 rankrank 0 : scalar rank 1 : vector rank 2 : matrix rank 3 : 3- tensor rank 4 : n-tensor dtype : numpy는 nbytesndarray의 object 메모리 크기를 반환함 ShapereshapeArray의 shape 크기를 변경함, element의 갯수는 동일 12345import numpy as nptest_matrix = [[1,2,3,4],[1,2,3,4]]np.array(test_metrix).reshape(2,4)np.array(test_metrix).reshape(-1,2).shape# (4,2) flatten다차원 array를 1차원 array로 변환 Indexing리스트와 달리 [0,0]과 같은 접근법을 허용한다 Slicingdata를 접근 할 때 slicing을 굉장히 많이 쓴다 이건 많이 써봐서 ㄱㅊ 123#건너 뛰는것도 씀arr[:,::2]#2칸씩 건너 뛰는 Creation Function123456np.arange(0, 10, 0.5)#array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. ,6.5, 7. , 7.5, 8. , 8.5, 9. , 9.5])np.zeros(shape = (10,),dtype =np.int8)np.empty(shape = (10,),dtype =np.int8)#empty함수는 memory에 빈공간이 잡힘 그안에는 쓰레기 값이 있음 identity단위행렬 (i행렬)을 생성함 eye대각선이 1인행렬, k값의 시작 index 지정가능 diag대각행렬 값을 추출함 random sampling데이터 분포에 따른 sampling으로 array를 생성 np.random Operation functionAxis기준이되는 dimension 축 12345678test_array = np.arrange(1,13).reshape(3,4)'''array([[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12]])'''test_array.sum(axis=1)#(array([10,26,42])) Concatenate numpy array를 붙이는 함수 np.concatenate((a,b),axis = 0) 값을 붙일때 2개 함수 concatenate,hstack,vstack 맞춰서 축을 늘리고 싶은때 newaxis 사용 이게 생각보다 익숙하지 않아서 연습이 좀 필요할듯 싶다 행렬의 dot product a.dot(test_b) Transpose test_a.T Broadcasting(element wise operation : array의 shape이 같아서 각각 연산) Broadcasting은 크기가 shape이 다를때 알아서 퍼져서 연산이 일어남 요런느낌 1234567891011test_matrix = np.arange(1, 13).reshape(4, 3)test_vector = np.arange(10, 40, 10)array([10, 20, 30])test_vector.reshape(-1, 3).T + test_vector'''array([[20, 30, 40], [30, 40, 50], [40, 50, 60]])'''# 이런 계산도 가능함 Numpy performance 연산의 속도는 list보다 훨씬 크다 (dynamic typing을 포기한 결과) 하지만 할당에서는 연산속도의 이점이 없음 Comparison123456789101112131415161718192021222324252627282930import numpy as npa = np.arrange(10)a&lt;4'''array([ True, True, True, True, False, False, False, False, False, False])'''np.any(a&gt;5)# 하나라도 true면 전체가 True다np.all(a&gt;5)#하나라도 false면 전체가 false이다#numpy 배열간의 shape이 동일할때는 element간의 크기를 비교np.logical_and(a&gt;0,a&lt;3)#조건의 andnp.logical_not(b)np.logical_or(a&gt;0,a&lt;3)# np.wherenp.where(a &gt; 5)#(array([6, 7, 8, 9], dtype=int64),np.where(a&gt;5,3,2)# array([2, 2, 2, 2, 2, 2, 3, 3, 3, 3])# argmax : 가장 큰값의 index 반환, argmin : 가장 작은값의 index반환a = np.array([1,2,3],[4,5,6],[7,8,9])a.argmax(axis = 0) Boolean Indexindex에 조건을 넣어주어 조건을 만족하는 해당하는 값만 뽑아옴 123456import numpy as npa = np.arange(10)condition = a&gt;5a[condition]# array([6, 7, 8, 9]) Fancy indexindex값을 넣어줌 123456789101112a = np.array([2, 4, 6, 8], float)cond1 = np.array([1, 1, 1, 2, 1, 1, 1, 3])cond2 = np.array([1, 1, 1, 2, 1, 1, 1, 2])a.take(cond1)#array([4., 4., 4., 6., 4., 4., 4., 8.])a[cond]#array([4., 4., 4., 6., 4., 4., 4., 8.])#다차원 리스트에 적용a = np.array([[1,2,3,4],[3,4,2,2],[0,0,0,0],[1,1,2,1]])a[cond1,cond2]#array([4, 4, 4, 0, 4, 4, 4, 2]) VectorWhat is vector 벡터는 숫자를 원소로 가지는 리스트 또는 배열 공간에서 한점을 나타낸다 (원점으로 부터 상대적 위치) Hadamard product (element product)성분곱이란 같은 shape에서의 각 element끼리의 곱 Vector의 덧셈 &amp; 뺄셈원점으로 부터가 아닌 다른 벡터로 부터 상대적 위치이동 뺄셈은 방향을 뒤집은 덧셈 벡터의 norm 어떠한 벡터의 원점에서부터의 거리 기호 밑 표현 norm은 임의의 차원 d에서 성립 L1 norm각성분의 변화량의 절대값 좌표명면에서 각좌표축을 따라 이동하는 거리의 합 : L2 norm**피타고라스 정리를 이용해 유클리드 거리를 계산 ** : 왜 다른 norm을 사용하나? norm의 종류에 따라 기하학적 성질들이 달라진다 원의 정의 : 원점에서 부터 거리가 r 인 점들의 집합 norm을 사용하여 두벡터 사이의 거리를 구하자 L1 norm을 사용하여 구한 벡터사이의 거리 L2 norm을 사용하여 구한 벡터사이의 각도 행렬행렬이란 행벡터를 원소로 가지는 이차원 배열 전치행렬 행렬의 이해벡터가 공간에서 한점을 의미한다면 행렬은 여러점들을 나타낸다 행렬의 행벡터 xi 는 i번째 data를 의미한다 xij는 x번째 data의 j 번째 변수를 의미 행렬의 곱셈i번째 행벡터와 j 번째 열벡터의 내적을 성분으로 가지는 행렬을 계산한다 numpy에서는 @ 연산을 활용한다 행렬도 내적이 있을까넘파이의 np.inner은 i번째 행벡터와 j째 행벡터 사이의 내적을 성분으로 가지는 행렬을 계산 수학에서 말하는 내적과 는 다른개념이다 행렬의 내적은 보통 두 행렬사이의 trace를 계산하게 되는데 numpy는 다르다 이렇게 transpose와의 행렬곱셈이 numpy에서의 innerproduct이다 행렬의 이해 2행렬은 벡터공간에서 사용되는 연산자(operator)로 이해한다 서로다른 data를 연결시키는 연산자 m차원 공간의 점인 x와 , n차원 공간의 점인 z를 연결해줄때 x라는 벡터에 A라는 행렬과 곱해주게 되면 새로운 Z라는 열벡터가 나온다 mapping하는 연산자!! 주어진 data에서 pattern을 추출하거나 data압축이 가능하다!!!!! 행렬을 사용하는 연산자 -&gt; Linear Transform(선형변환) 역행렬행과 열의 숫자가 같고 det가 0이 아니여야 한다 np.linalg.inv(x) 를 사용하여 주어진 행렬의 행의 갯수가 열의 숫자보다 많다면 -&gt; 주어진 행렬의 열의 갯수가 행의 갯수보다 많다면 -&gt; 유사역행렬을 사용하여 해중 1개를 계산 선형회귀 분석 (Linear regression)만약 변수의 갯수보다 식의 갯수가 많은경우 -&gt; 선형회귀로 푼다 유사역행렬을 구해서 선형회귀 방정식을 구한다 여러개의 점을 행렬로 표현후 계수벡터인 β를 곱해주게 되면 하나의 선으로 표현된다 어떠한 β를 쓸까를 구하는 -&gt; 선형회귀 분석 미지수가 더 많기 때문에 해를 찾는것은 불가능 따라서 최대한 가까운 선을 찾는다 -&gt; L2 norm을 사용하여 이 계수를 구하기 위해서는 2가지의 방법이 있다. 이때 Moore-Penrose로 구하려면 y절편값을 추가해주어야 한다. 공부를 마치고 궁금한게 너무 많았다 특히 선형대수학 파트에서 이다 무어펜로즈 역행렬을 이용하여 해를 하나 구하는 과정에서 이렇게 나와있는데 A가 R^n*m^ 인 차원에서 n은 식의 갯수 m은 미지수의 갯수 m이 n보다 크기 때문에 이 조건을 보면 왼쪽에 곱해주어야 항등행렬이 된다 근데 AxA+가 x가 될수있나? x가 벡터라 자리를 바꿔 곱해도 되는건가 무어 펜로즈를 다시한번 자세히 보면 조건이 생성되는 이유는 n과 m이 다를때 AA^T^ 와 A^T^A 중 하나는 역행렬이 존재하지 않는다 이 조건이 바로 m&lt;n이면 AA^T^ 의 역행렬이 존재하지 않고, m&gt;n이면 A^T^A의 역행렬이 존재하지 않는다 따라서 유사역행렬이 조건에 따라 저렇게 표현되는건 알겠는데 A+A나 AA+ 를 해주었을때 차원이 다르게 나올뿐이지 이게 진짜 I 가 하나는 안나오나? 컴퓨터로 돌려봐야겠다 그리고 뒤에 linear regression 부분도 너무 간단하게 알려주셔서 중요한 파트인데 더 따로 알아봐야겠다.","link":"/2021/01/25/2021-01-25-Boostcamp6/"},{"title":"딥러닝 기초","text":"베이즈 통계학 데이터가 추가되었을때 쓰는 인과관계에 대한 추론법 조건부 확률 베이즈 정리는 곧 조건부 확률을 이용하여 정보를 갱신하는 방법을 알려줍니다 결국 우리가 알고 싶은 것은 A라는 새로운 추가적인 정보가 들어왔을때 P(B)로 부터 P(B l A)를 계산하는 방법을 제공한다 실제로 그럼 어디에 쓸까 D : data , $\\theta$ : 모수 사후확률 : 데이터가 주어져 있을때 $\\theta$에 대한 확률 사전확률 : 데이터가 없는 상황에서 사전에 주어진 $\\theta$ 에 대한 확률 Likelihood : 현재 주어진 모수에서 어떠한 Data가 관찰될 확률 Evidence : Data 자체의 분포 ex) COVID의 발병률이 10%로 알려져 있을 때, 이 바이러스에 실제로 걸렸을때(조건부) 검진될 확률 : 99%, 오검진 확률 : 1% 이 때, 어떤 사람이 양성판정일때 정말로 이사람이 바이러스에 감였되었을 확률 발병률 : 사전확률 실제로 걸렸을 확률 : $\\theta$ 검진된 경우 : D 실제로 걸렸을 때 검진될 확률 : P(D l $\\theta$) : Likelihood 가능도와 사전확률이 주어져 있으므로, 사후확률 계산가능 그렇다면 Evidence의 계산법은???????? 따라서 $\\theta$를 부정했을때의 likelihood도 알아야 계산이 가능하다 조건부 확률의 시각화 데이터의 성격에 따라 1종오류을 줄이냐 2종오류를 줄이냐가 달라진다 Ex) 의료문제의 경우 False Negative : 질병이 아니다라고 했는데 실제로 질병일 경우 따라서 False Negative에 신경을 쓴다 사전 확률 : 질병에 걸릴 확률, 안걸릴 확률 민감도 : 걸린걸 걸렸다고 탐지 실제로 걸렸을 때 걸렸다고 할 확률 : 민감도 실제로 걸리지 않았을때 걸렸다고 할 확률 : 오탐 양성이 나왔을때 진짜 양성인 경우 : True Positive 음성이 나왔을때 진짜 음성인 경우 : True Negative 양성이 나왔을때 음성인 경우 : 1종 오류 음성이 나왔는데 질병에 걸린 경우 : 2종 오류 정밀도 : TP/(TP+FP) 베이즈 정리를 통한 정보의 갱신 베이즈 정리를 통해 새로운 데이터가 들어왔을 경우 앞서 계산한 사후확률을 사전확률로 사용하여 갱신된 사후확률을 계산가능 앞서 양성 판정을 받은 사람이 2번째 검진을 받았을 때도 양성이 나왔을때 진짜 COVID에 걸렸을 확률은?&gt;?????? 베이즈 정리를 활용하여 사후확률을 연속으로 계산해보면 정밀도가 화아아악 올라간다 이게 바로 베이즈 정리의 강점!! BUT!!! 조건부확률로 인과관계를 추론해서는 안된다!!!!!!!!!!! 인과 관계는 데이터 분포의 변화에 강건한 예측모형을 만들 때 고려해 주어야 한다 인과 관계를 고려했을 시에 예측도는 떨어질 수 있음 인과 관계를 알아내기 위해서는 중첩요인(confounding factor)의 효과를 제거하고 원인에 해당하는 변수만의 인과관계를 계산해야 한다 ex) 키와 지능 -&gt; 여기에 나이에 따른 효과를 제거하지 않는다면 키가 클수록 지능이 높다라는 결과가 나오게 됨 이러한 나이와 같은 중첩요인을 제거하는 것이 Main Point Ex) 위의 문제가 바로 아주 유명한 simpsons 역설 a와b의 중첩효과를 제거해야 됨 Z의 개입을 제거하기 위해 조정(intervention)효과를 사용한다??? 조정효과에 대한 개념이 조금 필요할듯 싶다 Deep Learning ; Historical Review사람의 지능을 모방하는 인공지능 AI &gt; data based Machine Learning &gt; Deep Learning (NN을 사용하는) Key components of Deep learning Data that model can learn Model how to transform data Loss Function that quantifies the badness of model Algorithm to adjust the parameters to minimize the Loss Function (최적화 알고리즘) Historical Review 1. AlexNet 224*224의 image data 분류대회에서 처음으로 Deep Learning을 사용하여 1등함 실제적으로 딥러닝의 성능이 입증이 되었던 사실 2. DQN Q -Learning의 function estimation에 NN을 추가하여 높은 성능을 이끌어냄 3. Encoder/Decoder NLP의 trend가 많이 바뀌었다 4. Adam Optimizer 우리가 optimizer를 선정할때 Adam을 그냥 쓰는 이유가 있을까? 결과가 잘나오는 이유가 있을까? 5. GAN Generate Model : Network가 data를 생성! 6. RESNET 왜 딥러닝이냐를 설명해주었다 Layer를 더욱더 깊게 쌓을수 있게 만들어줌 7. Transformer Attention is all you need!! 모든 기존의 RNN을 대체하였고, 이젠 CNN도 넘보고 있다 8. BERT (fine tuned NLP model) 일반적인 단어들로 model을 train한후에 내가 원하는 소수의 data에 fine tunning OPENAI의 GPT-3 9. Self Supervised Learning 이미지 분류와 같은 분류문제를 풀때 한정된 학습데이터로 model과 loss fun을 바꿔가는게 아닌 학습데이터 외에 라벨을 모르는 Unsupervised Learning 활용 SimCLR Neural Networks 단순히 Function approximate이다 gradient ascent 라면 reward를 키우도록 ! 이런게 되겠구만 step size $\\eta$ 행렬을 찾겠다 -&gt; 서로다른 차원의 선형변환을 찾겠다 이렇게 그저 연속된 선형변환은 하나의 선형변환으로 합쳐질수 있기 떄문에 층을 여러개 쌓는 이유가 사라진다 따라서 층 중간에 activation function을 넣어준다 -&gt; nonlinear transform 선형결합의 반복이 아닌 nonlinear transform의 결합 더 많은 표현력을 가지게 됨 존재성이 중요한게 아니라 표현력이 중요함 MSE는 제곱이라 data 사이에 error가 껴있을때 -&gt; 전체적인 NN이 망가질수도 있음 이렇게 항상 같은 Loss Function을 쓰는게 아니라 상황에 맞게 다른 값에 비해서 그값이 높기만 하면 그 index를 뽑는것 -&gt; 그래서 이걸 수학적으로 CE를 사용함","link":"/2021/02/01/2021-02-01-Boostcamp11/"},{"title":"Optimization","text":"1. Optimization 용어들의 명확한 정리가 필요 Concept of Optimization Generalization Under fitting vs Over fitting Cross Validation Bias-varience tradeoff Bootstraping Bagging and Boosting Generalization 일반화 성능을 높힌다? 일반화란 : Training error각 0 이라고 해서 Test error가 0인것은 아니기 때문에 좋은 generalization : network의 Test data 성능이 학습데이터와 비슷하게 나온다 Cross validation Training data에서 validation data를 나누어서 training된 모델이 validation data 기준으로 얼마나 잘 동작하는지를 판단 나누는 기준?????? 학습데이터가 적으면 안된다 따라서 Cross validation을 씀 학습데이터를 K개씩으로 나누어 하나씩 바꾸어가며 validation data로 설정하고 training과 validation을 반복진행 Test data는 저얼대 model 학습에 사용되어서는 안된다!! Bias-varience tradeoff 학습데이터에 noise가 껴있을때 Cost를 minimizing하는것은 3개로 decomposed 될수있다 bias varience noise bias와 varience는 trade off관계에 있다 Bootstrapping Any test or matric that uses random sampling with replacement 학습 data가 100개가 있으면 80개씩 random으로 뽑아서 모델을 여러개를 만들어서 하나의 입력에 대한 consensus를 보고 모델을 수정하는법 Bagging &amp; Boosting Bagging (Bootstrapping aggregating) Muitiple models are being trained with bootstrapping 학습 data가 100개가 있으면 80개씩 random으로 뽑아서(bootstrapping) 모델을 여러개를 만들어서 하나의 입력에 대한 consensus를 보고 모델을 수정하는법 Boosting 간단하게 모델을 만들어 testing후 안좋은 부분을 고쳐나가며 여러개의 model을 만든다 이들을 독립적인 모델이 아닌 이 모델들을 Sequential하게 합쳐서 하나의 strong learner를 만든다 Gradient Descent Method Stocastic (한번에 1개의 sample을 사용하여 gradient update) Mini batch (한번에 적당히 작은 batch size개수의 samples를 사용하여 update) S Batch (모든 data를 다 써서 gradient를 update) Batch gradient descent를 사용할 경우 step 한번에 모든 data에 대한 loss function을 계산해야 하므로 계산량이 터진다 이를 방지하기 위해 쓰는 것이 SGD (stocastic gradient descent), mini batch Batch보다 다소 부정확 할수는 있지만 빠른 계산속도로 인한 빠른 수렴속도 BATCH SIZE MATTERS!!! Large batch size converge to sharp minimizers Small batch size converge to flat minimizers —&gt; High Generalize performance 위의 그래프는 model의 training data와 testing data에 대한 loss function이다. 큰 batch size를 써서 sharp minimum 값을 가지게 된다면, 우리가 원했던 training fuction의 minimum에서의 testing function에서의 testing function의 값을 보면 최소점이 아닌 꽤나 큰값을 가진다. 이는 모델이 Generalize성능이 떨어진다는 이야기로 귀결된다. 하지만 작은 batch size를 써서 function들이 Flat Minumum값을 가지게 된다면, 꽤나 Generalize 성능이 좋다. 위의 논문 읽어보면 좋다고 추천해 주심 Automatic Differentiation SGD Momentum Adagrad RMSprop Adam … Momentum1번 gradient가 한쪽으로 흐르게 되면 이전의 gradient정보를 사용하여 이어가는 방향 ? 이런느낌 SGD와 달리 parameter업데이트시 gradient를 바로 사용하여 업데이트 하는게 아니라 a라는 term을 만들어 이전의 gradient 값을 반영해주는 term을 추가해 주었다. at+1 &lt;= Bat + gt Wt+1 &lt;= Wt - $\\eta$at+1 B가 momentum, at+1가 accumulation, a는 momentum을 포함하고 있어서 한번 흘러가기 시작한 gradient를 유지시켜줌 momentum을 사용하면 SGD에서 local minimum에 빠졌던 문제를 해결할수도 있다. Momentum으로 기존의 local minimum을 빠져나와 더 좋은 minimum으로 갈수도 있다는 것이다. NAG (Nesterov Accelerated Gradient) at+1 &lt;= Bat + $\\nabla$ L(Wt - $\\eta$Bat+1) Wt+1 &lt;= Wt - $\\eta$at+1 $\\nabla$ L(Wt - $\\eta$Bat+1) : a라고 불리우는 현재정보에서 그방향으로 한번가보고 (lookahead) 이를 포함해서 update momentum은 관성 : 따라서 값이 local minimum에 수렴하지 못하는 현상이 일어날 수도 있음 (관성을 가져서) NAG를 쓰면 convergance ratio가 좋다 Adagrad adapts the learning rate parameter가 변해왔는지 안변해왔는지를 보고 parameter를 업데이트 Sum of gradient squares -&gt; G G가 결국 계속커지기 때문에 W가 업데이트가 안되고 학습이 멈추는 현상이 발생 따라서 G의 문제를 해결하는게 뒤의 optimizer인 Adadelta Adadelta 엄청난 양의 memory가 필요 이를 해결하기 위해 감마, 1-감마 : exponential moving average(EMA) There is no learning rate in Adadelta!! RMSprop Adam adaptive moment estimation 2. Regulization For good generalization 학습을 방해하는게 요점 Overfitting 방지 이런거 종류 Early Stopping Parameter norm penalty Data augmentation Noise robustness Dropout Batch Normalization Early stopping 중간에 학습을 멈추어 validation data를 만드는 Parameter norm penalty weight가 작을수록 좋다? -&gt; function space내에서 부드러운 함수일수록 generalization performance가 높을것이다 Data augmentation 데이터의 개수를 늘리기 위해 label preserving data augmentation같은걸 사용 label이 변환되지 않는 선에서 data를 변환 Label smoothing 이러한 방법으로 dataset을 확장시켜 model을 training 해보면 성능향상이 뚜렸하다 Batch Normalization 내가 적용하고자 하는 statistics를 정규화 각각의 layer가 1000개의 parameter라면 각각의 parameter가 정규화되게 하는것 Internal feature shift를 줄인다???? -&gt; 논란이 많다 그럼에도 활용하면 일반적으로 성능이 많이 향상된다 하나하나를 활용하여 Normalize를 하면서 좋은 성능이 나는걸 선택 ㅋㅋ Further Question Regression Task, Classification Task, Probabilistic Task의 Loss 함수(or 클래스)는 Pytorch에서 어떻게 구현이 되어있을까요? 올바르게(?) cross-validation을 하기 위해서는 어떻 방법들이 존재할까요? Time series의 경우 일반적인 k-fold cv를 사용해도 될까요? TimeseriesCV Further Question 1 Pytorch 내부에서의 Loss function 구현 Regression Task의 Loss function torch.nn.L1Loss(size_average=None, reduce=None, reduction: str = ‘mean’) Measures the mean absolute error (MAE) between each element in the input x and target y . 내부적으로 L1Loss가 어찌 구현되어 있나 확인해 보자 내부적으로 reduction을 mean으로 설정시 우리가 알고있는 sum을 n으로 나눈 값을 loss로 사용(기본값 : mean) reduction을 sum으로 설정시 n으로 나누는게 사라진 그저 차이의 norm의 sum값 torch.nn.MSELoss(size_average=None, reduce=None, reduction: str = ‘mean’) Measures the mean squared error (squared L2 norm) between each element in the input xx and target yy . 위의 L1 loss와 다른점은 차이의 제곱 Classification Task의 Loss function torch.nn.BCELoss(weight: Optional[torch.Tensor] = None, size_average=None, reduce=None, reduction: str = ‘mean’) 내가 이 loss function을 사용했을때는 분류문제중, 2개의 label 사이에서 classification을 할때는 BCELoss를 사용하고 NN의 출력단에 sigmoid함수를 적용해주었다. nn.BCEWithLogitsLoss하지만 이 함수를 사용시 sigmoid가 내부적으로 포함되어있다 loss function을 보면 torch.nn.CrossEntropyLoss(weight: Optional[torch.Tensor] = None, size_average=None, ignore_index: int = -100, reduce=None, reduction: str = ‘mean’) Probabilistic Task의 Loss function torch.nn.NLLLoss(weight: Optional[torch.Tensor] = None, size_average=None, ignore_index: int = -100, reduce=None, reduction: str = ‘mean’) The negative log likelihood loss. It is useful to train a classification problem with C classes. Further Question 2 올바르게(?) cross-validation을 하기 위해서는 어떤 방법들이 존재할까요? 일정한 k개로 data를 나누어 그중에 하나를 validation data로 사용하는 -&gt; k-fold cv validation hyper paramter : 우리가 정하는 값 ex) lr, network 깊이, loss function 종류, 등등 cross validation으로 최적의 hyper parameter를 찾고 이걸 고정한 상태에서 전체 training data를 사용해서 학습을 시킨다 모형의 파라미터 추정에는 트레이닝셋을 사용하고, 하이퍼파라미터 설정에는 밸리데이션 셋을 사용합니다 Further Question 3 Time series의 경우 일반적인 k-fold cv를 사용해도 될까요? 시간의 정보를 가진 data를 기존의 k-fold cv를 사용하여 섞어 버린다면 , 과거와 미래가 뒤섞여 안된다 for time series data we utilize hold-out cross-validation where a subset of the data (split temporally) is reserved for validating the model performance. 이는 결국 training data set -&gt; validation data set -&gt; test data set 이 시간순으로 배열되야 한다는 뜻이다 Time dependency 현재의 시점에서 미래의 data를 예측하는 모델을 맞추는 데 사용 된 이벤트 이후에 시간순으로 발생하는 이벤트에 대한 모든 데이터를 보류해야합니다. 따라서 교차적으로 data를 바꾸어주는 K-fold 대신 hold-out cross-validation을 사용해야 한다 이는 결국 training data set -&gt; validation data set -&gt; test data set 이 시간순으로 배열되야 한다는 뜻이다 Arbitrary Choice of Test Set 만약 우리가 임의적으로 정한 test set에서 poor한 결과를 내었다면, 이는 전체적인 data에 대한 poor한결과가 아닌 그 특정한 독립적인 test set에의 poor한 결과이다 따라서 우리는 Nested Cross-Validation을 사용한다 그림을 보면서 Nested cv를 알아보자 Nested CV에는 오류 추정을 위한 외부 loop와 hyperparameter추정을 위한 Inner loop가 있다 내부루프는 train data set을 나누는 걸로 앞서 설명한일반적인 CV구조이다 이제 data set를 여러 train과 test set으로 나누는 외부루프가 추가되었고 각 분할된 오류의 평균을 구한다 Nested CV for Time series data Predict second half 데이터의 전반부 (일시적으로 분할)는 훈련 세트에 할당되고 후반부는 테스트 세트가 된다 validation data의 크기는 달라질수 있지만, 순서는 data의 시간 순서는 항상 test data set이 train보다 뒤에있어야 한다 이게 앞선 1의 time dependency를 해소시킨 것이다 Day Forward-Chaining predict second half의 단점은 hold-out dataset(앞서 말한 연대순)을 임의로 선택하게 되면 time dependency는 해결되었지만 Arbitrary Choice of Test Set을 해결하지 못하게 된다 따라서 앞서 말한 Nested CV와 같이 많은 train과 test data set을 만들어 이들의 오류값을 구해 평균을 내어준다 예를 들면 1일을 test set으로 간주하고 나머지를 train set으로 해주는 것이다","link":"/2021/02/02/2021-02-02-Boostcamp12.1/"},{"title":"CNN1","text":"CNN Convolution 연산 이해하기 지금까지 배운 MLP는 fully connected. 가중치 행들이 i번째 위치마다 필요해서 i가 커지면 가중치 행렬의 크기가 커지게 됨 우리가 이제부터 볼 Convolution 연산은 커널이라는 고정된 가중치 행렬을 사용하여 고정된 커널을 입력벡터에서 옮겨가며 적용 x라는 입력벡터 상에서 커널사이즈 만큼 움직여 가며 연산 다양한 차원에서의 Convolution ​ 2차원 Convolution 연산 입력을 kernal size에 맞춰서 입력위치에 해당하는 index만큼 옮겨다니면서, 성분곱을 연산하는 2D 이미지 다른 kernel을 적용하여 Convolution filter를 적용하면 kernel에 맞는 특성을 가지는 2D 이미지가 나온다 입력크기를 (H,W), 커널크기를 (KH, KW), 출력크기를 (OH, OW)라 할때 channel이 여러개인 2차원 입력의 경우 2차원 Convolution을 채널 개수만큼 적용한다 (2차원 이미지더라도, RGB가있어서 3 channel) 채널이 여러개인 입력인 경우 커널도 채널의 개수만큼 있어야 한다 채널이 여러개일때는 각커널을 적용한 각각의 채널의 결과를 더해준다 만약 출력의 channel을 늘리고 싶다면??? 커널의 개수를 여러개 만들면 된다. feature map의 채널 숫자를 늘리는 보통 이렇게 많이 사용한다 Convolution 연산의 Backpropagation Convolution연산은 모든 입력데이터에 공통으로 커널이 적용되기 때문에 역전파 계산시에도 convolution이 나오게 된다 Stack of Convolution MLP때와 마찬가지로 non-linear activation을 사이에 적용했다 연산을 정의하는 Parameter의 숫자가 중요 첫번째 Convolutional filter의 parameter수 : 5*5*3*4 = 300 개 두번째 Convolutional filter의 parameter수 : 5*5*4*10 = 1000개 Convolution NN CNN은 Convolution layer + Pooling layer + fully connected layer Convolution &amp; pooling layer : feature extraction fully connected layer : decision making 점점 뒤의 fully connected layer를 줄이는 추세 reason : parameter의 수 우리가 일반적으로 우리의 모델의 parameter 숫자가 늘어날수록 학습이 어렵고 generalize performace가 떨어진다 따라서 CNN은 parameter수를 줄이는데 집중한다 어떤 뉴럴네트워크에 대해서 parameter숫자를 계산해보자 Stride &amp; Paddingskip Convolution Arithmatic 우리가 사용하는 kernel을 계산해보면 일단 3*3의 width 와 height이며 kernel의 channel은 입력의 channel과 같아야 하므로 128이다 따라서 하나의 kernel의 size = 3*3*128이다 이제 이 kernel의 갯수를 찾으려면 output의 channel인 64이다 따라서 총 parameter의 개수는 3*3*128*64 = 73728이다 이제 padding과 stride는 parameter 수와는 연관이 없다 ex) 사실 alexnet은 network가 2 path로 나누어짐 일단 첫번째 layer의 kernel = 11x11x3 = 363 이게 48개 있으므로 parameter개수 = 17424개 원래는 이제 96짜리 channel을 만들었어야 되는데 2개로 나누어서 48 channel로 만들어줌 따라서 총 parameter수 = 34848 kernel = 5x5x48 = 1200 이게 128개 그리고 총 2개 있으니 -&gt; 1200x128x2 = 307k kernel = 3x3x128 = 1152 이게 2개 -&gt; 2304 이게 192개 그리고 총 -&gt; 2304x192x2 = 884k 똑같은 방법 -&gt; 663k 쭉쭉 그러다가 Fully connected layer의 parameter 개수 13x13x128x2x2048x2 = 177M 16M 4M 보면 dense layer에서 parameter숫자가 너무 커진다 결국은 parameter를 줄이기 위해서는 convolution layer를 깊게 쌓고 뒤의 dense layer를 최대한 줄이는 방향으로 발전하고 있다 1x1 convolution 여기서 parameter수를 계산해보면 1x1x128x32 = 4096 demension을 줄인다!!! 깊이는 깊어지지만 parameter수를 줄이는 역할을 한다 e.g) bottle neck architecture Modern Convolutional Neural Networks ILSVRC에서 우승하거나 좋은 성능을 거둔 model들에 대한 parameter 개수, depth 등등 AlexNet ILSVRC Imagenet Large-Scale Visual Recognition Challenge 1000 different categories over 1 millions images AlexNet gpu의 성능이 부족해서 한번에 계산이 안되서 2개로 나눠서 따로 training을 시킴 Receptive field : 하나의 kernel이 볼수있는 이미지 level에서의 영역은 커짐, 그러나 parameter가 늘어나게 됨 5 Convolutional layer 3 Dense layer Key idea use ReLU function (non-linear func, 마지막 slope가 1이라 gradient가 사라지거나 네트워크를 망칠 확률이 적음) preserve properties of linear model overcome the gradient vanishing problem 이전에 많이 활용하던 tanh나 sigmoid는 값이 크면 output의 gradient가 0에 가깝게 나온다 GPI implementation (2 GPU) Overlapping Pooling, Local response normalization Data augmentation Dropout 지금 보면 별로 대단한게 아니지만, 그당시에는 혁신적인 방법 일반적인 standard 를 잡았다! VGGNet Increasing depth with 3x3 convolution filter 1x1 convolution filter Dropout (p=0.5) VFF16,VGG19 Why 3x3????kernel size가 커지면서 가지는 이점 : Receptive field가 커진다 ex) 3x3을 2번 하게 되면 output의 1개의 값은 input의 5x5를 보게된다 -&gt; 이게 바로 Receptive field 3x3을 3번 하게 되면 output의 1개의 값은 input의 6x6을 보게된다 따라서 3x3을 2개 사용하는 것과, 5x5를 1개 사용하는 것은 receptive field의 관점에서는 같다 따라서 이둘의 parameter의 개수를 비교해 보면 (chaneel : 128) 3x3 2개 : 3x3x128x128x2 = 294k 5x5 1개 : 5x5x128x128 = 409k 따라서 3x3 2개를 쓰는게 parameter의 숫자 감소 측면에서 이득이다 왜이런일이 일어날까? 사실상 3x3x3 = 27, 6x6 = 36 | 3x3 = 9, 5x5 = 25 이런 맥락이다 뒤의 대부분을보면 kernel은 7x7을 벗어나지 않는다 GoogLeNet 보면 전체 network 안에 작은 network 구조들이 반복되고 있다 (network in network) Inception block 활용 하나의 입력에 대해서 여러개의 receptive field를 가지는 filter를 거치고 이들을 concatenation 하지만 그보다 중요한게 중간중간에 추가로 들어간 1x1 Conv 3x3x128x128 = 147456 1x1x128x32 = 4096, 3x3x32x128 = 36864 -&gt;합은 : 40960 parameter 수가 1/4로 줄었다 —-&gt; 사용하는게 이득이다!!! 과연 AlexNet,VGGNet, GoogLeNet 중 parameter수가 작은것은? AlexNet(8 layer) : 60M VGGNet(19-layer) : 110M GoogLeNet(22 layer) : 4M ResNet Deeper neural networks are hard to train Overfitting is usually caused by an excessive number of parameters Identity map x와 output의 차원을 맞춰주기 위해서 1x1 convolution으로 사용하는것 Convolution 연산과 batch norm의 순서?????? 더 자세한 ResNet의 구조???? Bottleneck architecture3x3의 연산을 하기 전에 channel 수를 줄이게 되면 parameter의 숫자를 줄일수 있지 않을까? DenseNet ResNet을 바라보게 되면 그냥 두개의 값을 더하지 말고 concatnate시키면 되지 않을까? 계속 concatnate하면 channel이 기하급수적으로 커지기 때문에 이를 해결하기 위해 중간에 1x1 conv를 해줌","link":"/2021/02/03/2021-02-03-Boostcamp13.1/"},{"title":"RNN1","text":"RNN Sequence Data &amp; Model 소리, 주가, 문자열 등의 데이터를 시퀀스 데이터로 분휴합니다 시계열 데이터는 시간순서에 따라 나열된 데이터로 시퀀스 데이터에 속한다 독립동등분포 가정을 잘 위해하기 때문에 순서를 바꾸거나 과거정보에 손실이 발생하면 데이터의 확률분포도 바뀌게 된다 Markov model : first order autoregressive model 이들의 문제를 해결하기 위해 Latent autoregressive model hidden state가 과거의 정보들을 summerize한다 다루는 법 조건부 확률을 이용(과의 정보를 가지고 미래를 예측 ) 바로직전까지의 정보 S-1를 사용해서 현재인 S를 업데이트 반드시 모든 과거의 정보를 가지고 업데이트 하는 것은 아니다 따라서 조건부에 들어가는 데이터의 길이는 가변적이다 고정된 길이인 $\\tau$만큼의 시퀀스만 활용하는 경우 Autoregressive Model(자기회귀모델)이라고 부른다 직전과거의 정보랑 직전정보가 아닌 정보들을 Ht로 묶어서 활용 길이가 가변적이지 않고 이제 고정되기 때문에 여러가지 장점을 가지고 있다 사실은 과거의 모든 정보를 고려하기가 힘든 문제점을 고쳐서 이제 이전의 정보를 요약하는Ht를 예측하는 모델 —-&gt; RNN RNN 이해하기 기본적인 모형은 MLP와 유사하다 RNN의 역전파는 잠재변수의 연결그래프에 따라 순차적으로 계산한다 Back Propagation Through Time BTTP를 살펴봅시다BTTP를 통해 gradient를 계산해보면 미분의 곱으로 이루어진 항이 계산이 된다 길어지면 계산이 불안정해짐으로(gradient vanishing과 같은)문제가 있기 때문에 길이를 끊는것으로 truncated BPTT Gradient vanishing 문제의 해결?? 시퀀스 길이가 길어지는 경우에는 BTTP를 통한 역전파 알고리즘의 계산이 불안정해 지므로 길이를 끊는것이 중요하다 ex) LSTM, GRU ….. RNN을 시간순으로 쭉 풀면 결국 fully connected layer network가 된다 가장어려운 ? 단점 ? —-&gt; 하나의 fixed rule로 이전의 정보들을 summerize하기 때문에 먼 과거의 정보들이 현재에서 살아남기가 힘들다!! 이게 short term dependencies 결국 먼 과거의 정보들은 많은 양의 activation function과 W곱의 결과로 vanishing or exploding되는 현상이 일어나게 된다 LSTM LSTM의 전체적인 구조 들어오는 입력이 3개 나가는게 3개 실제로 나가는건 ht (hidden state)","link":"/2021/02/04/2021-02-04-Boostcamp14.1/"},{"title":"Computer Vision application","text":"Computer Vision application Fully Convolutional Network 기존의 CNN 구조 : Fully Convolutional Network : Dense layer를 없앴다. - &gt; convolutionize 결국 input과 output은 같다 parameter도 같다 flat을 해서 dense layer를 거치나, convolution을 거치나 같다 ex) 4x4x16 이 였다면 이걸 256개의 vector로 flatten 시킨다 ​ FCN을 보면 4x4x16에 똑같은 크기를 가진 kernel을 적용한다 parameter 4x4x16x10 = 2560 4x4x16x10 = 2560 같다 이런짓이 convolutionization 왜 이런걸 할까???? image segmentation 관점에서 생각을 해보자 Fully convolutional network가 가지는 가자은 특징은 바로 input dimension, 특히나 input dimension의 spacial dimension이다 Transforming fully connected layer into convolutional layers enables a classification to output a hitmap output이 커지게 되면 이거에 비례해서 뒷단의 spacial dimension이 커지게 됨 Because of convolution이 가지는 shared parameter의 성질 때문에 원래는 출력의 hitmap은 input보다 크기가 줄어들기는 하다 그렇지만 hitmap이라는 가능성이 생겼구나!!! 그래서 어떠한 inputsize에도 돌아가지만 이러한 작아진 output을 다시 input size만큼 늘리는 방법 Deconvolution(convolution transpose) 간단하게 생각하면 convolution 연산으로 줄어든 결과를 convolution의 역연산으로 다시 늘려주는 근데 convolution의 역연산이 존재하나? 불가능하다 근데 그냥 간단하게 이렇게 생각하는게 편하다 deconvolution은 결국 convolution이후에 결과에 padding을 많이 줘서 여기에 같은 size에 kernel을 적용하는것이다 Dectection R-CNN takes and input image extract around 2000 region proposal (using selective search) computes features for each proposal (size는 모두 똑같이 맞추어 준다, use AlexNet) classifies with linear SVMs SPPNet RCNN의 문제 : 2000개를 뽑으면 2000개를 전부 CNN에 돌려야 되기 때문에 시간이 졸라 많이 걸린다 따라서 일단 image 안에서 bounding box를 뽑고 image 전체를 CNN에 돌리고, 해당하는 위치의 tensor만을 활용하자 Fast R-CNN selective search로 bounding box를 얻고 전체 이미지를 CNN에 통과 그리고 ROI pooling layer를 통해 각각의 region에 대해서 feature를 뽑는다 그리고 마지막에 Fully connected layer을 통해 bounding box를 어떻게 움직이면좋은지, label이 뭔지를 알아낸다 Faster R-CNN Bounding box를 뽑아내는 Region proposal도 학습을 하자! selective search를 하지말고 이것도 학습을 하자는 의미 이 network를 region proposal Network가 추가됨 Region proposal Network 이미지의 특정영역이 bounding box로서 의미가 있는지 없는지만 찾아주는 Network 여기서 필요한게 Anchor box 미리 정해놓은 Bounding box의 크기 : anchor box 어떤 크기의 물체가 있을것 같다? 해당하는 영역의 이미지에 물체가 들어있을지 안들어있을지가 해당하는 영역에 물체가있을지에 대한 정보를 알고있다 9개의 region size중에 1개를 골라서 얼마나 bounding box를 늘이거나 줄일지 , xy에 off set을 줘야 해서 4개의 parameter 해당 box가 쓸만한지 아닌지 9x(4+2) = 54만큼의 channel이 나오는 YOLO faster R-CNN보다 훨씬 빠름 Region에 해당하는 sub tensor을 분류하는게 아니라 그냥 image를 때려 넣으면 딱 나오기 떄문에 빠름 no exclusive bounding box sampling (region proposal) YOLO에 대해서는 추가로 정리해서 따로 posting 하겠다","link":"/2021/02/03/2021-02-03-Boostcamp13.2/"},{"title":"Generative Models","text":"Generative Models What I can not create, I do not understand https://deepgenerativemodels.github.io/ Introduction What does it mean to learn a generative model generative model은 단순히 생성모델이 아니다 Suppose we have some images of dogs We want to learn a probability distribution p(x) such that Generation : If we sample xnew ~ p(x), xnew should look like a dog implicit models Density estimation :p(x) should be high if x look like a dog (어떤이미지의 확률을 계산함) 이건 마치 image classification explicit models Unsupervised representation learning 특정 image가 어떤 특징을 가지고있는지를 학습 How can we represent p(x)?????? Bernoulli distribution D = {Heads, Tails} Specify P(X = Head) = p, P(X = Tails) = (1-p) Categorical distribution ex) Modeling and RGB joint distribution (r,g,b) ~ p(R,G,B) number of case = 256x256x256 parameters = 255x255x255 개가 필요 하나의 RGB pixel만해도 parameter를 표현하려면 어마어마한 숫자의 parameter가 필요하다 Structure Through IndependenceWhat if X1,….,Xn are independent and binary pixels p(x1,…,xn) = p(x1)p(x2)…p(xn) possible state : 2^n^ parameter : n개만 필요 만약 각각의 pixel이 독립적이라고 가정한다면 이렇게 parameter수가 줄어든다 근데 이건 너무 말이 안된다 따라서 Independence와 fully dependent사이의 절충안??? Conditional IndependenceThree Important Rule n개의 joint distrubution을 n개의 conditional distribution으로 바꾸고 z가 주어졌을때 x,y는 independent하다 -&gt;이게 가정 완전 xy가 independent한게 아니라 z가 주어졌을때 y는 상관이없다 이런느낌 Conditional IndependenceUsing the chain rule 이 수식 도출에서 어떠한 수학적인 가정이 없이 chain rule만으로 구한 수식이다 따라서 fully independent와 parameter 개수는 같다 p(x1) :1개 p(x2|x1) : 2개 (one per for p(x2|x1 = 0) and p(x2|x1 = 1)) p(x3|x1,x2) : 4개 Hence 1+2+2^2^+…+2^n-1^ = 2^n^-1 i+1번쨰 pixel은 i번째 pixel에만 dependent하다 가정 : markov assumption 그 중간에 있는 걸 conditional independence를 잘 활용해서 중간의 parameter값을 얻어냈다 Auto-regressive Model suppose we have 28x28 binary pixels goal : p(x) = p(x1,x2….,x784) how can we parametrize p(x) use chain rule to get joint distribution p(x1:784) = p(x1)p(x2|x1)p(x2|x1:2)…… 이게 바로 auto-regressive model (i번째 pixel이 1~i-1까지 모든 history에 dependent한) 가장 중요한게 순서를 매기는 과정 이미지에 순서???? —-&gt; 순서에 따라 성능이나 방법론이 달라질수 있다 NADE : Neural Autoregressive Density Estimator p(xi|x1:i-1) = i번째 pixel을 1~i-1에 dependent하게 만든다 —–&gt; dependent 하다 ? 1-i-1번째 pixel값을 입력으로 받고 network를 통과시켜서 나온 output에 sigmoid를 통과해서 확률이 나오도록하는것 neural network의 weight의 차원값은 지속해서 늘어남이전입력들이 계속해서 늘어나기 때문에 NADE is explicit model Suppose we have 784개의 binary pixel 알고있는 값들을 집어넣은뒤 계산하게 되면 확률값이 나옴 Density estimate : 확률적으로 무언가의 확률을 explit하게 계산한다 Continous한 r.v를 modeling할때는 Gaussian이 사용이 된다 Pixel RNN Use RNNs to define an auto regressive model 이전에 봤던 NADE는 dense layer을 사용함 하지만 Pixel RNN은 RNN을 통해 generate한다 ordering의 순서에 따라 Row LSTM Diagonal BiLTM Latent Variable ModelsVariational Auto-encoder Is an autoencoder generative model?? autoencoder은 input을 재정의하는 과정이지 generative model은 아니다 과연 무엇때문에 Variational Auto-Encoder은 generation 모델인가? Variational inference (VI) The goal of VI is to optimize the variational distribution that best matches the posterior distribution posterior distribution : observation이 주어졌을때 내가 관심있어하는 r.v의 확률분포 posterior distribution을 계산하는건 매우 힘들기 때문에 Variational distribution을 근사한다 KL divergence를 사용해서 Variational distribution과 Posterior distribution의 차이를 줄여보겠다 How? 원해는 KL divergence를 줄이는게 목적이지만 이게 불가능하기 때문에 ELBO라고 불리는 term을 최대화 한다 ELBO can further be decomposed into Reconstruction Term x라는 입력을 latent space로 보냈다가 Decoder로 돌아오는 Reconstruction loss를 줄이는 term Latent space에 올려놓은 점들이 이루는 분포가 Latent space의 prior distribution와 비슷하다? implicit한 model Decoder이후의 output domain의 값들이 generation result이다 Auto encoder은 이게 아니라 generation model이 아니다 Key limitation Interactable model (hard to evaluate likelihood) reconstruction term은 상관없는데 KL divergence를 사용한 prior distribution에는 무조건 미분이 가능한 distribution (like Gaussian)을 사용해야 한다. 따라서 diverse한 latent prior distributions에는 사용을 하기에 힘들다 In most cases, we use an isotropic Gaussian Adversarial Auto-encoder It allows us to use any arbitrary latent distributions that we can sample Prior fitting term을 gan을 사용하여 분포를 맞추어줌 sampling이 가능한 어떠한 분포도 맞출수있다는 장점이 있다 GAN discriminator가 점차 발전해 나가면서 generator도 따라서 성능이 올라가는 상생의? GAN vs VAE GAN의 Objective For discriminator where the optimal discriminator is For generator GAN의 objective는 나의 true generative distribution과 내가 학습하고자하는 generator사이의 Jenson-Shannon Divergence를 최소화 하는것이다 DCGAN Info-GAN 학습시에 class라는 random한 one-hot vector를 매번 집어 넣어준다 generation시에 gan이 특정모드에 집중할 수 있게끔해준다 Text2Image 텍스트로 이미지를 generate하는 연구 model이 매우 복잡하다……. CycleGAN 이 cycle consistency loss가 매우 중요하다","link":"/2021/02/05/2021-02-05-Boostcamp15.1/"},{"title":"RNN2","text":"Transformer Sequential Model 위와 같은 문제로, RNN같이 sequential한 문제들을 해결할 때, 중간에 단어가 빠지거나 하면 해결하기가 어려움 —&gt; 여기서 나온게 Attention을 사용한 Transformer Transformer Transformer is the first sequence transduction model based entirely on attention RNN처럼 재귀적인게 아니라 based on attention from a bird’s-eye view, this is what the Transformer does for machine translation tasks 결국은 sequence to sequnece (불어 -&gt; 영어 )machine translation 입출력 sequence 는 숫자가 다를수 있다 모델은 하나임. 100개가 들어가도 100번 재귀적으로 들어가는게아니라 한번에 n개를 처리 generation할때는 1단어씩 만들게 된다 동일한구조를 가지지만 공유하지 않는 encoder와 decoder가 stack되어있다 encoder가 바뀔수 있는 n개의 단어를 어떻게 처리하는지 1개의 encoder에 n개의 단어가 한번에 들어간다 self attention + Feed Forward Neural Network -&gt;next encoder Self- Attention is the cornerstone of Transformer Transformer","link":"/2021/02/04/2021-02-04-Boostcamp14.2/"},{"title":"Improving Language Understanding by Generative Pre-Training","text":"이번에는 openai에서 발표한 논문인 GPT를 review해보겠다 GPT3는 이전에 review한 transformer구조를 활용하여 Language understanding을 효과적으로 만들었다. Abstract 자연어를 이해는 text추론, 질문에 대한 대답, 의미의 유사성 평가, 문서분류등을 포함하고 있다. 라벨링 되지 않은 text들을 매우 넘처나지만, 특정 task의 학습을 위해 labed된 text들은 매우 적기때문에 좋은 모델을 학습시키는것은 매우 힘들다. Language 모델을 unlabled된 text로 generative pretrain을 한이후 각각의 task에 맞게 fine-tunning을 하였다. 이러한 많은 unlabed text를 사용하여 학습하였다. 이전의 연구와는 달리,필요한 task에 fine-tuning하여 응용하는 것이 매우 효과적이다. 1. Introduction Raw text를 사용하여 효과적인 NLP 학습을 하기위해서는 지도학습에 대한 의존성을 완화해야 한다. 많은 딥러닝 방법들은 labeled된 data를 사용해야 해서 한계가 존재한다. 이러한 상황에서 unlabed된 data는 시간과 노력이 필요한 annotation을 모으는 작업들을 대체할 수 있다. 만약 고려가능한 지도가 가능한 상황이라면, unsupervised 방법은 model의 성능을 증가시킬 수 있다. 이러한 방식은 pretrained된 word embedding을 사용하여 성능을 높이는것과 비슷한 이유이다. unlabed된 data로 word-level의 정보보다 많은 정보를 활용하는것은 2가지 이유에서 매우 어렵다 어떠한 종류의 optimization objective가 가장 효과적으로 text를 표현할수 있을까 가 매우 unclear하다 우리가 원하는 특정 task에 효과적으로 적용하는 방법에 대한 의견이 일치가 되징 않았다. 현재 존재하는 방법은 model에 특정한 task-specific한 변화를 가하는 것과, 복잡한 학습방법,그리고 학습을 도와주는 몇몇 learning objective들을 넣어주는, 이러한 방법들의 combination이다 이러한 불확실성은 language processing에서의 효과적인 semi-supervised learning을 발전시키기 힘들게 만든다. 이 논문에서는 unsupervised pre-training과 supervised fine-tunning을 조합한 방법을 사용하여 semi-supervised approach를 하였다. 목적은 가장 보편적으로 학습하여 약간의 응용으로 다양한 분야에 적용시키는것이다. 2-stage로 나누어 train하였다 초기 parameter를 학습하기 위해 unlabeled data를 사용하여 pre-train 하였다 / with transformer 우리는 이 parameter들을 특정한 task에 맞는 supervised objective 학습에 사용하였다. 또한 model에서 Transformer를 사용하여 long-term dependencies를 해결하였다. 2. Related WorkSemi-supervised learning for NLP 우리의 work는 Semi-supervied learning의 범주안에있다. 이 ssl은 sequence labeling, text 분류등에 쓰이면서 큰 관심을 받고 있다. 가장 초기에는 unlabeled data를 supervised learning의 feature로 사용하여 word나 phrase level의 통계를 계산하는데 사용되었다. 최근 몇년동안 word-embedding이 얼마나 좋은지 밝혀냈다. 이러한 접근은 word-level의 정보를 특정한 high-level에 맞추어 준다. 최근에는 word-level이 아닌 phrase나 sentence level의 embedding을 사용하여 text를 다양한 target task의 vector representation을 나타내 주었다. Unsupervised pre-training Unsupervised pre-training은 supervised learning을 바꾸는거 보다는 좋은 initialization을 찾는게 목적이다. 각각의 연구들은 image classification과 regression task의 기술이 사용되었다. Pre-training은 정규화 과정에서 generalization성능을 올려준다. 우리의 연구는 language modeling으로 model을 pre-train한후 task에 맞게 fine-tuning해주는 것이다. Pre-training이 언어적인 정보를 잘 잡아낼수 있지만,이전연구에서 사용된 LSTM을 사용하는 것은 긴 data를 해석하지 못한다는 단점이 존재한다. 따라서 우리는 Transformer를 사용하였다.또다른 연구에서는 몇몇 보조적인 feature들을 삽입해주어 성능을 향상시켰지만, 이는 새로운 parameter의 증가를 야기한다. 우리의 GPT는 transfer과정에서 최소한의 수정만을 필요로 한다. Auxiliary training objectives 여러 보조적인 unsupervised training은 semi-supervised learning의 대채적인 형태이다. 이전의 연구에서는 다양한 종류의 보조적인 NLP방법론(POS tagging, chunking,등등등)을 사용하였다. 최근 또다른 연구는 보조적인 language model를 추가하여 sequence labeling의 성능향상을 이야기 하였다. 3. Framework학습과정은 2개의 stage로 나누어져 있다 unlabeled된 큰 말뭉치를 사용하여 가장 범용적인 language model을 학습하는 stage 이후 labeled data를 사용한 fine-tuning stage 3.1 Unsupervised pre-training unsupervised의 token들 = 이 주어지고, 이어지는 likelihood를 maximize하기위해 보편적인 language model을 사용한다. k는 context의 size이고, conditional prob P는 NN을 사용하여 modeled 이들은 모두 SGD를 사용하여 training했다. multi-layer Transformer decoder를 사용했다. 이 model은 input context tokens에 multi-headed self-attention을 활용하였고, 이후에 position-wise feedforward layer를 적용하여 target token에대한 output distribution을 구한다. U는 token의 context vector이고, n은 layer의 숫자, We는 token embedding matrix, Wp는 position embedding matrix이다. 3.2 Supervised fine-tuning model을 train한후, supervised target test에 맞추어서 parameter를 적용한다. labeled된 dataset C(각각은 input token의 sequence로 이루어짐 ((x^1^,…,x^m^) and label y )) Input은 pre-trained된 model을 통과하여 최종 transformer block의 activation인 hl^m^을 얻어내고, 이후에 linear output layer에 Wy와 함께 들어간다. 이는 이후의 objective를 maximize하게 한다. 보조적인 장치로 language modeling을 사용하여 fine-tuning을 하는것은 (1) generalization성능을 높힌다 (2) 수렴속도를 높힌다. 우리는 아래의 objective를 optimize한다 Fine-tuning중에 유일한 extra parameter은 Wy와 구분token을 위한 embedding이다. 3.3 Task-specific input transformations text classification가 같은 몇몇 분야에서, 위에서 묘사했던대로 우리의 model을 fine-tune할 수 있었다. 질의응답과, textual entailment와 같은 문제에는 input을 ordered sentence pairs, triplets of document, question, answer으로 해주었다. 우리의 pre-trained model이 연속적인 sequence에서 학습되었기 때문에, 이러한 문제들에는 약간의 맞춤 수정이 필요하다. 이전의 연구들은 transffered representation위에 특정 architecture를 삽입하는 형태로 학습해왔다. 이는 많은양의 cutomization이 필요하며 이러한 추가적인 특정 architecture에는 transfer learning을 사용하지 않았다. 대신 우리는 traveral-stple approach(input을 정렬된 sequence로 만들어)를 사용하여 우리의 pre-trained model이 학습할 수 있게 하였다. 이러한 input의 조정은 문제상황에 따라 architecture의 큰 수정을 하지 않아도 되게 한다. 모든 transformation은 randomly initialized된 start,ending token을 포함한다. Textual entailment(문장의 포함관계) : 전제 p와 가설 h 중간에 delimiter token $를 삽입하여 합쳐주었다. Similarity (문장의 유사도 평가) : 두개의 비교대상은 순서가 딱히 없다. 한마디로 동등한 level에서 비교해야 되기 때문에 모든 가능한 순서를 사용하고 transformer이후에 나오는2개의 hl^m^ 을 합쳐준다. Question Answering and Commonsense Reasoning (질의응답) : 3. Model Atchitecture language model -&gt; label이 필요가 없다 주어진 단어들을 가지고 다음단어를 예측하는 Generative model Generative model data가 많아 질수록 정확도가 높아진다 Discriminative model 타이타닉같은 데이터가 많지 않을때 패턴파악이 쉬워서 많이들 사용한다 한정된 data에 과적합 되기가 쉽다 sample된 data로는 왜곡된 판단을 할 수 있다 GPT는 unlabeled된 data로 Pretraining LM finefuning 데이터만 task관련데이터로 학습 model은 그대로 Naural Language Inference -&gt; entailment contradiction파악 질의응답 비슷한 문장 판별 주어진 문장을 그룹으로 분류하는 비지도 학습 label이 있는 data로 fine tunning한다. 기존 language model 학습 공식과 같다 transformer의 decoder로 구성 layer추가없이 pretrained LM byte pair embedding을 사용하였다 신조어 오탈자에 약한 word embedding이 아닌 byte pair. —–&gt; hack,able, deep, learn, ing 이런식으로 embedding을 하였다. data가 주어졌을떄","link":"/2021/02/14/2021-02-14-GPT/"},{"title":"Sequence to sequence with Attention","text":"Sequence to sequence \\ Seq2Seq ModelEx) Are you free tomorrow? 서로 paramter를 share하지 않는 2개의 별개의 RNN model을 (보통 LSTM) 쓴다. 각각의 RNN을 Decoder, Encoder로 사용한다. Encoder의 마지막단의 output을 vertorize 시켜준후 decoder의 input에는 SOS token, hidden state에는 encoder의 output을 넣어준다. Seq2Seq with Attention앞에서의 RNN을 사용한 model은 hidden state vector의 dimesion이 정해져 있어서 입력문장의 길이가 길어지면 마지막 time step에 있는 hiddenstate vector에 앞서 나왔던 많은 정보들이 잘 담겨져 있지 않다. 아무리 이 LSTM에서 longterm dependency를 해결하려 해도구조상의 문제 때문에 해결하기에 매우힘들다 따라서 이러한 문제를 해결하기 위해서 seq2seq에서 Attention을 활용할 수 있다. Attention은 encoder의 각각의 hidden state vector를 전체적으로 decoder에 제공해주고 decoder에서는 그때그때 필요한 encoder의 hidden state vector를 가져가서 사용한다 decoder의 hidden state vector가 encoder의 어떤 hidden state vector를 가져올지를 결정하게 된다. 이거는 각각을 내적해보아서, 내적에 기반한 유사도를 판별하게 되고 이결과를 softmax에 통과 시켜서 확률값을 얻어내고 이를 각각의 가중치로 사용하여 이들의 가중평균으로서 나오는 하나의 encoding vector를 얻어낼수 있다!!!!!! 이러한 가중평균으로 나온 하나의 vector를 우리는 context vector라고 부른다. 이후에 decoder hidden state vector와 context vector가 concatnate 되어 output layer의 입력으로 들어가게 되고 다음나올 단어를 예측할 수 있게 된다 이러한 과정들을 EOS가 나올때 까지 반복한다. 잘못된 단어를 전단계에서 예측을 하더라도 다음단계에는 올바른 ground truth를 넣어주기 떄문에 하나가 틀려도 이후가 망가지지 않는다. 학습이 끝난후 이 잘못된 단어를 다시 넣어준다. 또한 It’s teacher forcing. Teacher forcing이 아닌 방식이 학습후에 우리가 실제로 사용할때와 비슷하다. Teacher forcing때는 ground truth를 넣어주어야 하기 때문에학습속도가 빠르다 학습의 전반부에는 teacher forcing을 사용후 어느정도 학습이 되면, 이전의 output을 다시 입력으로 넣어주는 방식으로 진행한다. 이처럼 유사도를 측정하는 과정에서 사용되는 내적은, 3가지의 종류로 계산해 낼 수 있다. 2번째인 general 방식으로 게산하는 것을 행렬으로 생각해보자.내적을 기반한 계산을 행렬의 곱으로 생각해보면, 대각행렬의 성분들은 같은 차원끼리의 가중치를 나타내고, 나머지 값들은 다른 차원끼리의 곱해진 값들의 가중치를 나타낸다 이처럼 간단한 내적으로 정의된 형태의 유사도를 그가운데 학습가능한 parameter를 추가함으로서 새롭게 score를 계산했다. 이게 바로 general한 dot product이다. 다음으로 concat을 사용한 score 측정 방식을 보자 이처럼 2개의 vector를 concat시켜 MLP의 입력으로 넣어준 후 non linear activation function을 적용하여 값을 구해낸다. 이수식을 간단하게 보면 Wa는 1번째 layer의 가중치, 그이후에 tanh를 적용한 후 v를 곱해주는데 이는 우리가 최종적으로 얻어야할 output이 scalar값이기 떄문에 v는 row의 형태를 띄어야 한다. 따라서 tranpose를 시켜준것을 확인 할 수 있다. 그렇다면 이들의 paramter은 어떠힌 방식으로 update될까? 결국은 이러한 유사도를 구하는데 필요한 parameter들또한 backpropagation을 통하여 선형변환 행렬들이 학습되게 된다. Attention is great Attention significantly impoves NMT performace 어떠한 한 부분에 집중할 수 있게 해주었다 It solves bottle neck problem encoder의 마지막을 사용했어야 해서 생기는 long term dependency를 해결 Gradient vanishing의 문제를 해결하였다. Attention provides some interpretability 우리가 transform과정에서 모델이 어떠한 부분에 집중 했는지를 확인 할 수 있다. Allignment를 NN이 스스로 배우는 현상을 보여주게 된다. Beam search test과정에서 더 좋은 결과를 얻을수 있게 해주는 하나의 방법 Greedy decoding가장 높은 확률을 가지는 단어 1개를 선택하는 방법 이렇게 되면 어떠한 단어를 잘못 생성해내었을때 다시 뒤로 돌아갈수 없어 최적의 예측값을 내지 못하게 된다 이를 해결하기 위해서 다양한 방법들이 제시된다 Exhaustive Search첫번째 생성하는 단어가 가장큰 확률이였다고 해도 뒷부분에서 나오는 확률값 가장큰 확률값이 아닌 경우가 발생될수가 있다. 이는 결국 time step t 까지의 가능한 모든경우를 따져서 이는 곧 vocab가지수가 되고 V^t^가 가능한 모든 경우의 수이다. 이는 너무 큰 숫자이기 때문에 beam search를 쓰게된다 Beam search매 time step마다 모든 경우의 수를 고려하는게 아니라, 우리가 정해놓은 k개의 가능하 가짓수를 고려하고 마지막까지 decoding을 진행한후 k개의 candidate중에서 가장확률값이 높은걸 선택하는 방식이다. 이를 우리는 hypothesis (가설)이라고 부른다 k는 beam size이 일반적으로 5~10으로 설정하게 된다. 확률들의 곱셈 앞에 log를 붙이게 되면 곱들이 모두 덧셈이 된다. 여기서 log함수 단조증가이기 때문에, 큰값이 큰값을 가진다. ex) k = 2 k가 2이기 때문에 가장 확률값이 높은 2개의 단어를 뽑는다 이중 값이 큰걸 계속해서 선택해 나감 greedy의 경우 end token이 나왔을때가 종료이지만, beam search에서는 서로다른 시점에서 end token이 생성되기 때문에, 각각이 끝날때마다 한곳에 저장해준다. 우리가정한 T라는 시간까지 수행하거나, 완료된 hypothesis가 n개가 되었을때 beam search를 중단한다. 우리가 고려하는 hypotheses의 길이가 다를때는 상대적으로 짧은 길이의 확률이 높은것이고, 길면 낮을것이다. 이를 고려해 주기 위해서는 각 joint prob을 문장의 길이로 나눔으로서 해결해줄 수 있다. BLEU score 생성 model의 점수를 평가하기 위한 척도 고정된 위치에서 정해진 단어가 나와야 된다는 평가방식은 매우 나쁜 방식이다. ex) Reference : Half of my heart is in Havana ooh na na Predicted : Half as my heart is in Obama ohh na Precision(실제로 위치상관없이 겹치는 단어가 몇개인가) = #(correct words)/length_of_prediction = 7/9 Recall(재현률) = #(correct words)/length_of_reference = 7/10 F-measure = (precision x recall) / 0.5(precision + recall) (두 값들의 조화평균) 보다 작은 작은 값에 가깝게 구하는 방식 -&gt; 조화평균 이렇게 구한 값들은 순서를 보장하지 않기 때문에 BLEU가 나왔다. BiLingual Evaluation UnderstudyNgram이란걸 사용했다. 연속된 N개의 단어로 이루어진 문구를 matching하여점수로 반영하였다.","link":"/2021/02/17/2021-02-15-Boostcamp18.1/"},{"title":"RNN심화1","text":"RNN서로다른 time step에서 들어오는 입력 데이터를 처리할때, 매번 반복되는 동일한 rnn module을 호출한다. 각 단어별로 품사를 예측해야 되는 경우 -&gt; 매 time step마다 y를 output으로 어떠한 문장의 긍부정을 판별하는 경우 -&gt; 최종 time step의 y만이 output으로 모든 time step에서 같은 parameter W를 공유한다 주어진 vector가 3차원의 입력벡터로 주어졌을때 ht-1은 2차원이라고 가정하자 xt와 ht-1를 같이 입력으로 받아서 fW에 넣어주면, ht가 나오게 된다 현재 timestep t에서추가적인 outputlayer를 만들고 ht에 Why를 곱해서 yt를 얻어낸다. Types of RNNOne-to-one 입출력 모두가 sequence data인 경우에 입출력이 단 1개인 one-to-many image captioning에서 이러한 구조를 띈다. 초기에 입력이 한번 들어가고 이후 입력으로는 0으로 채워진 tensor를 입력으로 주게된다 many-to-one 최종값을 마지막에서야 내주는 ex) I love movie에서 RNN이 처리한후 마지막의 ht를 봄으로서 긍부정을 예측하게 된다. 길이가 달라진다면 RNN CELL이 그만큼 확장이된다 many-to-many ex) machine translation Ex) POS, vidio의 frame이 sequence대로 주어질때 Character-level Language Model Example of training sequence “hello” vocab = [h,e,l,o] 각각의 character은 one-hot-vector로 표현이 가능하다 Back propagation through time (BPTT)Whh,Why,Wxh 와 같은 parameter들을 학습한다 sequence전체를 한번에 학습하기에는 physical적인 한계가 존재하기 때문에 군데군데 짤라서 제한된 길이의 sequenc 만으로 학습을 진행한다 매 time step마다 hidden state vector가 거의 모든 정보를 담고 있다. 그렇다면 만약에 hidden state의 차원이 3차원이라면, 우리가 원하는 정보가 그중 어느 node에 담겨져 있을까? 이걸 역추적. 첫번째 ht의 node를 고정해 놓고 이후의 변화들을 봄 정작 지금까지 배운 vanila RNN은 잘 활용하지 않는다. 이유는 만약 긴거리에 있는 정보가 매우 중요할 경우 back propagtion으로 구해지기 때문에 gradient vanishing이나 gradient explode가 일어나게 된다. gradient값이 증폭되고있다 LSTM &amp; GRULong short-term Memory보다 효과적으로 long term dependency를 처리할수 있게끔하기 위해 ht를 단기 기억소자로 생각할 수 있으며, 이러한 단기기억을 얼마나 길게 끌고갈 것이지를 판별해주는 역할들을 가진 gate들로 이루어져 있다 전 time step에서 넘어오는 정보가 2가지의 서로다른 vector가 들어오게 된다. 위에 들어오는 vector : Ct 아래쪽에 들어오는 vecor : ht Í Ct-1 이전 cell state와 이전 state의 hidden state를 입력으로 받아 현재의 cs와 ss를 내준다. Hidden state vector은 cell state vector중에 노출되는 정보를 담은, 한번 필터링 된 vector이다. 여기서 sigmoid의 결과와 곱해지면 얼마만큼 이전의 원래값을 반영할지를 결정하는 역할을 한다. 마지막 tanh를 통해 나오는 값은 현재 time step에서 LSTM에서 계산되는 유의미한 정보라고 생각할 수 있다. Forget gate 위를 보면 이전의 hidden state와 현재의 xt를 입력으로 받아 sigmoid 적용후 3차원의 vector가 나오게 되었다. 이렇게 나온 vector와 이전의 cell state의 element wise product를 해주어서 이전의 cell state를 얼마만큼 반영할지를 게산해 주었다. Gate gate Ct에 더해주어야 하는 값을 바로 더해주지 않고 it를 곱해서 더해준다 Output gate 이제 cell state vector Ct로 hidden state vector ht를 만들어준다. 앞서 sigmoid를 적용한 값또한 tanh를 거친 Celll state에 곱한값에 곱해주어 적절한 비율만큼 값을 작게 만들어주어 최종적인 ht를 만들어주게 된다. ht는 다음 rnn의 hidden state로 들어가는 동시에 현재 time step에서 예측을 수행할때 이걸 output layer에 넘겨주어 예측값을 생성해 낸다 GRULSTM에서 2가지 종류의 vector로 존재하던 cell state와 hidden state vector를 일원하 하여 하나의 vector만이 존재하게 한다는게 특징이다. 하지만 전체적인 동작원리는 거의 비슷 forget gate대신 1-zt를 사용, it대신 zt를 사용 input gate가 커질수록 forget gate의 값이 점차 작아지게 되어 결과적으로 이전 hidden state vector를 더 적게 반영하는 것이고, vice versa hidden state를 일원화 하였다 2개의 독립된 gate를 통하여 동작되었던 model을 하나의 gate만으로 줄여 계산량과 메모리 사용량을 줄였다. 정보를 주로담는 cell state가 update되는 과정이 행렬의 계속적인 곱의 연산이 아니라 그때그때 서로다른 gate를 거쳐가며 update되기 때문에 gradient vanishing이 사라진다. 덧셈연산은 이전의 state를 복사해주어 gradient를 유지하는 역할을 한다고 볼 수도 있다. RNN은 다양한 길이를 가질수 있는 유연한 형태의 deep learning구조.","link":"/2021/02/16/2021-02-15-Boostcamp16.1/"},{"title":"Graph2","text":"검색엔진에서의 그래프 페이지랭크의 배경1.1 웹과 그래프웹(방향성이 있는 그래프) = 웹페이지(node) + 하이퍼링크(edge) 웹페이지는 추가적으로 키워드 정보를 포함하고있다. 2.2 구글이전의 검색엔진 웹을 거대한 디렉토리로 정리 웹페이지의 수가 증가함에 따라 카테고리 수도 무한정 커지는 문제가 있다 카테고리 분류가 모호할수가 있다. 키워드에 의존한 검색엔진 악의적인 웹피이지에 취약하다 따라서 page rank라는 하나의 알고리즘을 구글이 만들었다 페이지랭크의 핵심은 투표이다. 웹페이지는 하이퍼 링크를 통해 투표를하게 된다. 사용자가 입력한 키워드를 포함한 웹페이지에서 u가 v에 연결되어있다면 v는 신뢰가능 많이 인용된 논문을 신뢰하는것과 비슷한 알고리즘 웹페이지를 여러개 만들어서 간서의 수를 부풀릴수 있다. 이런식의 악용은 온라인 sns에서도 흔히 발견이 된다. 이러한 악용을 막기위해 가중투표를 사용한다. 측정하려는 웹페이지의 관련성과 신뢰도 자시의 점수 / 나가는 이웃의 수 또한 페이지 랭크는 임의보행의 관점에서도 정의 할 수있다. 웹서퍼는 현재 하이퍼링크중 하나를 균일한 확률로 클릭하는 방식으로 웹을 서핑한다. 이 과정을 무한히 수행하면 p(t) = p(t+1)이된다. 수렴한 p는 정상분포라고 부른다. 결국 이를 정리해보면 투표관점에서의 pagerank정의 수식과 비슷함을 확인할 수 있었더. 페이지랭크의 계산 반복곱 Power iteration은 각웹페이지에 페이지 랭크를 구할때 사용된다 각웹페이지 i의 페이지 렝크 점수를 동일하게 (1/웹페이지수) 로 초기화 한다 아래의 식을 사용하여 웹페이지의 점수를 갱신한다 페이지 랭크 점수가 수렴할때까지 계산을 반복한다. 과연 반복곱이 할상 수렴을 할까요? 수렴하지 않을수가 있다. -&gt; 들어오는 정점은 있지만 나가는 간선이 없는 spider trap에 의해 일어남 과연 수렴을 한다고 해도 이 결과가 합리적인 값일까요? 이 문제점들에 대한 해결책 : 순간이동 임의 보행관점에서 (1) 현재 웹페이지에 하이퍼링크가 없다면 임의의 웹페이지로 순간이동을 한다 (2) 현재 웹페이지에 하이퍼링크가 있다면 $\\alpha$의 확률로 파이퍼 링크중 하나를 균일한 확률로 선택하고 클릭한다 (1-$\\alpha$)의 확률로 임의의 웹페이지로 순간이동 한다 이로 인해 spider trap이나 dead end에 갇히는 일이 사라졌다. 순간이동의 도입은 수식이 바뀐다 각 막다른 정점에서 다른모든 정점으로 가는 간선을 추가한 후 이제는 이 수식을 사용한다. 그래프를 이용한 바이럴 마케팅의사결정 기반의 전파주변의 의사결정을 고려하여 의사결정을 할때 의사결정 기반의 전파모형을 사용한다 —&gt; 선형임계치모형 (linear threshold model) 확률적 전파코로나의 전파를 수학적으로 나타낼때는 확률적 모형을 사용해야 한다. Independent Cascade model 바리럴 마케팅이란? 소비자로 하여금 상품에대한 긍정적인 입소문을 내게 하는 기법 바이럴 마케팅을 위해서는 소문의 시작점이 중요하다. 시드 집합이 전파에 많은 영향을 미친다 그래프. 전파모형, 시드집합의 크기가 주어졋을때 전파의 최대화를 위한 시드집합은 전파최대화 문제이다. 어려운 문제이다. 그래프에 V개의 정점이 있는경우 시드집합이 k개일때 경우의 수는 vCk 이다 이론적으로 전파최대화 문제는 풀기가 힘든 문제임이 증명이되어있다. 대표적 휴리스틱으로 정점의 중심성 을 사용합니다. 즉시드 크기가 k개로 고정이되어있을때 정점의 중심성이 높은수으로 k개 정점을 선택하는 방법이다. 정점의 중심성으로는 페이지 랭크 점수, 연결 중심성, 근접 중심성, 매개 중심성등이있다. 탐욕 알고리즘 시드 집합의 원소를 한번에 한명씩 선택을 한다. 정점의 집합 : {1,2,….,V} 각 집합에 대해 시뮬레이션을 반복하여 평균값을 사용한다. x라는정점이 최초의전파자로 선정이 되어있다. 이런 비교를 통해 뽑힌 집합은 x라고 하자. 이제 x를 포함한 크기가 2이 시드 집합을 찾는다.이를 목표의 크기까지 반복한다. 최초전파자간의 조합을 고려하지 않는다. 탐욕 알고리즘은 항상 최고의 시드 집합을 찾는다는 보장이 없는 근사의 알고리즘이다 항상 최적의 값이 아니라는 말이다. 하지만 적어도 어느정도의 시드집합은 찾을 수 있다.","link":"/2021/02/23/2021-02-23-Boostcamp22/"},{"title":"Graph","text":"그래프란 정점과 간선으로 이루어진 구조 하나의 간선은 반드시 두개의 정점을 연결한다 정점 : vertex,node 간선 : Edge,link 우리의 사회및 모든 다양한 것들은 구성요소간의 복잡한 살호작용으로 이루어진 복잡계이다 이것을 표현하는 방식이 바로 그래프이다 그래프란 복잡계를 간단하게 표현하는 방식이다 정점 분류문제 (node classification) ex) 어떠한 계정이 어떠걸 리트윗했는지를 간선으로 표현. 사람(node)의 보수성, 진보성을 판별하는 랭킹 및 정보검색문제 : 웹이라는 거대한 그래프로부터 어떻게 중요한 웹페이지를 찾아낼까? 군집분석문제 : 연결관계로 부터 사회적 무리(군집)을 찾아낼 수 있을까? 정보전파 &amp; 바이럴 마케팅 문제 : 정보라는 것이 어떻게 네트워크를 통해 전파가 될까? 본강의에서는 위의 문제들을 해결하는 기술들을 배우게될 예정 1주일이라는 짧은 시간이라 기초를 배우고 직관적인 방법론적인 설명 간선에 방향이 있는 directed graph vs undirected graph 협업관계그래프, 페이스북 친구그래프 : undirectied 인용그래프, 트윈터 팔로우 그래프 : directed 간선에 가중치가 있는 그래프 : 전화그래프, 유사도 그래프 간선에 가중치가 없는 그래프 : 페이스북 친구 그래프, 웹그래프 동종 그래프 vs 이종그래프 이종그래프는 두종류의 node를 가진다 . 서로다른 정점 사이에만 간선이 연결된다. Ex) 사용자,상품사이의 전자상거래 내역 동종그래프는 단일종류의 정점을 가진다 node의 집합 V, edge의 집합 : E, G = (V,E) Nout(1), Nin(1)이런거 그래프의 표현 및 저장 Networkx를 사용하여 그래프를 표현 snap.py라는 라이브러리도 많이 사용한다. 1 인접 리스트 1 : [2.5] 2 : [1,3,5] 3: [2] 5: [1,2] 인접행렬 간선이 있으면 1, 없으면 0 방향성이 없으면 대각으로 대칭 있으면 다름 희소행렬을 사용하면 저장공간을 절약할 수 있음 (대부분의 원소가 0일때) 1. 실제그래프 vs 랜덤그래프실제 그래프란 다양한 복잡계로부터 얻어진 그래프를 의미한다 본수업에서는 MSN 메신저 그래프를 실제 그래프의 예시로 사용하겠다. 랜덤그래프란 확률적 과정을 통해 생성된 그래프를 의미한다 에르되스-레니 랜덤그래프임의의 두 node사이의 간선 존재여부가 동일한 확률분포로 나타내어짐 G(n,p)는 n개의 정점, 두개의 정점 사이에 간선이 존재할 확률 = p 2. 작은 세상 효과정점 u와 v사이의 경로란 u에서 시작해서 v에서 끝나야 한다 순열에서 연속된 정점은 반드시 간선으로 연결되어있어야 한다. 경로, 거리, 및 지름 경로는 여러가지지만 이중 가장 짧은 경로의 길이가 거리이다 그래프에서 지름은 정점간 거리의 최댓값이다. 작은 세상 효과 임의의 두사람을 골랐을 때 이들은 몇단계의 지인을 거쳐야 연결되는가? 위치타에서 보스턴까지 지인을 6단게거치면 가능 MSN에서도 정점간의 평균거리는7정도밖에 되지 않는다 단 거대연결구조만을 고려하였다. 이러한 현상을 작은세상효과라고 한다. 작은 세상효과는 높은 확률로 랜덤그래프에도 존재한다. 체인 사이클 격자그래프에는 이 작은세상그래프효과가 존재하지 않는다. 연결성에 두터운 꼬리분포연결성? 정점의 Degree란 그정점과 연결된 간선의 수 : |N(v)| 라고 표현하기도 함 랜덤그래프의 연결성 분포는 높은 확률로 정규분포와 유사하다 실제 그래프는 연결성이 두터워서 hub 정점이 존재할 수있는데 랜덤그래프에서는 정규분포를 띌 가능성이 높다 연결요소 연결요소에 속하는 정점들은 경로로 연결될 수 있습니다. 1의 조건을 만족하면서 정점을 추가할 수 없다. 실제그래프에는 대다수의 정점을 포함하는 거대연결요소가 존재한다 MSN메신저 그래프에는 99,9%의 정점이 하나의 거대연결요소에 포함된다 정점들의 평균 연결성이 1보다 충분히 큰경우, 랜덤그래프에도 높은 확률로 거대연결 요소가 존재한다. 군집이란 정점들의 집합 같은 군집안에서의 정점 사이에는 많은 edge가 존재 지역적 군집 계수 : 그 정점이 군집을 형성하려는 정도 Ci = 정점 i의 이웃쌍중 간선으로 직접 연결된 것의 비율 정점i의 지역적 군집계수가 높으면 이웃들이 연결되어있다.-&gt; 정점 i와 이웃들이 군집을 형성한다 전역 군집 계수 전체 그래프에서 군집의 형성정도를 측정 각 정점에서 지역적 군집계수의 평균이다. 단 지역적 군집계수가 정의가 안되면 짤 세상에는 많은 군집이 존재한다 homophily : 유사한 정점끼리는 공통이웃이 있는경우 공통이웃이 두정점을 매개하는 역할","link":"/2021/02/22/2021-02-22-Boostcamp21.1/"},{"title":"Transformer심화","text":"Transformer Self-Attentionex) I go home I에 대한 input vector가 hidden state처럼 역할을 하여서 I와 각각의 단어에 대한 내적을 한후 이에대한 softmax를 구하여 가중평균을 구한다. 이렇게 encoding vector값을 구하게 되면 결국 자기자신과 내적한 값이 큰값을 가져, 자기 자신에 대한 특성만이 dominant하게 담길것이므로, 이를 해결해주기 위해 다른 architecture를 쓴다 각 vector들이 3가지의 역할을 하고있는 것이다. 동일한 set의 vector에서 출발했더라도 각혁할에 따라 vector가 서로다른형태로 변환할수있게해주는 linear transformation matrix가 있다. 한마디로 각각의 input이 서로다른 matrix에 적용이되어 각각이 key, quary,value가 된다는 의미이다. I 라는 word가 서로다른 matrix에 따라 quart, key, value값이 만들어지고 쿼리는 1개이고 이 쿼리 벡터와 각각의 key vector와의 내적값을 구하고 결과를 softmax에 통과시켜 가중치를 구한후 , 이값과 value vector를 각각 곱해주어 이들의 가중평균으로 최종적인 vector를 구하다. 결국 이 vector가 feature들이 담긴 encoding vector이다. 이러하게 행렬연산으로 위의 과정을 한번에 처리할 수 있다. Multi-head Attention 쿼리,key,value를 만들때 여러 set의 matrix를 적용하여 여러 attention을 수행한다. 이러한 서로다른 선형변환 matrix를 head라고 부른다. 동일한 sequence에서 특정한 quary에 대해서 여러측면으로 정보를 뽑아야하는 경우가 있다. 이후 하나의 선형변환 layer를 추가하여 우리가 원하는 shape의 output을 얻어낸다. 왜 이러한 shape으로 변환해야할까? for residual connection Residual connection을 사용했다. 이는 CV에서 널리쓰이던 Resnet에서 사용한 residue개념을 활용하여, attention 결과의 encoded vector와 원래 입력 vector를 더한다. 이러한 과정을 통해 gradient vanishing과 학습의 속도를 해결하였다. Layer Normalization 주어진 sample에 대해서 그값들의 평균을 0 분산을 1로 만들어준후 우리가 원하는 평균과 분산을 만들어주는 선형변환으로 이루어져있다. 표준화된 평균과 분산으로 만들어줌. 이후 affine transformation (ex : y = 2x+3)을 수행할 경우 평균은 3 분산은 4가 된다. 여기서의 2와 3은 NN이 optimize해야하는 paramter가 된다. 위의 transformer에도 이런식으로 적용이된다 affine transformation은 이제 parameter이라 학습하는? 왜성능이 올라갈까? Transformer에서의 self attention은 순서의 정보를 담고있지 않기 때문에 추가적인 작업이 필요하다. 이 작업을 transformer은 postition encoding에 sinusodial function을 적용하였다. Optimizer은 graident descent가 아닌 Adam을 사용하였다. Learning rate을 고정한 값을 사용하지 않고 학습중에 lr을 변경시켜주었다. Decoder, 나는, 집에, 간다 Masked Multi-Head Attention의 결과가가 얻어졌다면 이를 다시 multi-head attention에 넣어준다. 그데 이제 quary에만 이게 사용되고 encoding단의 encoded vector가 이제 key와 value값에 들어가게 된다. 이제 target language의 vocab size에 맞는 vector를 생성하는 linear transformation을 걸어준다. 그곳에 soft max를 취해서 다음 word를 찾아낸다. 이제 ground truth와의 softmax loss를 구해서 backpropagation으로 학습해 나간다. Masked Self Attention 전체 sequence에 대한 정보를 허용하게 되면 첫번째 time step에서 SOS만이 주어졌는데 나는이라는 단어를 예측해야 하는데 나는이라는 값과 sos사이의 행렬곱값이 있기 때문에 이상황에는 이를 masking해주어야 한다. 이렇게 masking해준후 normalize를 해주게 된다. 가중평균의 합이 1이 되도록","link":"/2021/02/18/2021-02-18-Boostcamp19.1/"},{"title":"Week-1","text":"BOOST camp의 첫주가 끝이 났다정신없이 지내다 아침에 일어나서 오후까지 컴퓨터만 보다보니 5일이 지나갔다.앞으로의 주말계획은 논문 1편과 1주일치 내용 복습이다다음주에 발표인 프로젝트는 어찌될지 모르겠다. 열심히 해보려 했는데 생각보다 쉽지가 않네….많은 사람들과 의견을 나누고 토론하는게 참 좋다. week 1 정리 시작 5. Variable먼저 5강 Variable에서 새롭게 안 사실들이다 이 리스트의 복사, 123a = [1,2,3,4,5]b = [5,4,3,2,1]a = b 위와 같이 코드를 했을때 b가 가르키는 메모리 주소를 a 가 가르키는 형태이다 원래 파이썬에서 이런 처리를 해주었을때 앝은 복사가 일어남을 알고있었다. 얕은 복사는 학교 객지프 시간에 배워서 개념은 알고있었다. 그때 프린터기기와 원본, 사본의 예시로 교수님께서 설명하셨던 것 같다. 파이썬에서도 C의 포인터와 비슷한 개념이 있는듯 하다. C에서의 얕은 복사또한 포인터가 같은 메모리를 가르킨다. a가 독립적으로 b의 값을 가지게 하기 위해서 깊은복사나 아니면 b[:]이런식으로 해준다 6. Function and Console I/O여기서는 f - string을 얻어가자 1234name = 'cho'age = 24print(f&quot;hello my name is {name} and I'm {age} years old&quot;)# hello my name is cho and I'm 24 years old 이걸 잴 많이쓴다고 한다. 나도 원래는 format을 많이 썼었는데 이참에 f-string을 써봐야 겠다 8. String and advanced function concept문자열 함수중에 좀 익숙하지 않은것들 a.titile() a.startswith a.endswith a.isdigit() 큰따옴표 선언이후 작은따옴표 사용 or back slash \\ 2줄 이상저장법 : ‘’’ 3번사용 지역변수와 전역변수의 개념을 파이썬에서 알아보았다 global을 붙히면 함수내부에서 전역변수를 사용가능 이부분 function type hint는 잘 사용하지 않던거라 잊고있었다. 다시 remind 필요 1234def type_hint_example(name:str)-&gt;str: return f&quot;Hello, {name}&quot;type_hint_example('cho') 이렇게 사용자가 타입을 알수있도록 (var:파라메터 타입) -&gt;리턴타입을 표시 그리고 함수의 설명을 위한 docstring 세개의 따옴표로 docstring 영역표시(함수명 아래) 123456789101112131415161718def add_binary(a,b): ''' 여기에 docstring을 작성 대충 작성해보면 Returns the sum of two decimal numbers Parameters: a(int) : A decimal integer b(int) : Another decimal integer Returns : binary_sum(str) 이런느낌으로 parameter과 리턴값들 함수의 역할을 작성해준다 ''' 이제까지 남들이 보는 코드라고 생각안하고 이런걸 몰랐었다 dynamic typing으로 인해 함수의 interface를 알기 힘들기 때문에 아래와 같이 type hint를 줄수 있다 12def do_function(var_name: var_type) -&gt; return_type: pass 사용자에게 명확하게 알려줄 수 있다. mypy또는 IDE,linter등을 통해 코드의 발생가능한 오류를 사전에 확인 시스템의 전체적인 안정성 함수작성 가이드 라인 함수는 가능하지만 짧게 여러개를 만든다 함수 이름에 함수의 역할, 의도가 명확히 들어낼 것 하나의 함수에는 유사한 역할을 하는 코드만 포함 인자로 받은 값 자체를 바꾸진 말고 복사로 처리를 해서 다루어 주자 Coding Convention 일반적으로 4space를 권장함 한줄은 79자까지 연산자는 1칸이상 안 띄움 주석은 항상 갱신 코드의 마지막에는 한줄 추가 소문자 엘 대문자 오 대문자 아이사용금지 기준 : flack8 모듈로 체크 9. Data structure여기서는 namedtuple만 짚고 넘어가자 named tuple이란 Tuple의 형태로 data의 variable들을 사전에 지정해서 저장한다. 1234567from collections import namedtuple# Basic examplePoint = namedtuple('Point', ['x', 'y'])#객체의 이름을 적어주고 뒤에 variable의 이름을 적어줌p = Point(11, y=22)print(p[0] + p[1]) 10. Pythonic Code이부분 Pythonic Code가 상당히 생소했다 이부분을 잘 짚고 넘어가야 추후 코드를 볼때 다시 인터넷을 뒤지거나 자료를 찾아보는 일이 없을것 같다 그리고 이걸 잘하면 간지나고 있어보임 List comprehension에서 주의해야 할점은 이차원 리스트 FIlter 이차원 리스트를 보면 123456case1 = ['A','B','C']case2 = ['D','E','F']result = [i+j for i in case1 for j in case2]# ['AD','AE','AA','BD','BE','BA','CD','CE','CA']result = [[i+j for i in case1] for j in case2]# [['AD', 'BD', 'CD'], ['AE', 'BE', 'CE'], ['AA', 'BA', 'CA']] 그리고 enumerate와 zip은 정말 많이 쓰이니 다시한번 index와 요소들을 tuple로 묶어 반환해줌 1{i:j for i,j in enumerate('hello my name is cho'.split())} 이 코드가 좀 함축적으로 나타내고 있지 편한 기능들을 zip : 2개이상의 list에서 하나씩 추출해서 tuple 형태로 리턴해줌 map 함수는 iterable한 객체 (반복가능한 타입)과 함수를 주어 반환값들을 하나하나 계산해서 묶어줌, 근데 이후에 list든 뭐든 변환이 필요 또 map함수는 iteration을 생성하기 때문에 generator로 자 이제 진짜 중요한데 몰랐던거 iter 과 next함수 12for i in ['a','b','c']: print(i) 이런 반복문에서 내부적으로 __iter__과 __next__가 사용된다 iterable한 객체에 next와 iter함수를 적용시켜 보자 iter을 사용하면 123456cities = [&quot;Seoul&quot;, &quot;Busan&quot;, &quot;Jeju&quot;]iter_obj= iter(cities)print(next(iter_obj))print(next(iter_obj))print(next(iter_obj))next(iter_obj) Generator iterable object의 특수한 형태 얘가 여기에 저장되어있대, 이것만 알고 호출시에 메모리에 올라감 yield를 쓰면 한번에 하나의 element return 리턴임 yeild는 메모리의 주소값만 가지고있다가 yield가 던져줌 제너레이터를 쓰는경우 list형태의 데이터를 반환해주는 함수 데이터의 크기가 매우커서 쓸때, 호출할때만 그걸 메모리에 올려서 사용하는 경우 아래 코드의 차이를 알아보자 12345678910111213141516171819202122232425262728293031323334x = [i for i in range(10)]y = (j for j in range(10))for i in x: print(i)for i in x: print(i) for j in y: print(j)for j in y: print(j) '''012340123401234보면 list로 만들어준 x는 2번 for문을 실핼했을때 모두 출력이 되었는데generator로 만들어준 y는 첫번째 for 문은 출력이 되는데 두번째 for문은 출력이 안된다''' Module and Project 여기서 좀 생소한데 중요한 개념인 package가 나온다 하나의 대형 프로젝트를 만드는 코드의 묶음 다양한 모듈들의 합, 폴더로 연결됨 __init__, main등 키워드 파일명이 사용됨 다양한 오픈소스들이 package로 관리가 된다 __init__ 현재 폴더가 패키지임을 알리는 초기화 스크립트 없을 경우 패키지로 간주하지 않음 하위폴더와 py파일을 모두 포함함 import 와 all을 사용한다 1234__all__ = ['image', 'sound', 'stage']from . import imagefrom . import soundfrom . import stage 이런느낌으로 현재사용할 __main__ 123456from sound import echoif __name__ == 'main': print(&quot;hello game&quot;) print(echo.echo_play())#이런 파일로다가 from game.graphic.render impoer render_test() from .render import render_test() 현재 디렉토리 기준 from ..sound.echo ..2개면 상위 디렉토리로 가자 File/Exeption/Log Handling 예상불가능한 예외 인터프리터 과정에서 발생하는 예외, 개발자 실수 리스트의 범위를 넘어가는 값 호출, 정수 0으로 나눔 수행 불가시 인터프리터가 자동 호출 예상이 불가능 한 예외 발생시 exception handling의 대처가 필요하다 12345678910111213141516171819202122232425262728try: 예외 발생 가능 코드except &lt;Exception Type&gt;: 예외 발생시 대응되는 코드 for i in range(10): try: print(10/i) except ZeroDivisionError: print(&quot;Not divided by 0&quot;) except IndexError as e: print(e) except Exception as e: print(e) #일반적으로 전체를 Exception으로 잡는거는 좋지 않다 finally: print(i,'-----') while True: value = input for difit in value: if digit not in '0123456789': raise ValueError('숫자가 아닙니다') print('정수=',int(value))# raise 구문 사용시 무조건 error를 발생시킨다# assert isinstance(decimal_number, int) 파이썬에서는 try except구문을 권장한다 Python File I/Oread() txt 파일 안에 있는 내용을 문자열로 반환 f = open(‘i_have_a_dream.txt’,’r’) contents = f.read() 여기서 f는 주소를 가지고 있는거임 print(contents) f.close() 파이썬에서 with는 무엇인가를 사용후 반납해야 하는 경우 주로 사용이 된다 파일을 열고 닫을때 닫는걸 까먹을 수 있으니 파이썬의 with구문과 함께 사용한다 123with open(&quot;i_have_a_dream.txt&quot;,'r') as my_file: contents = my_file.read() print(type(contents),contents) with가 끝날때 자동으로 close해줌 indentation이 있을때는 with안의 영역이 계속 사용되고 끝나면 자동으로 반납하는 개념 123with open(&quot;i_have_a_dream.txt&quot;,'r') as f: content_list = f.readlines() #readlines 함수사용시 한줄씩 읽어옴 type(content_list) #list형태로 순차적으로 가져오기 때문에 list가 출력됨 12345678with open(&quot;i_have_a_dream.txt&quot;,'r') as f: i = 0 while 1: line = f.readline() # readlines가 아닌 readline을 쓰면 한줄만올림 if not line: break print(str(i) + '===' + line.replace('\\n','')) i+=1 data가 너무커서 한번에 메모리에 올릴수 없다면 한줄씩 읽어서 해주고 싶은 작업을 해준다 encoding = ‘utf8’ 이걸 이렇게 정해주어야지 헙업도 하고 그럼 123456789 with open(&quot;i_have_a_dream.txt&quot;,mode = 'a', encoding) as f:#위와 같이 mode를 a 로 설정하면 쓰기 읽기 다 가능하다 import ostry: os.mkdir('abc') #현제 디렉토리에 새로운 dir를 만든다except FileExistsError as e: print(&quot;Already created&quot;)os.path.isfile(file.ipynb) 위와 같은 예제를 반드시 확인해 보자 먼저 if not os.path.isdir을 사용하면 현재 dir에 log라는 dir이있는지 확인한다 그이후 log안에 count_log가 업다면 생성해주고 닫는다 with구문 시작시 log를 열어서 그위에 for문을 순회하며 count_log에 문장을 추가한다 Pickle파이썬의 객체를 영속화하는 built in 객체 객체는 원래 memory에 있어야 한다 이 memory에 있는거를 이 객체를 실행중 정보를 저장하고 이후에 불러올때 사용한다 객체는 원래 interpreter가 끝날때 같이 사라지기 떄문에 이를 저장할 수 있는 pickle을 사용하는 것이다 1234567891011import picklef = open(&quot;list.pickle&quot;, &quot;wb&quot;) # pickle은 binary 파일이라 wbtest = [1,2,3,4]pickle.dump(test,f)f.close()f = open(&quot;list.pickle&quot;,'rb')test_pickle = pickle.load(f)test_picklef.close()#이러면 pickle을 불러와서 이전의 test값을 프로그램 종료후에도 읽어올 수 있다 Logging레벨별로 기록을 남길 필요가있음 1234567import logginglogging.debug(&quot;틀렸자나&quot;) #debugginglogging.info(&quot;확인해&quot;) #logging.warning(&quot;조심해&quot;)logging.error(&quot;에러났어&quot;)logging.critical(&quot;망했다&quot;) #프로그램이 완전히 의도치않게 종료가 되었을때 개발시점,운영시점마다 다른 log가 남을 수 있도록 지원함 Debug/info/warning/error/critical debug : 개발시 기록을 남겨야 하는 로그 정보를 남김 Info : 처리가 진행되는 동안의 정보를 알림 warning : 사용자가 잘못 정보를 입력하거나 처리는 가능하나 개발시 의도치 않는 정보가 들어왔을때 error : 잘못된 처리로 인해 에러가 났으나, 프로그램 동작은 가능 할 때 critical : 잘못된 처리로 데이터 손실이나 더이상 프로그램이 동작할 수 없음을 알림 위의 코드를 돌려보면 조심해,에러났어,망했다만 표시가 된다 그 이유는 logging level이 warning이상으로 setting이 되어있어서 이다 파이썬의 로깅레벨은 기본적으로 warning부터 이다 logging.basicConfig(level = logging.DEBUG) steam_handler = logging.FileHandler('my.log',mode = 'w',encoding = 'utf8') logger.addHandler(steam_handler) 이러한 코드 작성시 1. configparser 실행 설정을 파일에 저장 2. argparser 실행시점에 configparser 프로그램의 실행 설정을 file에 저장함 section, key,value값의 형태로 설정된 파일을 사용 설정파일을 Dict Type으로 호출후 사용 configparser를 사용하여 불러오고 argparser console 창에서 프로그램 실행시 setting정보를 저장함 comand-Line-Option이라고 많이부름 보통 cmd 창에서 명령어를 실행할 때 -나 –를 붙혀서 log파일을 실행한다 ex) ls –help help하는 이름으로 실행을 시켜보아라 이런느낌 arg값을 넣어준다 보통 -한개는 짧을 때, –는 길때 parser.add_argument사용시 입력을 받아 args에 저장해준다 그때 지정해준 format인 -a면 a에 저장을 해주고 -b나 –b_value면 b에 저장해준다 사용자가 cmd창에서 실험을 할 수 있게 해준다 epoch값, lr값,momentum값,cuda여부등을 바로 실행시에 지정해줄수 있다. Python data handling이부분에서 xml, json,csv파일들에 대한 개념을 정리하고 간다 정규식도 꼭 몰랐던 부분이니 짚고 넘어가자 Comma Separate Value 엑셀 양식의 데이터를 프로그램에 상관없이 쓰기 위한 데이터 형식이라고 생각하면 쉬움 TSV,SSV등으로 구분해서 만들기도 함 notepad로도 열 수 있고, 쉼표로 구분이 되어있다. 위에서 data_header라는게 있다. 여기에는 데이터의 필드가 담겨있ㅇ며 데이터 저장시 ,로 분리를 하는 코드이다 첫번째 data는 무조건 data의 필드이다 ex) data,index이런 느낌으로 카테고리이다 따라서 첫줄이라면 ,로 나누어서 data_header라는 리스트에 저장해준다 이후 줄부터는 ,로 나누어서 한줄씩 리스트에 저장해준다 text파일 형태로 데이터 처리시 문장내에 들어가있는 “,” 등에 대해 전처리 과정이 필요하다 파이썬에서는 간단히 CSV파일을 처리하기 위해 csv객체를 제공함 12import csvreader = csv.reader(f, delimiter = ',', quotechar = '&quot;',quoting = csv.QUOTE_ALL) delimiter : 글자를 나누는 기준 (default : ‘,’ ) lineterminator : 중바꿈기준 (default : \\r\\n) quotechar : 문자열을 둘러싸는 신호 문자 (default : “ ) quoting : 데이터를 나누는 기준이 quotechar에 의해 둘러싸인 라벨 123456789101112131415161718192021222324import csvseoung_nam_data = []header = []rownum = 0with open(&quot;korea_floating_population_data.csv&quot;,&quot;r&quot;, encoding=&quot;cp949&quot;) as p_file: csv_data = csv.reader(p_file) #csv 객체를 이용해서 csv_data 읽기 for row in csv_data: #읽어온 데이터를 한 줄씩 처리 if rownum == 0: header = row #첫 번째 줄은 데이터 필드로 따로 저장 location = row[7] #“행정구역”필드 데이터 추출, 한글 처리로 유니코드 데이터를 cp949로 변환 if location.find(u&quot;성남시&quot;) != -1: seoung_nam_data.append(row) #”행정구역” 데이터에 성남시가 들어가 있으면 seoung_nam_data List에 추가 rownum +=1with open(&quot;seoung_nam_floating_population_data.csv&quot;,&quot;w&quot;, encoding=&quot;utf8&quot;) as s_p_file: writer = csv.writer(s_p_file, delimiter='\\t', quotechar=&quot;'&quot;, quoting=csv.QUOTE_ALL) # csv.writer를 사용해서 csv 파일 만들기 delimiter 필드 구분자 # quotechar는 필드 각 데이터는 묶는 문자, quoting는 묶는 범위 writer.writerow(header) #제목 필드 파일에 쓰기 for row in seoung_nam_data: writer.writerow(row) #seoung_nam_data에 있는 정보 list에 쓰기 위는 유동인구 데이터중 성남의 데이터만을 수집하는 코드이다 window에서 관리되는 코드는 cp949이다 vscode는 utf8이기 떄문에 encoding을 바꾸어 주어햐 한다 따라서 읽을 때 cp949로 읽는다고 별도로 지정한다 encoding은 cp949, utf8로 왠만하면 저장하기 왠만하면 작은 ‘ 이걸로 나누고 보통 csv는 다른도구를 사용하기 때문에 지금은 이거에 집착할 필요가 없다 왠만하면 pandas를 사용하기 때문에 WebHTML 제목, 단락, 링크 등 요소 표시를 위해 Tag를 사용 모든 요소들은 꺾쇠 괄호 안에 둘러 쌓여있음 &lt;title&gt; Hello, World &lt;/title&gt; 보통 HTML도 일종의 프로그램이기 떄문에 규칙을 분석하여 데이터의 추출이 가능하다 추출된 데이터를 바탕으로 하여 다양한 분석이 가능 string, 정규식 (regex), beautifulSoup Refular expression 복잡한 문자열 패턴을 정의하는 문자 표현 공식 특정한 규칙을 가진 문자열의 집합을 추출 기본문법 문자클래스 [ ] : [와] 사이의 문자들과 매치라는 의미 ex) [abc] : 해당글자가 a,b,c중 하나가 있다 - : 범위를 지정가능 정규식 표현을 위해 원래의미가 아닌 다른용도로 사용되는 문자들 . ^ $ * + ? { } [ ] : ( ) .은 전체를 의미한다 * 는 앞에있는글자를을 반복해서 나올 수 있음 +를 하게 되면 앞에 있는 글자를 최소 1회이상 반복 앞에있는글자를 최소한 1회 이상 반보기냐추배 {숫자} : 반복횟수를 지정해줄 수 있다 ? : 반복횟수가 1회 | : or ex) (0|1){3} 0이나 1이 3번 반복 정규식 추출 연습 Expression부분을 수정해가면 Zip로 끝나는 파일명만 추출 Expression에 (http)(.+)(zip) 정규식 in python re모듈을 import 하여 사용 : import re 함수 : search - 한개만 찾기, findall - 전체찾기 추출된 패턴은 tuple로 반환됨 123456789import reimport urllib.requesturl= &quot;https://bit.ly/3rxQFS4&quot;html= urllib.request.urlopen(url)html_contents= str(html.read())id_results= re.findall(r&quot;([A-Za-z0-9]+\\*\\*\\*)&quot;,html_contents)#findall전체찾기, 패턴대로데이터찾기for result in id_results: print(result) urllib의 request한의 urlopen함수를 쓰면 해당 주소에 해당하는 cotents를 가져옴 findall로 해당패턴을 가진 모든 data를 찾아서 tuple의 형태로 list에 담기게됨 따라서 for loop으로 찍어내면 하나씩 나온다 XMLtag어ㅣ tag사이에 값이 표시되고 구조적인 정보를 표현할 수 있음 HTML과 문법이 비슷, 대표적인 데이터 저장방식 XML은 컴퓨터간의 정보를 주고받기 매우유용한 저장방식으로 쓰임 정규표현ㅅㄱ으로 Parsing이 가능함 가장많이 쓰이는 parser인 beautifulsoup lxml과 html의 from bs4 import BeautifulSoup soup = BeautifulSOup(book_xml,’lxml’) soup.find_all(“author”) tag를 찾는 함수 find_all 123456789101112131415161718192021222324252627import urllibfrom bs4 import BeautifulSoupwith open(&quot;US08621662-20140107.XML&quot;, &quot;r&quot;, encoding=&quot;utf8&quot;) as patent_xml: xml = patent_xml.read() # File을 String으로 읽어오기soup = BeautifulSoup(xml, &quot;xml&quot;) # xml parser 호출invention_title_tag = soup.find(&quot;invention-title&quot;)print(invention_title_tag.get_text())publication_reference_tag = soup.find(&quot;publication-reference&quot;)p_document_id_tag = publication_reference_tag.find(&quot;document-id&quot;)p_country = p_document_id_tag.find(&quot;country&quot;).get_text()p_doc_number = p_document_id_tag.find(&quot;doc-number&quot;).get_text()p_kind = p_document_id_tag.find(&quot;kind&quot;).get_text()p_date = p_document_id_tag.find(&quot;date&quot;).get_text()print(p_doc_number, p_kind, p_date)application_reference_tag = soup.find(&quot;application-reference&quot;)a_document_id_tag = publication_reference_tag.find(&quot;document-id&quot;)a_country = p_document_id_tag.find(&quot;country&quot;).get_text()a_doc_number = p_document_id_tag.find(&quot;doc-number&quot;).get_text()a_date = p_document_id_tag.find(&quot;date&quot;).get_text()print(a_country, a_doc_number, a_date) 이중으로 tag가 쌓여있는 정보는 이중으로 코드도 작성해 주어야 한다 연습 ipg140107.xml분석 분할된 특허문서로 부터 특허의 등록번호,등록일자,출원번호등등을 뽑아 CSV파일로 만들어보자 JSON파이썬에서 json 모듈을 이용하여손쉽게 파싱 및 저장 가능 데이터 write와 read는 모두 dict 와 상호호환 가능 대부분 요즘은 json을 사용한다","link":"/2021/01/22/2021-01-22-Boostcamp_week1/"},{"title":"Attention Is All You Need","text":"AbstractSeuence transduction model들은 현재 복잡한 recurrent한 구조 (RNN) 이나 encoder decoder를 포함한 CNN이 주를 이룬다. 가장 좋은성능을 내는 model또한 attention mechanism을 이용하여 encoder와 decoder를 연결하는 형태이다. 이 논문에서는 새로운 방법인 Transformer를 제안 이는 오로지 attention mechanism만을 사용! 이는 RNN이나 CNN보다 더 병렬화가 가능하고 train하는데 적은 시간이 걸린다! WMT 2014 English to-German data를 사용하여 BLEU라는 score에서 28.4점을 얻었다.(여러 논문을 읽다보면 자주 등장하는 이 BLUE score은 정리해 놓은게 있는데 추후에 posting ) 이는 앙상블을 포함한 이전의 가장 좋은 성능보다 2BLUE가 높다. 1. Introduction RNN모델 (LSTM이나 GRU)는 machine translation과 같은 sequence modeling의 State of the art한(최신의 가장 좋은 성능의) 접근방식으로 알려져있다. RNN은 input과 output의 위치를 계산한 결과를 담고있다. 계산하는 시간이나 순서에 의해 정렬된 위치들은 이전의 hidden state ht-1로 표현된 연속적인 hidden state ht를 생성한다. 이것은 본질적으로 training examples의 병렬화를 배제하며, 이로인해 memory의 한계로 인한 batch size의 한계 때문에 긴 sequence length에 굉장히 critical한 요소로 작용한다. 최근의 연구들은 factorization과 conditional computation(이것에 대한 논문: Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks, Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer )을 이용한 계산과정의 효율화로 큰 발전을 이루어냈다. 하지만 순차적인 계산에 의한 제약은 아직 남아있다 Attention mechanism은 input과 out사이의 길이에 상관없이 dependencies를 modeling할수있다는 부분에서 sequence modeling과 transduction modeling의 필수적인 부분이 되었다. 하지만 몇몇 경우에서는 아직 attention mechanism과 RNN을 합쳐서 사용하고 있다. 이 연구에서는 Transformer라는 attention mechanism에만 의존하여 input과 output의 dependency를 이끌어내는 architecture을 제안한다. Transformer은 병렬화를 가능하게 하고, 성능을 더욱 향상시킬수 있다 요약 : 우리의 transformer가 training example의 병렬화로 인한 속도 향상과 좋은 점수를 낸다. 2. Backgroundencoder , decoder에대한 background 중간의 latent space는 input이나 output보다 훨씬 최소화된 vector이다. Sequential한 계산을 줄이는 목표는 기본적인 building block에서 CNN을 사용하여 병렬적으로 input과 output의 hidden representation을 계산하는 ByteNet이나 ConvS2S의 기반을 이루고있다. 위와 같은 model에서는 2개의 input이나 output의 길이가 증가할수록 계산량이 늘어난다.(ByteNet은 log적으로, ConvS2S는 linear하게). 이러한 결과는 거리에 따른 dependencies들을 학습하기에 더욱 어렵게 만든다. ※ (input과 output사이의 길이가 길어지면 계산량이 증가해 서로의 연관관계를 학습하기가 어렵다는 뜻 cnn은 한번에 kernel size를 진짜 커봤자 최대 7x7을 쓰기 때문에 만약 input이 엄청 길다면 CNN연산시 계산량이 증가하게 되고 위치에 대한 정보의 일부만이 담기게됨. 예를 들어 간단하게 she is pretty and good at playing piano with her own piano와 같은 문장에서 뒤에 her과 처음 she는 긴 거리를 가지게 되어서 이 정보를 담기에 CNN은 부적절?). Transformer에서는 linear나 logmatric하게 계산량이 증가하지 않고 constant한 number로 증가한다.Attention-weigheted position의 평균을 사용하여 Effective한 **해상도?**가 감소함에도Multi-Head Attention과 상호작용 함으로서 계산량을 줄였다. Self-attention은 서로 다른 position에 있는 sequence를 표현하기 위해 서로를 relating한다. Self-attention은 reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representation과 같은 분야에서 성공적으로 사용되어져 왔다. End-to-end memory network은 순서에 따라 정렬된 recurrence가 아닌 recurrent attention mechanism을 기반으로 하고있고, 좋은 성능을 보여주고 있다. Transformer은 처음으로 RNN을 사용하지 않고 오로지 self-attention만이 쓰인 첫번째 변역 model이다. 요약 : Sequence한 문제에서의 모델 ​ RNN의 단점 : 병렬화의 어려움으로 인한 계산의 복잡도 증가, train 시간의 증가, ​ (그리고 강의에서 만약 sequential한 데이터중 중간에 어느 하나가빠진다면 해결하기가 어렵다고 했다) ​ CNN의 단점 : 병렬적인 계산은 이루어 지지만, input이나 output의 길이가 증가할수록 계산도 많고 단어간의 관 계파악이 빡셈 ​ 따라서 Attention만 쓴 Transformer 짱 3. Model Atchitecture가장 경쟁력이 좋은 neural sequence transduction model은 encoder-decoder 구조를 가지고 있다. Encoder은 입력 sequence를 x = (x1,….xn)으로 표현하였고 이를 z = (z1,….zn)으로 map한다. 주어진 z로 decoder가 output sequence인 (y1,…,ym)을 생성해 낸다.(보통 중간의 latent 층은 input과 output에 비해 작은 dimension을 가진다고 조교님께서 설명) 각 step마다 model은 auto-regressive하며, 문장을 생성할때 이전에 생성된 symbol을 additional input으로 가정한다. ※ Auto regressive 복습 (고정된 길이인 $\\tau$만큼의 시퀀스만 활용하는 경우 Autoregressive Model(자기회귀모델)이라고 부른다 직전과거의 정보랑 직전정보가 아닌 정보들을 Ht로 묶어서 활용) Transformer은 stacked된 self-attention을 사용하고 있고, encoder와 decoder부분에 모두 fully connected layer를 삽입하였다. 3.1 Encoder and Decoder StacksEncoder : encoder은 N=6 (6개)인 각각의 identical한 layer들이 층층이 쌓여있다. 각각의 layer들은 2개의 sub-layer로 구성되어있다. 첫번째는 Multi-Head Attention이고, 두번째는 간단한 fully-connected된 feed-forward network이다. 우리는 각각의 sublayer에 residual connection을 들어 주었다. ※여기서 과연 residual connection을 넣은 이유가 뭘까? overfitting 방지 like ResNet?? 각각의 sub-layer의 output은 LayerNorm(x + Sublayer(x)). 모든 sublayer model과 embedding layer의 output의 차원은 dmodel = 512 이다. Decoder : Decoder또한 N=6인 각각의 identical한 layer들이 층층이 쌓여있다. Encoder의 2개의 각 sub-layer에 Decoder은 encoder stack의 output에 대한 multi-Head attention을 수행하는 층이 추가가 되었다. Encoder와 같이 sublayer에 residual connection을 만들어 주었다. 하위 position이 attend하는것을 방지하기 위해 self-attention-layer을 약간 수정하였다**(이게 masking). 이 masking은 i위치의 예측이 i보다 과거의 것으로만 구해지게 하기 위함이다.** ※ Masking은 NLP 문제에서 굉장히 많이 쓰인다고 한다. 알아두자 3.2 AttentionAttention function은 query와 set of key-value pair들을 output에 mapping하는 함수이다. (query,key,value,output은 모두 vector). Output은 value들의 weighted sum으로 계산하며, 이 weight는 query와 다른모든 key값들의 compatibility function으로 정해진다. ※여기서 compatibility function이란? 뒤에서 sum의 형태와 dot product로 나누어 진다. 이들의 차이점은 뒤에 기술 3.2.1 Scaled Dot-Product Attention우리는 이러한 attention을 “Scaled Dot-Product Attention”이라고 부른다. Input은 dk의 차원을 가지는 keys와 queries(둘은 연산(내적)을 위해 같은 차원을 가진다)와 dv의 차원을 가지는 values로 이루어져 있다. 우리는 하나의 query 를 다은 모든 keys들과 내적하고(이 결과 값이 바로 강의에서 score) sqrt(dk)로 나누어 준다. 이후 softmax function을 적용하여 value의 weight를 얻어낸다. ※ i번째 단어에 대한 score vector 계산시 i의 쿼리 vector와 다른모든 key vectors 사이의 내적 (Matmul) 위의 과정들을 queries들을 Q matrix, keys and values를 각각 K and V라고 한다면 아래의 식으로 표현가능 가장 많이쓰는 attention function의 함수는 additive attention과 위와 같은 dot-product attention이다. 우리는 dot-product attention을 썼다. Additive attention은 하나의 hidden layer와 feed forward network를 사용하여compatibility function을 계산한다. 이두가지는 복잡도 측면에서 비슷하지만, dot-product attention이 더빠르고 공간 절약적이다.(이유는 행렬의 계산으로 표현가능) 작은 값을 가지는 dk에서의 두 mechanism은 유사하겠지만, 큰 dk로 나누어 주지 않으면 additive attention이 dot-product의 성능을 넘는다. 큰 dk는 dot -product는 큰값을 가지게 되고, 이는 softmax function이 매우 작은 gradient를 가지게 한다. 이러한 영향을 줄이기 위해 sqrt(dk)로 나누어 주었다. (scale) 3.2.2 Multi-Head Attentiondmodel(max sequence)의 차원을 가지는 keys,values,queries으로 이루어진 single attention을 수행하는것이 아니라, 우리는 queries, keys, values들에 각각 h번 dk,dk,dv를 곱하여 project한값이 더욱 좋은것을 알아내었다. 이러한 projection을 거치면 attention function을 병렬적으로 수행 할 수있으며, dv의 차원을 가지는 output을 얻어낼 수 있다. 이 output은 다시 하나로 concatnated되어 projected된다. Multi-Head Attention은 서로다른 위치에서의 서로다른 subspace의 표현을 jointly attend 하게 한다. 우리는 h = 8개의 parallel attention layer을 사용하였고,dk,dv,dmodel/h = 64 각각의 head에서 차원을 줄임으로서 전체적인 계싼비용이 single head attention과 비슷하게 만들었다??????? ※ 한마디로 이제 dmodel의 차원을 h만큼 parrel layer에 나누어서 넣었으니 결국 single head attention과 비슷하다는 이야기인가?? 3.2.3 Applications of Attention in our ModelTransformer은 multi-head attention을 3가지 다른 방식으로 사용하고 있다 “Encoder - Decoder attention” layer에서 queries는 이전의 decoder layer에서 오고, encoder의 output에서 오는 key와 value들을 저장한다. 이것은 input sequence의 모든 position들을 모든 position의 decoder가 attend 하게 해준다. 한마디로 decoder을 쿼리만 들고있어도 된다. Self attention layer를 포함하는 encoder. Self attention layer에서는 이전 encoder의 layer의 결과에서부터 나온 위치와 같은 위치에서 모든 key, values, and queries가 나온다. encoder속의 각각의 위치들은 이전 encoder의 이전 layer의 모든 위치에 집중한다. (모든 현재 layer의 위치가 이전 layer의 모든 position 정보들을 가진다? 이런느낌?) 비슷하게 decoder의 self attention layer또한 모든 decoder안의 모든(자기자신까지) position에 집중한다.Auto regressive 특성을 보존시키기 위해 왼쪽의 정보들이 decoder로 flow in 하는걸 막아주어야 한다. 우리는 이러한 걸 scaled dot product attention안의 softmax의 output 값에masking out함으로서 해결한다 이걸 다시한번 생각해 보아야 겠다 Auto regressive한 특성이란 이전의 정보들 만으로 현재값을 도출해내는 특성 그니까 결국은 위에(1)식에서 softmax의 결과값에 미래의 정보들은 모두 masking 해준다는것이다. 그니까 결국엔 이식에서 미래의 정보들까지 K와 Q의 내적결과가 다담고 있으니까 미래의 정보는 마스킹 해준다. 3.3 Position-wise Feed Forward Networksencoder와 decoder안의 각 layer에는 fully connected feed forward network를 가져야 한다. 이 fully connected feed forward network는 각각의 위치에 독립적으로 따로 적용된다 위식을 보면 2개의 linear transformation과 ReLU activation을 그사이에 사용하였다. Linear transformation을 각각의 position에 같은 걸 적용해야 한다. 그리고 layer과 layer사이에는 다른 parameter를 적용해야 한다. 또다른 방법은 1의 kernel size를 가지는 2개의 convoltion을 사용하는 것이다. ※ 그니까 하나의 layer에는 같은 weight를 적용하고 다른 layer사이에는 다른 weight를 적용한다는 뜻??? dmodel = 512 inner-layer’s dimension dff = 2048 3.4 Embedding and Softmax여타 다른 번역 모델과 같이 여기서도 학습된 embadding을 사용했다. Decode되어 나온결과도 학습된 linear transformation과 softmax함수를 사용하여 다음 token의 확률을 계산하였다. embedding layer들에는 같은 weight와 presoftmax linear transformation을 사용, weight의 결과에 sqrt(dmodel)을 사용했다. 3.5 Positional Encoding우리의 model은 recurrence도 없고 convolution도 없어서 각각의 model이 sequence의 순서를 사용하게 하여면 position 정보를 삽입해 주어야 한다. 따라서 이와 같이 각각 embedding된 vector에 positional encoding된 vector를 더해준다 이 연구에서는 cos과 sin 함수를 사용했다 pos 는 position i는 차원 각각의 위치가 sinusodial하게 encoding 되도혹 하였다. 주기가 조올라 길어서 다른위친데 주기성 때문에 같은 값을 가지는 경우는 드물다 4. Why Self-Attention이 부분에서는 self attention layer를 RNN과 CNN에 더욱 자세히 비교한다. 앞에서 설명한거에 대한 보충설명 한 layer에서 계산 복잡도에서의 이득 병렬화 될 수 있는계산의 총량 길이가 기이이일어졌을때 얼마나 network에 영향을 미치는지 길이가 졸라리 길어졌을때 dependencies는 매우 중요하다. 이에 가장 중요하게 미치는 영향중 하나가 signal하나가 network를 순회하는 길이이다?? 이게 짧아질수록 긴 길이에 대한 상호적인 관계가 더 잘 학습된다. 따라서 각 model마다 model에서 input과 output 사이의 거리? 뭐하이튼 그 얼마나 model이 compact한가 위 table을 보면 computational한 성능을 높이기 위해 self attention 모델은 해당하는 output 위치의 오직 size r 만큼의 input sequence 주위를 고려하도록 제한되어있다. 이거에 대한 연구는 추후에 발표하겠다고 적혀있다. 그럼 이미 나와있겠지? 하이튼 위에꺼 비교해보면 모든 측면에서 self attention이 와따 5. Training1. Training data and batchingWMT 2014를 사용하여 train함 그리고 문장은 target vocab와 37000개를 공유하는 byte-pair encoding이라는 방식을 사용했음 각 traning batch는 25000개의 source token과 25000개의 target token을 포함하는 문장의 set으로 정해주었다. 2. OptimizerAdam을 썼고, lr을 단계적으로 변화시켰다.아래와 같은 수식으로 3. Regularization Residual Dropout 더해지고 normalized 되기전에 각각의 sublayer의 output에 dropout을 적용하였다 그리고 또 embedding된 vector와 position의 합이후에도 적용하였다 Pdrop은 0.1 Label Smoothing??? 이런걸 적용했다고 하느네 이게 약간 예측불가능한걸 더해줘서 model이 더 새로운것을 배우게끔하는 거라하는데 걍 간단하게 나와있다 6. ResultsBLEU score 잘나왔다 어쩌구 저쩌구 하다가 base model에서는 5개의 checkpoint를 만들어서 그것의 평균을 낸 하나의 model을 썼고, 각 check point는 10분마다 한번씩 interval을 주었다. 이게 내가 조교님한테 질문했던 부분과 좀 연관성이 있다. 이렇게 중간중간에 model을 기록하고 평균을 내는 방식도 있구나 그리고 이 beam search를 사용하였다고 한다 이건 이전의 논문들을 읽을때도 자주 사용했던 기법이다 간단하게 설명하면 가장 확률이 높은 K개을 선택하며 진행하는 것이다 greedy방법보다 효율적이고 score가 잘나온다고 들었다 6.2 Model Variations아래 table의 (A)를 보면 attention의 head의 개수와 key,value의 dimension을 변화시켜주었다. 너무 많은 head를 사용해도 안되고 하나만 사용해도 안됨 이건 너무 당연한거다 뭐든지 적당한게 좋다 6.3 English Constituency Parsing이 Transformer를 활용한 model은 통역에서 나아가서 영어 구문을 분석해주는 방법으로 발전시켜나가야 한다. 이건 별로 중요하지 않은것 같다 해보니까 RNN보다 좋은 성능을 나타내었다 끝 7. Conclusion결론 기존과 다르게 attention에만 기반을 둔 multi-headed self attention을 사용한 이 transformer은 다른 RNN이나 CNN보다 성능이 빠르며 이 Transfomer를 더욱큰 input과 output을 가지는 image나 비디오 오디오 등에적용시키는 것을 기대하고 있다. Transformer 짱짱맨 그리고 조교님께서 관심 있으신 Neuroscience와 Attention사이의 관련 보면 우리 인간의 황반에서도 이 attention의 개념을 적용해서 사물을 인지하고 있으니, 잘되는게 어찌보면 당연하다 출처 : https://arxiv.org/pdf/1706.03762.pdf 그리고 naver boostcamp","link":"/2021/02/05/2021-02-05-Attention/"},{"title":"Image Classification","text":"K Nearest Neighbors (k-NN) 기존의 data가 가지고있는 label을 활용해서 새로운 data의 label을 분류하는 문제가 된다. 이렇게 된다면 미리 유사도를 정의해야 한다. 그리고 system 복잡도가 너무 높다. 따라서 data를 NN의 parameter에 녹여넣는 것이다. Yann Lecun의 CNN 개발 : 우편번호인식에 혁신을 이루어냄 Using better activation function annotation data의 효율적인 학습 기법 data 부족문제의 완화 : 대표적인 방법들 Data augmentation Leveraging pre-trained information Leveraging unlabeled dataset for training Data augmentation Data를 통한 pattern의 분석 Dataset is almost biased != real data결국 우리가 사용하는 data들은 사람이 bias해서 찍은 사진들이 대부분이기 때문에 우리가 얻어놓은 training data까지 모두 표현하지 못하는 data들이다. ex) crop, rotate, Brightness, … Affine transformation 변환전후에 선으로 유지가 되고, 길이의 비율과 평행관계가 유지가 되지만 각도가 달라지는. 기본적인 틀을 맞춘 attine transofrmation mixing both images and labels RandAugment random하게 augmentation 방법을 수행후 잘나온것을 가져다 쓰자. 어떤걸 적용할까, 어떤 강도로 augmentation을 할까? 이걸 policy리고 한다. Random sampling시 dataset을 만들어야 하는데 이러한 data를 모을때 label이 필요하기 떄문에 이러한 data를 단기간에 수집하기가 쉽지가 않다. Transfer learning 기존에 학습시킨 model에 조금 바꿔서 적용. 한데이터set에서 배운 지식을 다른 task에 적용 한 dataset에 적용된 경우에 다른곳에도 적용할 수 있지 않을까?Freeze 기존의 CNN layer’s parameter적은 data로 부터 Pseudo-labeling이 좀 신기하다. Knowledge distillation 더 깊은 network -&gt; 더 높은 성능 깊게 쌓을수록 gradient explosion이나 vanishing gradient가 발생하였다, 계산복잡도가 올라가서 속도의 저하, overfitting문제가 아니라 degradation problem이라는게 밝혀졌다. 네트워크를 깊게 쌓기위한 network GoogLeNet 하나의 layer에서 다양한 크기의 cnn filter를 사용하서 여러측면으로 image를 관찰하겠다. 한층에 이렇게 여러 filter를 사용하게 되면 계산복잡도가 올라가고, parameter숫자가 늘어나기 때문에, 1x1 filter를 추가해 주었다. 1x1 layer as bottle neck architecture 공간크기는 변하지 않고, channel 수만 변화시켜준다. Overall architecture inception module을 깊게 쌓아서 전체 network 형성 Auxiliary classifiers : gradient vanising 문제를 해결하기 위해 추가해준 classifiers. 중간중간에 gradient를 꼽자주는 역할을 한다. loss가 중간에서 부터 흘러들어가기 때문에 멀리있는 단까지 gradient 전달이 가능하다. Auxiliary classifier ResNet아직도 큰 영향력을 발휘하고 있는 network이다. 최초로 100개 이상의 layer를 쌓았다. 최초로 인간 level의 성능을 뛰어넘었다. 이러한 성과로 cvpr best paper를 받았다. 기존연구자들의 layer를 깊게 쌓는데 문제점 원래는 model parameter가 많으면 error가 줄어들 것이라고 생각했는데, 56 layer의 error가 더 크다는 결과가 나왔기 때문에, over fitting때문이 아니라는 결론이 나옴. 대신에 최적화 문제에 대해서 56 layer이 최적화 되지 않은 결과이다. Í 이렇게 만들어 버리면 학습의 부담감이 덜어지고 분할정복이 가능한 문제가 되지 않았는가? 이를 해결해 주기 위해ㅐ shortcut connection을 통해 back prop과정에서 길이 하나가 더생기는 것이다.gradient. vanishing 문제가 해결이 되었다. 왜성능이 잘나올까? residual connection을 하나 추가할때마다 2배씩 path가 늘어난다. 다양한 경로를 통해서 굉장히 복잡한 mapping의 학습이 가능했다. initialization으로 He initialization을 사용했다. Reason ? -&gt; initialize를 작게 해주어야 이후에 더해줄때 균형이 맞는다. 3x3 conv layer로 모두 이루어져 있다. Only a single FC layer at final output DenseNetchannel 축으로 concatnate한다. 훨씬이전의 layer에 대한 정보들도 모두 이어준다. 상위 layer에서도 모든 하위 layer의 특징을 참조할 수 있도록 해주었다. 더하기 두 신호를 합쳐버린다 concatnate chanel은 늘어나지만 feature를 더욱 잘 보존 fix된 3x3 만큼의 weight paramter가 이미 존재를 하고 2d offset을 위한 branch가 따로 존재 한다. 각각의 weight들을 벌려준다? Semantic segmentation픽셀단위로 분류해보자 영상속의 mask를 생성하게 되는데 같은 class이지만 서로다른 물체를 구분하지는 않는다. 영상속에 자동차가 여러대 있어도다 같은 class (색) 으로 구분한다. 영상내의 장면 content를 이해하는데 사용하는 필수적인 기술이다. object들이 구분되는 특징을 이해를 하여 Fully Convolutional Networks입력에서 부터 끝까지 NN으로 구성한다.입력으로 임의의 해상도 출력도 입력에 맞춘 해상도, 중간의 layer들도 모두 미분가능한 layer들이다. 각위치다 channel축으로 flattening이후 각각의 vector를 쌓아서 각 위치마다 vector가 하나씩 나오게 된다. Upsampling receptive field가 작기 때문에 upsampling을 통해서 강제로 resolution을 맞추어준다. 일단은 작게 만들어서 receptive field를 최대한 키운다음에 upsampling한다. Transpose Convolution 결과를 이렇게 그냥 더해도 되는건가?cnn과 stride 사이즈를 조절해서 겹치는부분이 없게끔 조절해주어야 한다. (overlap problem) Upsampling Convolution 학습가능한 upsampling을 학습가능한 하나의 layer로 만들어주었다. 해상도가 낮아지지만 semantic하고 Holistic 중간층의 map을 upsampling한 이후에 높은 layer에 있는 feature map을 upsampling을 통해 해상도를 올리고 이에 맞춰서 중간층의 map들또한 upsampling한다. 이들을 concatnate하여서 각픽셀마다 class의 score를 뱉어주게 된다. 최대한 많은 layer들을 합친것이 큰 도움이 된다. FCN은 end to end로 손으로 만든게 아니라 모두 NN이라 병렬처리도 가능하고 성능도 좋으며, low high feature모두 잘 포함한다. U-Net built upon fully convolutional networks with skip connections channel size가 줄고 해상도가 느는 expanding path fusion - concatnation을 사용한다. DeepLabpixel과 pixel사이의 관계를 이어준후 pixel간의 거리를 모델링하였다.확산의 반복으로 물체의 경계에 잘맞는 segmetation을 Dilated convolution parameter수는 늘어나지만 depthwise convolution channel별로 conv연산을 해서 값을 각각 뽑은후, 각 channel별로 pointwise convolution을 통하여 하나로 합쳐준다. Instance segmentation으로 빠르게 발전을 하고있다. Instance segmantation : 같은 사람이여도 같은색이 아닌 따로따로 segmentation이 가능한 기능 panoptic segmentation Instance segmentation을 포함하는 기술 객체들을 구분하는 기술 : object detection scene understanding을 위한 기술 bounding ob와 classification을 동시에 추정하는 기술이다. 해당하는 box의 물체의 category까지 추정한다.2개의 좌표로 bounging box를 결정한다. 나머지는 class에대한 probability를 결정해 준다. Bounding box localization selective search oversegmentation 이후 비슷한 색깔끼리 합쳐준다. Two-stage detector R-CNN 기존의 image classification을 활용 selective serch 로 region proposal을 구하고적절한 크기로 warping을 해서 CNN (pretrained)에 넣어준후 category를 구해준다.마지막 classifier은 SVM을 썼다.단점 : model 하나하나마다 모두 cnn을 돌려야하고 selective search를 사용해서 학습을 통한 성능향상에 제한이있다. Fast R-CNN recycle a pre-computed feature for multiple object detection 영상전체에 대한 feature을 추출후 이를 재활용 CNN에서 Convolutional feature map을 뽑아주고(warping x) ROI pooling layer로 feature map으로 부터 ROI feature를 뽑아낸다 feature pooling이후 class와 bbox regression을 사용한다. 여전히 roi를 찾기 위해 selective search를 쓰고있다 Faster R-CNN 최초의 endtoend object detection IoU = Area overlap/Area of Union, 높을수록 두영역이 많이 겹친다 Anchor boxes- 9개의 actor box를 사용하였다. 미리 정해놓은 bbox의 크기 Selective search를 대체하는 RPN을 제안하였다. 그럴듯한 bbox만 남기기 위해 non maximum suppression을 사용하였다. Single stage detector정확도 보다 속도를 선택한 것이다 image를 gird로 나누어서 4개의 좌표와 confidence score를 예측한다. 각각의 task보다 Instance segmentation과 Panoptic segmentation Instance SegmentationInstance segmentation = Sementic segmentation + distuguishing instances Mask R-CNN RPN 기존의 ROI 풀링은 정수좌표만 지원을 했었는데, interpolation을 위해서 소수점 pixel level을 지원하였다. Panoptic SegmentationUPSNetFPN구조로 고해상도 feature를 뽑은 이후 Semantic Head 와 Instance Head로 나누어 predict를 하게 된다. Landmark localizationFacial landmark localizaiton Human pose estimation 다양한 data를 사용한 학습 Multi-model learningChallenges 각각의 감각의 데이터가 모두 다른 representation을 띈다 Feature space에 대하여 balance가 맞지 않는다. 여러 modelity를 사용할 경우 특정 model에 bias될수 있다. 대표적인 구조 Matching Translating Referencing Visual data &amp; TextJoint embedding Image tagging 태그 -&gt; 이미지, 이미지 -&gt; 태그 각 feature들은 차원을 맞춰주고 이둘의 Joint embedding을 만들어준다. 같은 space에 이미지와 text를 embedding해주고 matching되는 image와 text 끼리 거리가 가까워 지게끔 학습을 진행한다. Metric Learning 창 - 프로세스 (현재 진행중)탭 - 쓰레드 (그냥 띄워진 창) 쓰레드마다 갖는 메모리 공간 / 프로세스가 공유하는 메모리 공간이 있다. 프로세스가 늘어나면 쓰레드 공유 공간이 늘어나게 된다. process: 코어수에 따라 병렬처리 가능thread: 프로세스 위에 올라가있는 task보통 1개의 process로 concurrent로 처리하는 것 보다, max core의 5070%정도로 process를 나눠서 처리해주는 게 훨씬 좋은 성능을 낸다. —- ps. 파이썬은 멀티프로세싱 &gt;&gt; 멀티쓰레딩search keyword: multi threading/processing, python global interpreter lock(GIL) mini task) [멀티 프로세싱/쓰레딩] 으로 10만까지의 소수 찾고 성능 비교 후 github에 올리기 (~3.15 월) —- point —- 피어세션 뿐만 아니라, 앞으로 공부방향에 있어 수업 내용 외적으로 전체적인 그림을 그리며 공부를 이어나갈 것! (cs, ml pipeline 등…) Q) ml 엔지니어라면, 검색을 했을 때 가장 좋은 결과를 내기 위해서는 어떻게 해야 할까요? A) 어떤 것이랑 어떤 것을 연결시킬 건지…등등 잘 생각해보자! ^_^","link":"/2021/03/08/2021-03-08-Boostcamp31.1/"},{"title":"Pstage2_KLUE","text":"문장내 개체관 관계 추출뭔가 아쉬웠던 P-stage 2 KLUE가 끝이 났다. 이번 stage에서는 리더보드 순위를 올리는데에만 집중하기 보다는 다양한 task를 써보고 원리를 이해하고 결과를 토론계시판에 꼭 조금이라도 공유하는 방식으로 하기로 마음먹었었다. Overview 문제정의 : 문장의 단어(Entity)에 대한 속성과 관계를 예측하라. Input data : 9000개의 train data, 1000개의 test data 우리가 빼내야 할 column : sentence, entities, place of entities Output 총 42개의 class를 예측 1{'관계_없음': 0, '인물:배우자': 1, '인물:직업/직함': 2, '단체:모회사': 3, '인물:소속단체': 4, '인물:동료': 5, '단체:별칭': 6, '인물:출신성분/국적': 7, '인물:부모님': 8, '단체:본사_국가': 9, '단체:구성원': 10, '인물:기타_친족': 11, '단체:창립자': 12, '단체:주주': 13, '인물:사망_일시': 14, '단체:상위_단체': 15, '단체:본사_주(도)': 16, '단체:제작': 17, '인물:사망_원인': 18, '인물:출생_도시': 19, '단체:본사_도시': 20, '인물:자녀': 21, '인물:제작': 22, '단체:하위_단체': 23, '인물:별칭': 24, '인물:형제/자매/남매': 25, '인물:출생_국가': 26, '인물:출생_일시': 27, '단체:구성원_수': 28, '단체:자회사': 29, '인물:거주_주(도)': 30, '단체:해산일': 31, '인물:거주_도시': 32, '단체:창립일': 33, '인물:종교': 34, '인물:거주_국가': 35, '인물:용의자': 36, '인물:사망_도시': 37, '단체:정치/종교성향': 38, '인물:학교': 39, '인물:사망_국가': 40, '인물:나이': 41} EDA이번 자연어 task의 경우에는 데이터도 적고 image task에 비해 복잡한 eda가 필요한것 같지는 않았다. 따라서 가장 중요한 label들의 갯수만을 확인하고 빠르게 다음 단계로 넘어갔다. (토론글에도 하나 작성하긴 했지만, 거기서 작성했던 label들간의 유사성으로 class imbalnace를 해결하기 보다는 focal loss를 사용하는게 직관적으로 더 쉬워 보여서 focal loss를 사용하기로 하였다. label들의 분포 주피터 노트북을 통해 빠르게 data를 불러와서 label들의 분포를 확인하여 보니. label들간의 불균형이 매우 매우 심했다. 심지어 인물 : 사망국가의 label을 가지는 data는 1개 뿐 이였다. 이 1개로 과연 test data의 해당 label을 잘 맞출수 있을까? 아닐것 같다. 이러한 imbalnace문제에 효과적으로 대처하는 방법은 이전 p-stage에서도 배웠었다. 바아아아로 focal loss focal loss에 대해서 간략하게 알아보자 1234567891011121314151617class FocalLoss(nn.Module): def __init__(self, weight=None, gamma=2., reduction='mean'): nn.Module.__init__(self) self.weight = weight self.gamma = gamma self.reduction = reduction def forward(self, input_tensor, target_tensor): log_prob = F.log_softmax(input_tensor, dim=-1) prob = torch.exp(log_prob) return F.nll_loss( ((1 - prob) ** self.gamma) * log_prob, target_tensor, weight=self.weight, reduction=self.reduction ) Focal loss는 페이스북의 Lin et al이 제안한 loss function이다 간단하게 말하면 분류 에러에 근거하여 맞춘 확률이 높은 Class는 조금의 loss를, 맞춘 확률이 낮은 Class는 Loss를 훨씬 높게 부여해주는 가중치를 주어서 class imbalance에 더욱 효율적으로 대처하는 loss funtion이라고 생각하면 된다. loss 함수를 호출하게 되면 input tensor들에 대한 확률로 표현된 tensor를 얻은뒤 F.nll_loss를 호출한다. F.nll_loss(((1-prob) ** self.gamma) * log_prob,target_tensor,weight=self.weight,reduction=self.reduction) Weight 값을 우리가 가지고 있는 label 분포에 대한 1-d tensor로 넣어준다. Ex) 우리가 가지고 있는 label들은 42개니까 42 length를 가지는 1-d tensor 값은 label의 분포에 맞게 이렇게 loss 함수를 불러다가 짜주면 끄읏 Model모델을 굉장히 여러개 불러다가처음엔 돌려보았다. 사용해본 model 목록 bert-base-multilingual-cased xlm-roberta-large koelectra-base-v3-discriminator kobert 이렇게 4개정도 실험해 보았던것 같았다. 결국 결론만 말해보자면 RoBERTa-large를 사용했다.hevitz 님의 토론계시판 글을 보니, bert도 large를 사용하면 좋겠지만?????? 아직 Huggingface에 model이 공개된것 같지 않다. 그리고 This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. 라는 RoBERTa의 논문을 보면 이 significant performance gains 라는 문구는 이게 맞다라는 확신을 가져다 주었다. (물론 논문에서는 다들 자기가 짱이라고하긴하지) 또한 4개를 각각 적당한 hyperparameter로 돌려본 결과 평균적인 정확도가 roberta-large를 사용하면 대폭 증가함을 확인하였다.RoBERTa는 bert와 유사하지만 BERT에 다양한 방법을 적용시켜 성능을 향상한 model이다. Model의 구조는 bert와 흡사하니 생략하겠다. 단지 hyperparameter를 최적화하고 NSP를 없애고 최대한 max_length에 맞춰서 문장을 넣어주고, masking을 더 다양한 방법으로 해주었다고 한다. train 방법? loss optimizer Train-set, validation-set 나누기 이번 KLUE에서 Huggingface의 라이브러리는 질리도록 다룬것 같다. 물론 아직 모자르지만 ㅎㅎ이전 P-stage에서는 training 과정을 우리가 다 pytorch 라이브러리를 사용하여 train 함수를 만들고 쏼라쏼라 해서 구현했었는데!!!! 이런 편리한 trainer라는게 있는 hugging face 아주 칭찬해 ^^ 하지만 단순히 trainer를 사용하는것은 실력 증진에 별로 의미가 없다고 생각했다. (물론 마스터님 말씀처럼 Hugging face만 잘 사용하더라도 그만큼 장점이 있다고 한다!!!) 그래서 huggingface 홈페이지에 들어가서 trainer를 자세히 살펴보았다. 처음에 정했던 focal loss를 사용하기 위해서는 trainer class를 상속받아서 나만의 trainer class를 구현해주어야 했다. 홈페이지를 보면 관련 예제가 있어 쉽게 바꿔줄수 있었다. (현규님의 도움과 함께라면 ㅎㅎ) 12345678class FocalLossTrainer(Trainer) : def compute_loss(self, model, inputs, return_outputs=False) : labels = inputs.pop('labels') outputs = model(**inputs) logits = outputs.logits loss_fn = FocalLoss(weight=weight) loss = loss_fn(logits, labels) return (loss, outputs) if return_outputs else los 여기서 loss_fn만 위에서 정의한 FocalLoss()를 불러다가 사용하면 끄읏! easy Optimizer관련해서는 trainer 안에서 사용하는 AdamW를 그대로 사용하면 될거 같았고 수정해주어야 할 것은 lr_scheduler lr 이정도? 인것 같다. default 로 설정된 lr_scheduler은 step에 따라 linear 하게 lr 이 감소하는 scheduler를 쓴거 같은데저번에 사용했던 CosineAnnealingWarmRestarts 으로 바꾸어서 사용해보면 어떨까? 생각을 해보았다. 저번 stage에서 AdamP와 CosineAnnealingWarmRestarts의 조합으로 꽤나 쏠쏠한 재미를 보았기 때문에 ㅎㅎ 세세한 parameter은 seed를 고정시킨 이후 validation score를 기준으로 설정해주면 될듯 싶다.validation 과 train은 2:8 로 나누었고 제출전에는 data모두를 사용해서 train 한 model로 inference 하였다. 자 이제 가장크게 고민을 해준 부분이다 Input 형식에 따른 성능이번 stage에서 제공된 baseline code는 꽤나 simple하고 간결하지만 있을거는 다있다. 가장 의문점이 들었던 것은 tokenizer에 넣어주는 data 의 형식이였다. ent01 : 이순신 ent02 : 무신 sentence : 이순신은 조선중기의 무신이다. # RoBERTa tokenizer 기준 special token 123456789{'bos_token': '&lt;s&gt;', 'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'sep_token': '&lt;/s&gt;', 'pad_token': '&lt;pad&gt;', 'cls_token': '&lt;s&gt;', 'mask_token': '&lt;mask&gt;'} &lt;s&gt; 이순신 &lt;/s&gt; 무신 &lt;/s&gt;&lt;s&gt; 문장 &lt;/s&gt; 이러한 형식으로 들어갔다.오피스아워에서 여쭈어 보니까, 별다른 이유 없이 간단하게 정해준 형식이라고 했다. 따라서 초코송이님께서 올려주신 ner을 entitiy사이에 넣어주는 논문글을 읽고 pororo library를 이용하여 ner을 얻어낸뒤 이를 entity양옆에 삽입하여 주었다. 이와같은 형식이다. 처음에는 저 기호들을 special token에 추가해준뒤, model에 넣어주었지만!! 혜린님의 질문과 피드백으로 논문에서는 special 토큰으로 지정하지 않고, 원래 vocab에 있는 기호들을 사용하였다고 한다……이미 토론글에 올렸는데……… 올리기 잘했다는 생각이 들었다. 올리지 않았다면 이 문제를 평생 모르고 잘못된 지식을 가진채로 실험하였을것이다. 마스터이나 조교님들 말씀대로 일단 나대는게 좋은것 같다. 남들에게 배울점도 많고, 나대면서 스스로 좀더 찾아보고 학습하게 되는것 같다. 이러한 input 형식은 동일 seed model hyperparamter으로 약 1.5퍼 정도 leaderboard acc의 상승을 이끌어냈다. autotokenizer로 불러온 tokenizer은 대부분의 vocab들을 포함하고 있어, unknown으로 나오는 token들이 적은것을 확인하였다.특히 entity가 unk로 나오게되면 큰 문제임으로 이를 체크하였는데, 모두 잘 tokenize된것을 확인하였다. optimzer은 trainer에서 사용했던걸로 동일하게 사용하였고lr과 scheduler를 바꾸어 가며 RoBERTa에 맞는 hyperparameter를 찾기위해 노렸했다.;) 이번 stage에는 따로 하고있는 ROS 실습과 겹쳐 하고싶은게 3가지 있었는데 못해봤다ㅠㅠ wandb로 실험 관리하기, sweep 사용해서 automl까지 해보기 input에 무작위로 masking 적용해보기 augmentation으로 data 증강해보기 하지만 이번 stage로 관심이 없었던 NLP에 대해 다시한번 생각해보게 되었다. 생각보다 재미있는 아이디어들이 많았고, 특히 오피스아워나 마스터 클래스에서 멘토님과 마스터님의 열정을 보고 굉장히 큰 영감과 자극을 받았다. 아직 한국어 관련 NLP task들이 많이 부족하다는 걸 듣고 확실히 고려해보게 되었다. Hugging face 를 많이 다루어 본점도 매우 득이되었던 stage였다.","link":"/2021/04/22/Pstage2-KLUE/"},{"title":"Pstage3_Image_Segmentation_Detection","text":"Image Segmentationhttps://github.com/bcaitech1/p3-ims-obd-hansarang Overview 문제정의 : 쓰레기가 찍힌 사진에서 쓰레기를 Segmentation Input data : 4109장의 쓰레기 사진중, 3287장 (80%)는 train data, 나머지 812장(20%)는 private test data (512,512)의 이미지 Annotation train_all.json: train에 쓰일 수 있는 모든 image, annotation 정보 (image: 3272, annotation: 26400) train.json: train_all.json 중 4/5에 해당하는 정보 (image: 2617, annotation: 21116) val.json: train_all.json 중 1/5에 해당하는 정보 (image: 655, annotation: 5284) test.json: 예측해야할 이미지들의 정보 (image: 837) id: 파일 안에 annotation 고유 id, 이건 한 image 안에 여러가지의 객체가 있기 떄문에 image별로 각각의 객체의 annotation들이 있다. segmentation: masking 되어 있는 고유의 좌표 bbox: 객체가 존재하는 박스의 좌표 (x_min, y_min, w, h) area: 객체가 존재하는 영역의 크기 category_id: 객체가 해당하는 class의 id image_id: annotation이 표시된 이미지 고유 id images id: 파일 안에서 image 고유 id, ex) 1 height: 512 width: 512 file_name: ex) batch_01_vt/002.jpg Output data : 11 class = {UNKNOWN, General trash, Paper, Paper pack, Metal, Glass, Plastic, Styrofoam, Plastic bag, Battery, Clothing} 평가 Metric 벌써 2주전의 기억이라 가물가물하지만 일렬의 과정들을 하나하나 되짚어 보며 이어나가도록 하겠습니다. deeplab V3+, efficientnet-b5 EDA 먼저 EDA 부터 진행하였습니다. 수업에서 제공해주신 eda들로 우리 data의 label분포를 확인할 수 있었고, 따로 이미지 data를 시각화를 해보며 이전의 stage와 마찬가지로 상당히 imbalance하다는 사실을 알게되었습니다. 이에 stage 2때도 적용하였던 focal loss와 다양한 augmentation을 사용하여 해결해야겠다는 생각이 들었습니다. Model 위에서 언급한것 처럼 처음부터 sota에 가까운 deeplab V3+를 선정하였고, 효율적이고 다양한 실험과정을 위해 backbone은 efficientnet b1으로진행하였습니다. 이에 작은 model에서의 parameter가 과연 큰 model에서도 똑같이 적용될까라는 의문이 들었지만, 이는 lr이나 scheduler에 해당한다고 생각이 되어, augmentation실험에서만 model의 size를 낮추었습니다. 가장처음 한 실험은 제기억에는 동일조건에서의 backbone에 따른 성능이였습니다. 다양한 크기의 Resnext와 efficientNet으로 실험을 진행하였고, 결론적으로 efficientNet-b5를 backbone으로 쓴 model이 가장 성능이 좋았습니다.Resnext는 efficientNet에 비해 수렴속도도 빠르고 epoch당 시간도 적게 걸려, 이후의 앙상블을 위해 best model을 저장해 두었습니다. Augmentation 이번 task에서 쓰게된 augmentation은 아래와 같습니다 HorizontalFlip(p=0.5) Rotate(p=0.5, limit=45) Cutout(num_holes=4, max_h_size=20, max_w_size=20), CLAHE(), RandomBrightnessContrast(p=0.5), Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0) 이렇게 조합해서 썼을때 가장 성능이 좋았다는 결론을 얻었었습니다.다양한 조합과 확률 값들을 적용하여 비교하여 진행하는 일련의 과정들은 매우 고되고 많은 시간을 필요로 하였습니다.여기서 지금와서 생각해보면 Auto Augmentation을 적용해 보았으면 좋았을듯 싶습니다… 또한 추가적으로 horizontalflip을 이용한 tta를 적용시켜 보았지만, 성능의 하락을 야기했습니다. Loss &amp; Optimizer &amp; Scheduler Loss는 Focal loss와 soft-crossentropy-loss를 각각 0.3,0.7의 가중치를 두어 학습하였습니다. 원래는 Focal만을 사용하였지만, stage1에서 multi loss에서 재미를 많이 봤었기 때문에, 마스터님의 의견을 듣고 scl을 추가해 주었습니다. 왜인지는 모르겠지만 soft-crossentropy-loss만을 사용하였을때 가장 성능이 좋아, 앙상블때의 다양성을 위해 multiloss로도 학습을 해두었습니다. Optimizer또한 Adam계열의 Adamp를 사용하였고, Adam계열과 잘어울리는 Customized된 CosineAnnealingWarmRestarts의 scheduler를 사용하였습니다. 확실이 중간중간에 lr을 높혀주는게 local minimum을 잘빠져나오는 모습을 확인 할 수있었습니다. 내부에 내장된 Cosin scheduler은 gamma가 없기때문에 customized된 scheduler를 불러다 사용하였습니다. Adam에 잘맞는 cosine 계열의 scheduler를 사용한 결과, steplr을 사용한 타 팀원의 model 대비 제 model의 성능이 잘나왔음을 확인하였습니다. wandb의 그래프를 보시면 보통 18 epoch쯤에서 최고점을 찍고 수렴하는 모습을 관찰하였습니다. K-fold &amp; Pseudo Labeled data single model의 성능의 한계에 부딛혀 0.63대를 헤어나오지 못하고있었던 2주차…기존의 최고성능 parameter를 고정하고 Train+all과 pseudo labeled된 data를 합쳐서 2배의 data로 학습을 진행하였고 결과는 매우 성공적이였습니다. K-fold로 진행하고 싶었지만, GPU자원의 부족으로 인한 시간의 한계때문에 Train-all로 진행하여 제가 경험적으로 체득한 18 epoch에서 끊는 방식을 체택하였습니다. 결과는 매우성공적으로 single model 기준 0.6842라는 큰 성능향상을 얻어내었습니다. 다른 팀원분들도 pseudo label을 적용하여 앙상블을 하였다면 더 좋은 결과를 얻어낼 수 있었을텐데 매우 아쉽습니다. 앙상블 최종적으로 저의 다양한 backbone과 loss를 가지는 model들을 조합하여 soft voting을 하였습니다. 가중치는 LB상으로 가장높은 model에 0.4를 주었고 나머지에 0.2씩을 주어 총 4개의 singel model을 앙상블 하여 제출을 해봤는데, 0.6961이라는 아주 높은 점수가 나왔습니다. 이 model에 다른 팀원분들의 model hard voting 해보았지만 성능이 계속 하락하여 결국에는 저의 model만을 사용한 점수가 최종점수가 되는 아쉬운 상황이 연출되었습니다… 어느정도 팀원들간의 평균적인 점수대가 비슷해야 앙상블 했을때 좋은 점수를 낼수있었지만, psudo label을 저만 돌렸었기 때문에…시간이 2일정도 더있었다면 다른 팀원 분들도 수도라벨로 성능을 어느정도 향상시켜 비슷한 점수대로 맞춰줄수 있었을 텐데 하는 아쉬움이 남았습니다… Object Detection 평가 Metric 위와 같은 PB curve를 그린다. 이 때 recall과 precision은 confidence score별로 점들이 생성됩니다.이후 (A+B)에 해당하는 영역의 넓이가 AP가 되고, 각각의 class의 AP의 평균이 저희가 구하려는 mAP입니다. 이번 object task에서는 mmdetection이라는 강력한 tool을 기반으로 실험을 진행하였습니다.mmdetection에서는 저희가 config파일만을 수정하여 준다면 손쉽게 다양한 방법으로 다양한 model들을 실험해 볼 수있었습니다. Model 이번 task에서또한 sota model로 알려진 swin transformer를 backbone으로 쓰고 detector 부분은 cascade mask rcnn과 htc를 사용하였습니다.다행이도 Swin transformer의 config파일이 git에 전부 올라와있었고, 저희의 실험환경에 맞게 조금 변경해 주면 되었습니다. 하지만 이 mmdetection이라는 툴자체에 적응을 하는데 시간이 좀 소요가 되었고, 한 3-4일 정도가 지나서야 어느정도 가닥이 잡히면서 어떻게 써야할지 감이 잡혔던것 같습니다. backbone을 고정한 후 neck을 변경시켜서 다양한 실험을 해보았습니다. FPN PAFPN NAS-FPN BiFPN 이렇게 4가지의 선택지가 있었는데 이중 가장 오래된 FPN을 선택한 이유는 한가지 입니다. 왜냐하면 pretrained된 pth파일을 github에서 제공해주고 있는데, 이 model에서 FPN을 쓰고있기 때문입니다. 처음에는 backbone만을 pretrained된걸 가져와서 쓰다가, 전체가 trained된 model이 있는것을 발견하고 실험해보았는데 전체가 pretrained된걸 가져와서 저희 task에 fine tuning? transfer learning하는 방식이 더욱 성능이 좋았습니다. FPN으로 trained된 전체 model을 가져다가 NAS-FPN으로 바꾸어준 model에 적용을 시켜줄시 neck쪽의 weight들에는 값이 들어가지 않게됩니다. 이것이 성능이 더 좋을 수도 있기때문에 이러한 방법도 시도해 보았지만, 바꾸지 않고 그대로 FPN을 사용하는것이 더 좋았습니다. 한가지 아쉬웠던 것은 pretrained된 Swin transformer를 backbone으로 쓰면서 swin에 대한 어느정도 전반적인 이해만을 가지고 있었을뿐, 세세한 model의 구조는 알지 못한채 작성되어진 config 파일만을 가지고 실험에만 집중할 수 밖에 없었던 상황이였습니다. 약간씩 parameter들을 수정해 주면서, 근본적인 이해없이 직관에 의해 실험을 반복하고 있는 제 자신을 발견한 후 competition에 대한 약간의 회의감이 들었습니다. 하지만 멘토님께서 library를 잘다루는 것도 하나의 능력이라고 말씀해주셔서 다시 한번 생각해 보았던것 같습니다. Augmentation Augmentation에는 Flip과 Autoaugmentation, Normalize등을 적용해보았습니다.가장 critical하게 작용했던 augementation이 바로 autoaug로, 여기서 resize와 crop size를 어떻게 주느냐에 따라 성능차이가 조금씩 발생했습니다. 이전 stage들에서의 경험으로, image task에서 high scale의 image training은 오랜 시간을 요구하지만 그만큼 성능이 잘나왔습니다. 따라서 resize의 list에는 upscale된 정사각형과 가로가 긴 직사각형, 세로가 긴 직사각형등을 고루 섞어 autoaug안에 인자로 넣어주었습니다. 이러한 변경점은 정사각형만을 넣어줬을때, low scaleing 해주었을때에 비해서 점수의 큰 향상을 야기했습니다. Loss Optimizer Scheduler Loss 저희가 바꾸어 줄 수있었던 loss는 bbox loss로 3가지 정도의 선택지가 있었습니다. 그중 DIoU Loss를 채택하였을때 성능이 약간 상승했고, classification loss쪽의 cross entropy loss는 건드리지 않았습니다. Optimizer Optimizer은 AdamW를 사용하였고, 초기 lr값은 1e-4으로 고정시켜 주었습니다. Scheduler Scheduler은 model에 따라 다르게 적용시켜 주었습니다. HTC를 이용한 model에는 cosineannealing을 cascade mask rcnn을 적용한 model에는 steplr을 사용하였습니다. StepLR을 사용시 어떠한 epoch에서 lr값을 감소시켜줄지를 정할 수 있었는데, 평균적으로 8,11 epoch에서 map50값이 수렴하기 시작하는것을 확인하고 이쯤에서 gamma=0.1의 factor로 lr값을 감소시켜주었습니다. Pseudo-labeling 이전 segementation에서 pseudo labeling으로 큰 재미를 보았었기 때문에 이번 task에서는 좀 일찍 최고성능의 model로 pseudo data를 만들어 빠르게 실험해 보았습니다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import jsonimport pandas as pdimport numpy as npwith open('./train_all.json') as json_file1: train_data = json.load(json_file1)with open('./test.json') as json_file2: test_data = json.load(json_file2)start = len(train_data['images'])train_id = train_data['images']test_id = test_data['images']for idx in test_id: idx['id'] = start start+=1PredictionString = pd.read_csv('./output1.csv')['PredictionString']annotation = []ids = 26402image_id = startfor bboxs in PredictionString: bboxs = bboxs.strip().split(' ') bboxs = [float(i) for i in bboxs] bboxs = [bboxs[i:i + 6] for i in range(0, len(bboxs), 6)] for bbox in bboxs: prob = bbox[1] if prob&lt;0.8: continue temp = dict() temp['id'] = ids temp['image_id'] = image_id temp['category_id'] = int(bbox[0]) temp['segmentation'] = [[1,2,3,4,5]] temp['area'] = 1 bbox = bbox[2:] new = [bbox[0],bbox[3],bbox[2]-bbox[0],bbox[3]-bbox[1]] new = [i for i in list(np.round(new, 1))] temp['bbox'] = new temp['iscrowd'] = 0 annotation.append(temp) ids+=1 image_id+=1train_data['images'] += test_idtrain_data['annotations'] += annotationwith open('./train+pseudo.json', 'w') as outfile: json.dump(json_data1, outfile,indent=4) 위의 코드로 pseudo data를 coco형태로 바꾸어준뒤 통합된 json 파일로 train을 진행하였습니다. 그러나 threshold값을 설정해주는게 매우 애매했다.이에 따라 성능이 너무 하락하는 현상이 발생하였고 결국 pseudo label된 data는 쓰지 못하였습니다… segementation에서 잘먹히던 pseudo label이 detection에서는 부정확한 label값들로 학습이 어려운가 봅니다. WBF 마지막으로 최고 single model 기준 htc와 cascade 모두 0.5572의 점수를 얻어냈고 총 4개의 model을 앙상블한 결과 0.5824의 결과를 얻어냈습니다.이후 모든 팀원들의 csv 파일을 WBF하여 최종적인 score 0.5884를 얻어내었습니다. Conclusion 한가지 가장 중요하게 느낀점은 이렇게 competition을 마친이후에 관련 논문들과 kaggle notebook들을 자세히 정독하며 쓰였던 방법론들과 model들을 상세하게 공부해야 겠다는 필요성입니다. competition 진행중에 개선해야 할 사항은 중간중간 팀원들간의 평균적인 점수대를 맞추어 놓아야 최종 앙상블 과정에서 큰 성능 향상을 이룰수 있다는 점입니다. 굉장히 열정적인 4주를 보냈습니다… 아쉬움도 남고 후련하기도 합니다…진행했던 많은 실험 내용들을 모두 랩업레포트에 담지 못했다…. 추후에 체계적으로 정리해서 git에 올려둬야겠습니다.","link":"/2021/04/25/Pstage3-Image-Segmentation-Detection/"},{"title":"VQA (Visual Question Answering)","text":"Boostcamp에서 만난 동료들과 함께 2021 인공지능 온라인 경진대회에 참여했습니다.총 10개의 과제중 시각장애인 시스템 개발을 위한 VQA 모델이라는 Competition에 참여하였습니다. 개요이미지를 보고 주어진 질문에 답변하는 Visual Question Answering 모델 개발VQA란 시각정보를 기반으로 질문에 답변하는 시스템입니다.실내 및 실외 생활 거주 환경에서 촬영된 이미지와 그에 관련된 질문, 대답이 세트로 이루어져 있습니다.총 224,464개의 이미지 파일과 702,135건의 질문-답변 쌍이 train data로 주어졌습니다. 관련 논문 review VQA: Visual Question Answering Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks UNITER: UNiversal Image-TExt Representation Learning","link":"/2021/07/12/VQA/"},{"title":"VQA: Visual Question Answering vs Competition Baseline","text":"VQA task의 시초격인 논문이다. VQA challenge의 전반적인 개요와 dataset, Base model등을 다루고 있다. 1. Introduction VQA란 Vision, NLP, knowledge representation을 모두 접목시킨 multi-discipline task이다 위와 같이 어떠한 image가 주어지고, image가 없으면 맞추기 힘든 질문들로 구성되어있다. Answer의 유형에 따라 open-ended questions 과 multiple-choice task로 나누어 진다. Open-ended question은 answer가 다양해 질 수 있지만, multiple-choice task의 경우에는 미리 정해진 answer list중 하나가 답안이여야 한다. 이 논문에서는 두가지의 answer유형 전부 다루고 있다. (우리의 VQA task는 multiple-choice task 만을 다루고 있기 때문에 open-ended 는생략) 2. DatasetImage data MS COCO dataset Object Detection과 image captioning에서 사용되는 dataset containing multiple objects and rich contextual information 123,287 training and validation images and 81,434 test images Abstract Scene dataset The dataset contains 20 “paperdoll” human models [2] spanning genders, races, and ages with 8 different expressions. Paperdoll로 real한 상황을 표현함 we create a new abstract scenes dataset containing 50K scenes Question data간단한 question이 아닌 복잡하고 어려운 question을 만들어내기 위해 “We have built a smart robot. It understands a lot about images. It can recognize and name all the objects, it knows where the objects are, it can recognize the scene (e.g., kitchen, beach), people’s expressions and poses, and properties of objects (e.g., color of objects, their texture). Your task is to stump this smart robot! Ask a question about this scene that this smart robot probably can not answer, but any human can easily answer while looking at the scene in the image.” 이러한 요청을 주어 question을 만들어 내도록 하였다. Answer data18개의 선택지를 구성 Correct : The most common (out of ten) correct answer Plausible : To generate incorrect, but still plausible answers we ask three subjects to answer the questions without seeing the image Popular : These are the 10 most popular answers. For instance, these are “yes”, “no”, “2”, “1”, “white”, “3”, “red”, “blue”, “4”, “green” for real images Random : Correct answers from random questions in the dataset 3. Model VQA를 위한 Base model과 Competition에서 제공해준 Baseline의 구조도이다. Image channel Image로 부터 embedding vector를 뽑아내는 역할을 한다.Pretrained된 VGGNet을 사용하였으면 기존 VGGNet의 마지막 Fully-Connected MLP의 layer에서 4096 dim으로 뽑아내었다. 인공지능 경진대회측에서 제공한 baseline에서는 pretrained된 Resnet-50을 사용하여 마지막 Fully-Connected MLP의 layer에서 768 dim으로 뽑아내고 있다. 이후 논문과 다르게 별도의 Fully-Connected layer를 통과시켜주지 않는다. 애초에 question channel과 dimension을 맞추어주었다. 마지막 target dimension은 83으로 train data의 label 개수이다. Question channel Question으로 부터 embedding vector를 뽑아내는 역할을 한다.Each question word is encoded with 300-dim embedding by a fully-connected layer + tanh non-linearity which is then fed to the LSTM. Cell state, hidden state dim = 512Concate last cell state and last hidden state representations 을 하여 1024의 차원을 만들어주었다. 인공지능 경진대회측에서 제공한 baseline에서는 huggingface를 사용하여 pretrained된 RoBERTa-base model을 통해 embedding vector를 뽑아 내었다. 뽑아낸 embedding vector의 dim이 768이라 Image channel의 last dimension또한 768로 맞춰준것이다. Multi-Layer Perceptron 이부분은 둘의 구조가 동일하다. 각 channel에서 나온 embedding vector들을 element wise multiplication해준다. 이후 layer을 추가해주어 차원을 늘려준뒤, 마지막 layer에서 target label의 개수만큼의 dimension을 뽑아낸다. 이부분에서 궁금했던점은 각 channel의 embedding vector를 합치는 방법에 따른 성능의 차이였다. Concat Element-wise Multiplication Element-wise Add 이와 관련된 논문을 찾아보았다. Component Analysis for Visual Question Answering Architectures 이라는 논문에서 각 fusion 방식에 따른 성능을 실험해보았다. 위 논문의 결과에 따라 3가지 fusion 방식중 Multiplication 방식을 계속해서 고수했다. 또한 위논문에서는 BERT를 사용하여 question문장을 embedding만 하고 GRU를 사용하여 최종 vector를 뽑아내고 있다. 이러한 방식도 시도해 볼만 할것 같다. 4. Result","link":"/2021/07/12/vqa_paper1/"},{"title":"fpn","text":"","link":"/2021/07/12/fpn/"},{"title":"swin_transformer","text":"","link":"/2021/07/12/swin-transformer/"},{"title":"satrn","text":"","link":"/2021/07/12/satrn/"},{"title":"Pstage4_수식인식","text":"Pstage4_수식인식기술스택: AWS, Git, OpenCV, Pytorch, Streamlit, wandb발표시간: 15:00 - 15:20발표트랙: TRACK 4캠퍼 ID: T1076, T1104, T1116, T1200, T1220, T1224캠퍼 이름: 원 조, 광원 송, 찬엽 신, jaesub huh, Geumji Tak프로젝트(대회): OCR Github Repository 팀 소개 🎸 조원 SATRN (Locality Feedforward, Shallow CNN) 구현 Augmentation (Resize &amp; Noramlization&amp; pixel 평균에 따른 이진화…) 제안 &amp; 실험 Ensemble (soft voting) 구현 &amp; 실험 teacherforcing ratio scheduling 실험 Beam search 구현 부족한 token에 대한추가 Data 생성 BERT를 사용한 misspelled된 수식 잡아내기 제안 CSTR 논문 reading &amp; 공유 협업 Git의 Discussion, Pull &amp; Request, Wiki를 활용하여 토론, 자료, 결과공유 Wandb를 사용하여 실험공유 수식인식 Competition Overall 상세개요 수식 이미지를 latex 포맷의 텍스트로 변환하는 문제입니다. 수식은 여러 자연과학 분야에서 어려운 개념들은 간단하고 간결하게 표현하는 방법으로서 널리 사용되어 왔습니다. Latex 또한 여러 과학 분야에서 사용되는 논문 및 기술 문서 작성 포맷으로서 현재까지도 널리 사용되고 있습니다. 일반적인 OCR과 달리 분수, 시그마, 극한과 같은 표현을 인식하기 위해 multi line recognition을 특징으로 가집니다. Dataset Scale : 각각의 image scale은 제각각 Label : Latex 형식의 수식 Train Data Hand Written Data : 50000 Printed Data : 50000 Evaluation Data Public : 6000 Private : 6000 Prediction 241개의 token class중 하나를 생성한 뒤 이들의 sequence 평가 Metric 0.9 x “Sentence Accuracy” + 0.1 x (1-“WER”) Sentence Accuracy 전체 추론 결과(수식) 중에서 몇 개의 수식이 정답(ground truth)과 정확히 일치하는 지 WER (Word Error Rate) 단어 단위로 삽입(insertion), 삭제(deletion), 대체(substitution)된 글자 개수를 계산 Problem &amp; Solving EDA 부족한 token data Train data에 등장하는 token들중 빈도수가 가장적은 50개를 뽑아보았다. 10회 미만으로 등장한 token들에 대한 추가적인 data가 필요하다고 판단되었다. 사이트를 활용하여 부족한 token들을 포함하는 수식을 train파일에 50개 정도의 data를 추가해주었다. train_100003.jpg \\ominus \\vdots \\nexists \\rightleftarrows A \\supsetneq B 세로형태의 수식 이미지 다양한 Aspect Ratio 잘못된 labeling P = 1이 아닌 P = 으로 labeling 되어있음 선이 그어져 있거나 형광팬이 칠해져 있는 case Augmentation Resize Rotate OCR task에서 세로로 되어진 image들은 모두 noise로 작용 EDA에서 적당한 Aspect ratio의 threshold를 찾은 뒤, threshold 미만인 data들에 한하여 Rotate Normalization 이미지의 각 pixel 값들을 0-1의 값으로 normalize 시켜준다 이진화 &amp; 가로선 제거 Model (SATRN) Locality-aware feedforward layer Base-line model에 구현된 Fully-connected feed forward에서 논문에서 제시하고 있는 Convolutuon feed forward로 교체 Adaptive 2D Positional Encoding Baseline에서 구현된 일반적인 2D positional encoding에서 논문에서 제시한 학습가능한 adaptive 2D positional encoding으로 변경 Backbone EfficientNet DenseNet ShallowCNN Mini SATRN for fast experiment 다양한 실험을 빠르게 진행하기 위해 SATRN의 size를 줄여서 Mini SATRN으로 다양한 실험을 진행 SATRN의 layer parameter 수정 Increase number of Decoder Layer mini model 기준, Decoder layer를 추가할수록 성능이 향상 Change Activation Function ShallowCNN의 activation function을 ReLU대신 mish를 사용 Post - Processing Beam search Token을 뽑을 때 argmax를 사용하여 하나의 token만을 뽑는게 아닌 각 step마다 beam size k 만큼의 token을 뽑아 최대한 적합한 sequence를 선택하려 시도함 Ensemble 다양한 augementation을 거친 model들을 한번에 불러와서 soft voting을 사용하여 ensemble Baseline에는 빠져있는 torch.nograd로 memory 절약 Use Language Model (x) 관련 토론 link OCR Demo데모페이지 주소 : http://35.74.99.158:8501/ Paper Review 프로젝트 진행을 위해 읽은 논문의 목록은 다음과 같음 CSTR.pdf Misspelling Correction with Pre-trained Contextual Language Model.pdf CRNN.pdf An_Attentional_Scene_Text_Recognizer_with_Flexible_Rectification.pdf CBAM.pdf Towards End-to-end Text Spotting with Convolutional Recurrent Neural Networks.pdf TextBoxes_A Fast Text Detector with a Single Deep Neural Network.pdf","link":"/2021/06/24/%5BOCR-04%5D%202021%20%E1%84%8E%E1%85%AE%E1%86%AB%E1%84%92%E1%85%A1%E1%84%80%E1%85%A8%20%E1%84%83%E1%85%A1%E1%86%AB%E1%84%80%E1%85%B5%E1%84%80%E1%85%A2%E1%84%87%E1%85%A1%E1%86%AF%20%E1%84%8E%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A7%E1%86%AF%E1%84%89%E1%85%A1%E1%86%AB%20f82316ab87ef4617ab2e8ce26be53669/"},{"title":"cstr","text":"","link":"/2021/07/12/cstr/"}],"tags":[{"name":"boj","slug":"boj","link":"/tags/boj/"},{"name":"nlp","slug":"nlp","link":"/tags/nlp/"},{"name":"hugging_face","slug":"hugging-face","link":"/tags/hugging-face/"},{"name":"ner","slug":"ner","link":"/tags/ner/"},{"name":"Basic","slug":"Basic","link":"/tags/Basic/"},{"name":"Math","slug":"Math","link":"/tags/Math/"},{"name":"Books","slug":"Books","link":"/tags/Books/"},{"name":"Book","slug":"Book","link":"/tags/Book/"},{"name":"Summary","slug":"Summary","link":"/tags/Summary/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"Vision","slug":"Vision","link":"/tags/Vision/"},{"name":"RNN","slug":"RNN","link":"/tags/RNN/"},{"name":"Transformer","slug":"Transformer","link":"/tags/Transformer/"},{"name":"GAN","slug":"GAN","link":"/tags/GAN/"},{"name":"GPT","slug":"GPT","link":"/tags/GPT/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"Graph","slug":"Graph","link":"/tags/Graph/"},{"name":"Multimodal","slug":"Multimodal","link":"/tags/Multimodal/"},{"name":"Competition","slug":"Competition","link":"/tags/Competition/"},{"name":"VQA","slug":"VQA","link":"/tags/VQA/"},{"name":"CV","slug":"CV","link":"/tags/CV/"},{"name":"OCR","slug":"OCR","link":"/tags/OCR/"}],"categories":[{"name":"Algorithm","slug":"Algorithm","link":"/categories/Algorithm/"},{"name":"blog","slug":"blog","link":"/categories/blog/"},{"name":"Mathmatics_for_ML","slug":"Mathmatics-for-ML","link":"/categories/Mathmatics-for-ML/"},{"name":"Boostcamp","slug":"Boostcamp","link":"/categories/Boostcamp/"},{"name":"PaperReview","slug":"PaperReview","link":"/categories/PaperReview/"},{"name":"Competition","slug":"Competition","link":"/categories/Competition/"}]}